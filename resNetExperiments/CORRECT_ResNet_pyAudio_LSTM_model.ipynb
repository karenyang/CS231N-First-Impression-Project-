{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_dir = '/home/noa_glaser/proj/data/train-frames/train'\n",
    "val_dataset_dir = '/home/noa_glaser/proj/data/train-frames/val'\n",
    "audio_dataset_dir = '/home/noa_glaser/proj/data/train-audio'\n",
    "label_dataset_dir = '/home/noa_glaser/proj/data/'\n",
    "exp_name = 'ResNet_LSTM_L1' # roughly 3K videos\n",
    "num_classes = 5 \n",
    "num_partition = 10\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import time\n",
    "import copy\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first, get the Images (path) and their labels (five personality traits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_img_audio_label(dataset_dir,audio_dataset_dir,label_dataset_dir):\n",
    "    \"\"\"Returns a list of np.array(img_paths), np.array(audio_paths),\n",
    "        np.array(labels), np.array(raw_movienames)\n",
    "    Args:\n",
    "    dataset for video, for audio_feats from pyAudioAnalysis, labels\n",
    "    \"\"\"\n",
    " \n",
    "    print(\"processing dataset: \"+ dataset_dir)\n",
    "    img_paths = [] \n",
    "    audio_paths=[]\n",
    "    raw_movienames = []\n",
    "    labels = []\n",
    "\n",
    "    annotaion_filename = label_dataset_dir + \"/annotation_training.pkl\"\n",
    "    \n",
    "    with open(annotaion_filename, 'rb') as f:\n",
    "        label_dicts = pickle.load(f, encoding='latin1') \n",
    "\n",
    "    for movie in os.listdir(dataset_dir):\n",
    "        fileEnding ='_50uniform' #TODO: figure out how to make more general\n",
    "        if fileEnding not in movie: continue #skip non-movie files\n",
    "        raw_moviename = movie.replace(fileEnding,'.mp4')      \n",
    "        big_five = [label_dicts['extraversion'][raw_moviename], \n",
    "                    label_dicts['neuroticism'][raw_moviename],\n",
    "                    label_dicts['agreeableness'][raw_moviename],\n",
    "                    label_dicts['conscientiousness'][raw_moviename],\n",
    "                    label_dicts['openness'][raw_moviename] ]\n",
    "                    #label_dicts['interview'][raw_moviename]]      \n",
    "        movie_path = os.path.join(dataset_dir, movie)\n",
    "        mv_partitions = []\n",
    "        p = 0\n",
    "        all_imgs = os.listdir(movie_path)\n",
    "        assert(len(all_imgs) >= num_partition)\n",
    "        opened = True\n",
    "        for i in range(num_partition):\n",
    "            path = os.path.join(movie_path, all_imgs[i])\n",
    "            try:\n",
    "                open(path)\n",
    "            except:\n",
    "                print('image failed to open',path)\n",
    "                opened = False\n",
    "                \n",
    "            mv_partitions.append(path)\n",
    "        assert(len(mv_partitions)==num_partition)\n",
    "        \n",
    "        \n",
    "        audiofeat_path = os.path.join(audio_dataset_dir,raw_moviename+'.wav.csv')\n",
    "        try:\n",
    "            open(audiofeat_path)\n",
    "        except:\n",
    "            print('audio failed to open',path)\n",
    "            opened = False\n",
    "        if opened :\n",
    "            img_paths.append(mv_partitions)\n",
    "            audio_paths.append(audiofeat_path)\n",
    "            raw_movienames.append(raw_moviename)\n",
    "            labels.append(big_five)\n",
    "            \n",
    "    \n",
    "    return np.array(img_paths),np.array(audio_paths),\\\n",
    "                np.array(labels), np.array(raw_movienames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## use this if we have seperated train/val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dataset: /home/noa_glaser/proj/data/train-frames/train\n",
      "processing dataset: /home/noa_glaser/proj/data/train-frames/val\n"
     ]
    }
   ],
   "source": [
    "train_img_paths,train_audio_paths, train_labels, train_movienames \\\n",
    "        = get_img_audio_label(train_dataset_dir,audio_dataset_dir,label_dataset_dir) \n",
    "val_img_paths,val_audio_paths, val_labels, val_movienames \\\n",
    "= get_img_audio_label(val_dataset_dir,audio_dataset_dir,label_dataset_dir)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Loader. Data is normalized before feeding into model (as required by the pretrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def default_img_loader(img_paths,transform):\n",
    "    ten_img_tensor = []\n",
    "    for path in img_paths:\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if transform is not None:\n",
    "            img = transform(img)\n",
    "        ten_img_tensor.append(img)\n",
    "        \n",
    "    return torch.cat(ten_img_tensor)\n",
    "        \n",
    "\n",
    "def default_audio_loader(path):\n",
    "\treturn np.loadtxt(path,delimiter=',')\n",
    "\n",
    "class VisualAudio(data.Dataset):\n",
    "    def __init__(self,split,img_paths,audio_paths, movie_names,labels,transform=None,\n",
    "                 img_loader=default_img_loader,audio_loader=default_audio_loader):\n",
    "        self.split = split \n",
    "        self.img_paths = img_paths\n",
    "        self.audio_paths = audio_paths\n",
    "        self.movie_names = movie_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.img_loader=img_loader\n",
    "        self.audio_loader= audio_loader\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_paths, audio_paths,target = self.img_paths[index], \\\n",
    "                                        self.audio_paths[index], self.labels[index]\n",
    "        ten_img_tensor = self.img_loader(img_paths,self.transform)\n",
    "        ten_audio = self.audio_loader(audio_paths)\n",
    "        #return 30x224x224 , 10x68, 10 x 5\n",
    "        \n",
    "        assert(ten_img_tensor.size() == (30,256,256))\n",
    "        \n",
    "        return ten_img_tensor, ten_audio[:10,:], target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 4800, 'val': 1200}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import  transforms\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.RandomCrop(256),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "        \n",
    "dsets = {}\n",
    "dsets['train'] = VisualAudio('train',train_img_paths,train_audio_paths,\\\n",
    "                    train_movienames ,train_labels,transform=data_transforms['train'] )\n",
    "dsets['val'] = VisualAudio('val',val_img_paths,val_audio_paths,\\\n",
    "                         val_movienames,val_labels,transform=data_transforms['val'] )\n",
    "\n",
    "dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=1) for x in ['train', 'val']}\n",
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "dset_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Some dataset examples (each batch is 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    \n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "#train_imgsamples,train_audiosamle,train_labelsample = next(iter(dset_loaders['train']))\n",
    "\n",
    "#train_unflattened_sample = train_imgsamples.view(-1,3,256,256)[0:10,:,:,:]\n",
    "## Make a grid from batch\n",
    "#plt.figure( figsize=(100, 20))\n",
    "#out = torchvision.utils.make_grid(train_unflattened_sample,nrow=10)\n",
    "#imshow(out, title='trainning sample')\n",
    "#plt.savefig('train_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#val_imgsamples,val_audiosamle,val_labelsample = next(iter(dset_loaders['val']))\n",
    "#val_unflattened_sample = val_imgsamples.view(-1,3,256,256)\n",
    "### Make a grid from val batch\n",
    "#plt.figure( figsize=(10, 16))\n",
    "#out2 = torchvision.utils.make_grid(val_unflattened_sample,nrow=10)\n",
    "#imshow(out2, title='validation sample')\n",
    "#plt.savefig('val_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AudioVisualLSTM(nn.Module):\n",
    "    NUM_AUDIO_INPUT = 68\n",
    "    NUM_VID_FEATURES = 128\n",
    "    NUM_AUDIO_FEATURES = 32\n",
    "    NUM_LSTM_HIDDEN = 128\n",
    "    NUM_PARTITIONS = 10\n",
    "    NUM_CLASS = 5\n",
    "    NUM_IMG_SIZE = 256\n",
    "    NUM_CHANNEL = 3\n",
    "    \n",
    "    def __init__(self):        \n",
    "        super(AudioVisualLSTM, self).__init__()\n",
    "        self.audioBranch =  nn.Sequential(nn.Linear(self.NUM_AUDIO_INPUT,self.NUM_AUDIO_FEATURES))\n",
    "        self.videoBranch = self._createVideoBranch()\n",
    "        self.video_dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=(self.NUM_VID_FEATURES+self.NUM_AUDIO_FEATURES),\n",
    "            hidden_size=self.NUM_LSTM_HIDDEN,\n",
    "            num_layers=1,\n",
    "            bias=True,\n",
    "            batch_first=True # input and output tensors provided as (batch, seq, feature)\n",
    "            # can add dropout later\n",
    "            )\n",
    "        self.fc = nn.Linear(self.NUM_LSTM_HIDDEN,self.NUM_CLASS)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.avg = nn.AvgPool1d(self.NUM_PARTITIONS,self.NUM_PARTITIONS)\n",
    "\n",
    "    def _createVideoBranch(self):\n",
    "        model_pretrained = torchvision.models.resnet34(pretrained=True)\n",
    "        # All of the parameters are freezed, not to change (newly constructed layers' params won't be influenced)\n",
    "        for param in model_pretrained.parameters():\n",
    "            param.requires_grad = False   \n",
    "        model_pretrained.fc = nn.Linear(model_pretrained.fc.in_features, self.NUM_VID_FEATURES)\n",
    "        return model_pretrained\n",
    "    \n",
    "    def forward(self, x):\n",
    "        videoData = x[0].view(-1,self.NUM_CHANNEL,self.NUM_IMG_SIZE,self.NUM_IMG_SIZE)\n",
    "        audioData = x[1].view(-1,self.NUM_AUDIO_INPUT)\n",
    "        videoProcessed = self.videoBranch(videoData).view(-1,self.NUM_PARTITIONS,self.NUM_VID_FEATURES) # will output a (n x partitions)x 32 tensor\n",
    "        videoProcessed = self.video_dropout(videoProcessed)\n",
    "        audioProcessed = self.audioBranch(audioData).view(-1,self.NUM_PARTITIONS,self.NUM_AUDIO_FEATURES)# will output a (n x partitions)x 128 tensor\n",
    "        x = torch.cat((videoProcessed, audioProcessed), 2).type(gpu_dtype) #(N,10,160)\n",
    "        h0 = Variable(torch.zeros(1, x.size()[0], self.NUM_LSTM_HIDDEN)).type(gpu_dtype)\n",
    "        c0 = Variable(torch.zeros(1, x.size()[0], self.NUM_LSTM_HIDDEN)).type(gpu_dtype)\n",
    "        x,cn = self.lstm(x, (h0, c0))\n",
    "        x = x.contiguous().view(-1,self.NUM_LSTM_HIDDEN)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x) #(N*P,5)\n",
    "        x = self.avg(x.view(-1,self.NUM_PARTITIONS,self.NUM_CLASS).transpose(1,2)).squeeze() #(N,5)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some gpu configs\n",
    "use_gpu = True\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "\n",
    "def get_learnable_params(m,verbose = 0):\n",
    "    ret = []\n",
    "    for l in m.parameters():\n",
    "        if l.requires_grad == True:\n",
    "            ret.append(l)\n",
    "            if verbose == 1:\n",
    "                print (l.size())\n",
    "            if verbose == 2:\n",
    "                print (l)\n",
    "    return ret\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"Saves checkpoint to disk\"\"\"\n",
    "    directory = \"resnet_for_rnn/%s/\"%(exp_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filename = directory + filename\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'resnet_for_rnn/%s/'%(exp_name) + 'model_best.pth.tar')\n",
    "\n",
    "def log_value(to_log, log_path = './log_'+ exp_name + '.txt'):\n",
    "    log_file = open(log_path, 'a+')\n",
    "    log_file.write(to_log)\n",
    "    log_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_freq = 50\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch) :\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        input_image,input_audio, target = data\n",
    "        input_image_var, input_audio_var,target_var = Variable(input_image.type(gpu_dtype)), \\\n",
    "            Variable(input_audio.type(gpu_dtype)),Variable(target.type(gpu_dtype))\n",
    "        # compute output\n",
    "        output = model([input_image_var,input_audio_var])\n",
    "        loss = criterion(output, target_var)\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data[0], input_image.size(0))\n",
    "        # compute gradient and do Adam step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i % log_freq == 0:\n",
    "            to_log = 'Epoch: [{0}][{1}/{2}]\\t Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t Loss {loss.val:f} ({loss.avg:f})\\n'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time, loss=losses)\n",
    "            log_value(to_log)\n",
    "            print(to_log)\n",
    "            \n",
    "       \n",
    "    return losses.avg\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    \"\"\"Perform validation on the validation set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_E = AverageMeter()\n",
    "    losses_N = AverageMeter()\n",
    "    losses_A= AverageMeter()\n",
    "    losses_C= AverageMeter()\n",
    "    losses_O= AverageMeter()\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, data in enumerate(val_loader):\n",
    "        input_image,input_audio, target = data\n",
    "        input_image_var, input_audio_var,target_var = Variable(input_image.type(gpu_dtype)), \\\n",
    "        Variable(input_audio.type(gpu_dtype)),Variable(target.type(gpu_dtype))\n",
    "        # compute output\n",
    "        output = model([input_image_var,input_audio_var])\n",
    "           \n",
    "        loss = criterion(output, target_var)\n",
    "        loss_E= criterion(output[:,0], target_var[:,0])\n",
    "        loss_N= criterion(output[:,1], target_var[:,1])\n",
    "        loss_A= criterion(output[:,2], target_var[:,2])\n",
    "        loss_C= criterion(output[:,3], target_var[:,3])\n",
    "        loss_O= criterion(output[:,4], target_var[:,4])\n",
    "        \n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data[0], input_image.size(0))\n",
    "        \n",
    "        losses_E.update(loss_E.data[0], input_image.size(0))\n",
    "        losses_N.update(loss_N.data[0], input_image.size(0))\n",
    "        losses_A.update(loss_A.data[0], input_image.size(0))\n",
    "        losses_C.update(loss_C.data[0], input_image.size(0))\n",
    "        losses_O.update(loss_O.data[0], input_image.size(0))\n",
    "        ocean_losses = [losses_O.avg,losses_C.avg,losses_E.avg,losses_A.avg,losses_N.avg]\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i %  log_freq == 0:\n",
    "            \n",
    "            to_log = 'Val/Test: [{0}/{1}]\\t Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t Loss {loss.val:f} ({loss.avg:f})\\t Loss_O {losses_O.val:f} ({losses_O.avg:f}) \\t Loss_C {losses_C.val:f} ({losses_C.avg:f})\\t Loss_E {losses_E.val:f} ({losses_E.avg:f})\\t Loss_A {losses_A.val:f} ({losses_A.avg:f})\\t Loss_N {losses_N.val:f} ({losses_N.avg:f})\\n'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses,losses_O=losses_O,losses_C=losses_C,losses_E=losses_E,losses_A=losses_A,losses_N=losses_N)\n",
    "            log_value(to_log)\n",
    "            print(to_log)\n",
    "                     \n",
    "    return losses.avg, output,input_image,target,ocean_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: for larger dataset, consider a step function or exponentialdecay\n",
    "def lr_scheduler(optimizer, epoch):\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# can retriecve these latest val results for plotting\n",
    "val_preds,val_imgs,val_targets = None,None,None\n",
    "\n",
    "def train_model(startModel=None, startEpoch=0, numEpochs=10):    \n",
    "    model = AudioVisualLSTM().type(gpu_dtype)\n",
    "    #del model_base\n",
    "\n",
    "    #  changed to l1 loss to reflect competition \n",
    "    criterion = nn.L1Loss().type(gpu_dtype)\n",
    "\n",
    "    #only optimizing the new_fc layer parameters, other pretrained weights are freezed¶\n",
    "    optimizer  = optim.SGD(get_learnable_params(model),lr=1e-7, momentum=0.9,weight_decay=5e-3)\n",
    "\n",
    "    best_loss = 1000 # will get overwritten\n",
    "    \n",
    "    if(startModel != None):\n",
    "        print(\"=> loading checkpoint '{}'\".format(startModel))\n",
    "        checkpoint = torch.load(startModel)\n",
    "        startEpoch = checkpoint['epoch']\n",
    "        best_loss = checkpoint['best_loss'] # for now because old loss is stale\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        # todo - figure out why not working\n",
    "        #optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        # print(optimizer.param_groups)\n",
    "        \n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(startModel, checkpoint['epoch']))\n",
    "    #else:\n",
    "        # benchmark the model \n",
    "        #best_loss = validate(dset_loaders['val'], model, criterion, startEpoch)\n",
    "        \n",
    "    bestModel = model\n",
    "\n",
    "    for epoch in range(startEpoch,startEpoch+numEpochs):\n",
    "        # train for one epoch\n",
    "        train_loss = train(dset_loaders['train'], model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        val_loss,val_preds,val_imgs,val_targets,val_ocean_loss = validate(dset_loaders['val'], model, criterion, epoch)\n",
    "\n",
    "        # log \n",
    "        log_value('Epoch: [{0}]\\t Train Loss: {train_loss:f}  \\t Val Loss: {val_loss:f} Loss_O: {Loss_O:f} Loss_C: {Loss_C:f} Loss_E: {Loss_E:f} Loss_A: {Loss_A:f} Loss_N: {Loss_N:f}\\n'.format(epoch,\\\n",
    "                    train_loss=train_loss,val_loss=val_loss,Loss_O=val_ocean_loss[0],Loss_C=val_ocean_loss[1],Loss_E=val_ocean_loss[2],Loss_A=val_ocean_loss[3],Loss_N=val_ocean_loss[4]),'./%s_epoch_log.txt'%exp_name)\n",
    "        print('Epoch: [{0}]\\t Train Loss: {train_loss:f}  \\t Val Loss: {val_loss:f} Loss_O: {Loss_O:f} Loss_C: {Loss_C:f} Loss_E: {Loss_E:f} Loss_A: {Loss_A:f} Loss_N: {Loss_N:f}\\n'.format(epoch,\\\n",
    "                    train_loss=train_loss,val_loss=val_loss,Loss_O=val_ocean_loss[0],Loss_C=val_ocean_loss[1],Loss_E=val_ocean_loss[2],Loss_A=val_ocean_loss[3],Loss_N=val_ocean_loss[4]))\n",
    "\n",
    "        # remember best loss and save checkpoint\n",
    "        is_best = val_loss <= best_loss\n",
    "        best_loss = min(val_loss, best_loss)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': 'resnet_LSTM_l1',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print (is_best, best_loss)\n",
    "\n",
    "    print ('Best Loss: ', best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Start to train a new model run this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train from the latest model checpoint run this cell, specify startepoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint './resnet_for_rnn/ResNet_LSTM_L1/checkpoint.pth.tar'\n",
      "=> loaded checkpoint './resnet_for_rnn/ResNet_LSTM_L1/checkpoint.pth.tar' (epoch 238)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /b/wheel/pytorch-src/torch/lib/THC/generic/THCStorage.cu:66",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6d239c3e3eff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstartModel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./resnet_for_rnn/ResNet_LSTM_L1/checkpoint.pth.tar'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstartModel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstartModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstartEpoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mnumEpochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-d72b766b9090>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(startModel, startEpoch, numEpochs)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstartEpoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstartEpoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnumEpochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# train for one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdset_loaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m# evaluate on validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-6927aab2a2dd>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0minput_image_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_audio_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpu_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m             \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_audio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpu_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpu_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# compute output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_image_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_audio_var\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# measure accuracy and record loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-24edc8d297ac>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mvideoData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_CHANNEL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_IMG_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_IMG_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0maudioData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_AUDIO_INPUT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mvideoProcessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideoBranch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideoData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_PARTITIONS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_VID_FEATURES\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# will output a (n x partitions)x 32 tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mvideoProcessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_dropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideoProcessed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0maudioProcessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maudioBranch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudioData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_PARTITIONS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_AUDIO_FEATURES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# will output a (n x partitions)x 128 tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 237\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[0;32m     37\u001b[0m     f = ConvNd(_pair(stride), _pair(padding), _pair(dilation), False,\n\u001b[0;32m     38\u001b[0m                _pair(0), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /b/wheel/pytorch-src/torch/lib/THC/generic/THCStorage.cu:66"
     ]
    }
   ],
   "source": [
    "startModel='./resnet_for_rnn/ResNet_LSTM_L1/checkpoint.pth.tar'\n",
    "train_model(startModel=startModel, startEpoch=1 , numEpochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# visualize the pred and groundtruth and images of the latest val batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "val_pred = (val_preds.data).cpu().numpy()\n",
    "val_gt = (Variable(val_targets).data).cpu().numpy()\n",
    "# select a video in batch to see: idx can be 0-7\n",
    "idx = 7\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(np.arange(5),val_gt[idx])\n",
    "plt.xticks(np.arange(5),['e','n','a','c','o'])\n",
    "plt.gca().set_ylim([0.0,1.0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(np.arange(5),val_pred[idx])\n",
    "plt.xticks(np.arange(5),['e','n','a','c','o'])\n",
    "plt.gca().set_ylim([0.0,1.0])\n",
    "plt.show()\n",
    "val_unflattened_sample = val_imgs.view(-1,3,256,256)\n",
    "# Make a grid from batch\n",
    "plt.figure( figsize=(30, 3))\n",
    "out = torchvision.utils.make_grid(val_unflattened_sample[idx*10:idx*10+10],nrow=10)\n",
    "imshow(out, title='trainning sample')\n",
    "#plt.savefig('train_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

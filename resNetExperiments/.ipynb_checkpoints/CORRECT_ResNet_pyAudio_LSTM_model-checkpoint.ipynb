{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_dir = '/home/noa_glaser/dataBig/train-1-2-3-4/train'\n",
    "val_dataset_dir = '/home/noa_glaser/dataBig/train-1-2-3-4/val'\n",
    "audio_dataset_dir = '/home/noa_glaser/dataBig/train-1-2-3-4-audio'\n",
    "label_dataset_dir = train_dataset_dir\n",
    "exp_name = 'avepool_dropout_ResNet34_LSTM_experiment' # roughly 3K videos\n",
    "num_classes = 5 \n",
    "num_partition = 10\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import time\n",
    "import copy\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first, get the Images (path) and their labels (five personality traits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_img_audio_label(dataset_dir,audio_dataset_dir,label_dataset_dir):\n",
    "    \"\"\"Returns a list of np.array(img_paths), np.array(audio_paths),\n",
    "        np.array(labels), np.array(raw_movienames)\n",
    "    Args:\n",
    "    dataset for video, for audio_feats from pyAudioAnalysis, labels\n",
    "    \"\"\"\n",
    " \n",
    "    print(\"processing dataset: \"+ dataset_dir)\n",
    "    img_paths = [] \n",
    "    audio_paths=[]\n",
    "    raw_movienames = []\n",
    "    labels = []\n",
    "\n",
    "    annotaion_filename = label_dataset_dir + \"/annotation_training.pkl\"\n",
    "    \n",
    "    with open(annotaion_filename, 'rb') as f:\n",
    "        label_dicts = pickle.load(f, encoding='latin1') \n",
    "\n",
    "    for movie in os.listdir(dataset_dir):\n",
    "        fileEnding ='_50uniform' #TODO: figure out how to make more general\n",
    "        if fileEnding not in movie: continue #skip non-movie files\n",
    "        raw_moviename = movie.replace(fileEnding,'.mp4')      \n",
    "        big_five = [label_dicts['extraversion'][raw_moviename], \n",
    "                    label_dicts['neuroticism'][raw_moviename],\n",
    "                    label_dicts['agreeableness'][raw_moviename],\n",
    "                    label_dicts['conscientiousness'][raw_moviename],\n",
    "                    label_dicts['openness'][raw_moviename] ]\n",
    "                    #label_dicts['interview'][raw_moviename]]      \n",
    "        movie_path = os.path.join(dataset_dir, movie)\n",
    "        mv_partitions = []\n",
    "        p = 0\n",
    "        all_imgs = os.listdir(movie_path)\n",
    "        assert(len(all_imgs) >= num_partition)\n",
    "        opened = True\n",
    "        for i in range(num_partition):\n",
    "            path = os.path.join(movie_path, all_imgs[i])\n",
    "            try:\n",
    "                open(path)\n",
    "            except:\n",
    "                print('image failed to open',path)\n",
    "                opened = False\n",
    "                \n",
    "            mv_partitions.append(path)\n",
    "        assert(len(mv_partitions)==num_partition)\n",
    "        \n",
    "        \n",
    "        audiofeat_path = os.path.join(audio_dataset_dir,raw_moviename+'.wav.csv')\n",
    "        try:\n",
    "            open(audiofeat_path)\n",
    "        except:\n",
    "            print('audio failed to open',path)\n",
    "            opened = False\n",
    "        if opened :\n",
    "            img_paths.append(mv_partitions)\n",
    "            audio_paths.append(audiofeat_path)\n",
    "            raw_movienames.append(raw_moviename)\n",
    "            labels.append(big_five)\n",
    "            \n",
    "    \n",
    "    return np.array(img_paths),np.array(audio_paths),\\\n",
    "                np.array(labels), np.array(raw_movienames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## use this if we have seperated train/val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dataset: /home/noa_glaser/dataBig/train-1-2-3-4/train\n",
      "processing dataset: /home/noa_glaser/dataBig/train-1-2-3-4/val\n"
     ]
    }
   ],
   "source": [
    "train_img_paths,train_audio_paths, train_labels, train_movienames \\\n",
    "        = get_img_audio_label(train_dataset_dir,audio_dataset_dir,label_dataset_dir) \n",
    "val_img_paths,val_audio_paths, val_labels, val_movienames \\\n",
    "= get_img_audio_label(val_dataset_dir,audio_dataset_dir,label_dataset_dir)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Loader. Data is normalized before feeding into model (as required by the pretrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def default_img_loader(img_paths,transform):\n",
    "    ten_img_tensor = []\n",
    "    for path in img_paths:\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if transform is not None:\n",
    "            img = transform(img)\n",
    "        ten_img_tensor.append(img)\n",
    "        \n",
    "    return torch.cat(ten_img_tensor)\n",
    "        \n",
    "\n",
    "def default_audio_loader(path):\n",
    "\treturn np.loadtxt(path,delimiter=',')\n",
    "\n",
    "class VisualAudio(data.Dataset):\n",
    "    def __init__(self,split,img_paths,audio_paths, movie_names,labels,transform=None,\n",
    "                 img_loader=default_img_loader,audio_loader=default_audio_loader):\n",
    "        self.split = split \n",
    "        self.img_paths = img_paths\n",
    "        self.audio_paths = audio_paths\n",
    "        self.movie_names = movie_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.img_loader=img_loader\n",
    "        self.audio_loader= audio_loader\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_paths, audio_paths,target = self.img_paths[index], \\\n",
    "                                        self.audio_paths[index], self.labels[index]\n",
    "        ten_img_tensor = self.img_loader(img_paths,self.transform)\n",
    "        ten_audio = self.audio_loader(audio_paths)\n",
    "        #return 30x224x224 , 10x68, 10 x 5\n",
    "        \n",
    "        assert(ten_img_tensor.size() == (30,256,256))\n",
    "        \n",
    "        return ten_img_tensor, ten_audio[:10,:], target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 3072, 'val': 768}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import  transforms\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.RandomCrop(256),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "        \n",
    "dsets = {}\n",
    "dsets['train'] = VisualAudio('train',train_img_paths,train_audio_paths,\\\n",
    "                    train_movienames ,train_labels,transform=data_transforms['train'] )\n",
    "dsets['val'] = VisualAudio('val',val_img_paths,val_audio_paths,\\\n",
    "                         val_movienames,val_labels,transform=data_transforms['val'] )\n",
    "\n",
    "dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=1) for x in ['train', 'val']}\n",
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "dset_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Some dataset examples (each batch is 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    \n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "#train_imgsamples,train_audiosamle,train_labelsample = next(iter(dset_loaders['train']))\n",
    "\n",
    "#train_unflattened_sample = train_imgsamples.view(-1,3,256,256)\n",
    "# Make a grid from batch\n",
    "#plt.figure( figsize=(10, 16))\n",
    "#out = torchvision.utils.make_grid(train_unflattened_sample,nrow=10)\n",
    "#imshow(out, title='trainning sample')\n",
    "#plt.savefig('train_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#val_imgsamples,val_audiosamle,val_labelsample = next(iter(dset_loaders['val']))\n",
    "#val_unflattened_sample = val_imgsamples.view(-1,3,256,256)\n",
    "### Make a grid from val batch\n",
    "#plt.figure( figsize=(10, 16))\n",
    "#out2 = torchvision.utils.make_grid(val_unflattened_sample,nrow=10)\n",
    "#imshow(out2, title='validation sample')\n",
    "#plt.savefig('val_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AudioVisualLSTM(nn.Module):\n",
    "    NUM_AUDIO_INPUT = 68\n",
    "    NUM_VID_FEATURES = 128\n",
    "    NUM_AUDIO_FEATURES = 32\n",
    "    NUM_LSTM_HIDDEN = 128\n",
    "    NUM_PARTITIONS = 10\n",
    "    NUM_CLASS = 5\n",
    "    NUM_IMG_SIZE = 256\n",
    "    NUM_CHANNEL = 3\n",
    "    \n",
    "    def __init__(self):        \n",
    "        super(AudioVisualLSTM, self).__init__()\n",
    "        self.audioBranch =  nn.Sequential(nn.Linear(self.NUM_AUDIO_INPUT,self.NUM_AUDIO_FEATURES))\n",
    "        self.videoBranch = self._createVideoBranch()\n",
    "        self.video_dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=(self.NUM_VID_FEATURES+self.NUM_AUDIO_FEATURES),\n",
    "            hidden_size=self.NUM_LSTM_HIDDEN,\n",
    "            num_layers=1,\n",
    "            bias=True,\n",
    "            batch_first=True # input and output tensors provided as (batch, seq, feature)\n",
    "            # can add dropout later\n",
    "            )\n",
    "        self.fc = nn.Linear(self.NUM_LSTM_HIDDEN,self.NUM_CLASS)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.avg = nn.AvgPool1d(self.NUM_PARTITIONS,self.NUM_PARTITIONS)\n",
    "\n",
    "    def _createVideoBranch(self):\n",
    "        model_pretrained = torchvision.models.resnet34(pretrained=True)\n",
    "        # All of the parameters are freezed, not to change (newly constructed layers' params won't be influenced)\n",
    "        for param in model_pretrained.parameters():\n",
    "            param.requires_grad = False   \n",
    "        model_pretrained.fc = nn.Linear(model_pretrained.fc.in_features, self.NUM_VID_FEATURES)\n",
    "        return model_pretrained\n",
    "    \n",
    "    def forward(self, x):\n",
    "        videoData = x[0].view(-1,self.NUM_CHANNEL,self.NUM_IMG_SIZE,self.NUM_IMG_SIZE)\n",
    "        audioData = x[1].view(-1,self.NUM_AUDIO_INPUT)\n",
    "        videoProcessed = self.videoBranch(videoData).view(-1,self.NUM_PARTITIONS,self.NUM_VID_FEATURES) # will output a (n x partitions)x 32 tensor\n",
    "        videoProcessed = self.video_dropout(videoProcessed)\n",
    "        audioProcessed = self.audioBranch(audioData).view(-1,self.NUM_PARTITIONS,self.NUM_AUDIO_FEATURES)# will output a (n x partitions)x 128 tensor\n",
    "        x = torch.cat((videoProcessed, audioProcessed), 2).type(gpu_dtype) #(N,10,160)\n",
    "        h0 = Variable(torch.zeros(1, x.size()[0], self.NUM_LSTM_HIDDEN)).type(gpu_dtype)\n",
    "        c0 = Variable(torch.zeros(1, x.size()[0], self.NUM_LSTM_HIDDEN)).type(gpu_dtype)\n",
    "        x,cn = self.lstm(x, (h0, c0))\n",
    "        x = x.contiguous().view(-1,self.NUM_LSTM_HIDDEN)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x) #(N*P,5)\n",
    "        x = self.avg(x.view(-1,self.NUM_PARTITIONS,self.NUM_CLASS).transpose(1,2)).squeeze() #(N,5)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some gpu configs\n",
    "use_gpu = True\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "\n",
    "def get_learnable_params(m,verbose = 0):\n",
    "    ret = []\n",
    "    for l in m.parameters():\n",
    "        if l.requires_grad == True:\n",
    "            ret.append(l)\n",
    "            if verbose == 1:\n",
    "                print (l.size())\n",
    "            if verbose == 2:\n",
    "                print (l)\n",
    "    return ret\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"Saves checkpoint to disk\"\"\"\n",
    "    directory = \"resnet_for_rnn/%s/\"%(exp_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filename = directory + filename\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'resnet_for_rnn/%s/'%(exp_name) + 'model_best.pth.tar')\n",
    "\n",
    "def log_value(to_log, log_path = './log_'+ exp_name + '.txt'):\n",
    "    log_file = open(log_path, 'a+')\n",
    "    log_file.write(to_log)\n",
    "    log_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_freq = 10\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch) :\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        input_image,input_audio, target = data\n",
    "        input_image_var, input_audio_var,target_var = Variable(input_image.type(gpu_dtype)), \\\n",
    "            Variable(input_audio.type(gpu_dtype)),Variable(target.type(gpu_dtype))\n",
    "        # compute output\n",
    "        output = model([input_image_var,input_audio_var])\n",
    "        loss = criterion(output, target_var)\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data[0], input_image.size(0))\n",
    "        # compute gradient and do Adam step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i % log_freq == 0:\n",
    "            to_log = 'Epoch: [{0}][{1}/{2}]\\t Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t Loss {loss.val:f} ({loss.avg:f})\\n'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time, loss=losses)\n",
    "            log_value(to_log)\n",
    "            print(to_log)\n",
    "            \n",
    "       \n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    \"\"\"Perform validation on the validation set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, data in enumerate(val_loader):\n",
    "        input_image,input_audio, target = data\n",
    "        input_image_var, input_audio_var,target_var = Variable(input_image.type(gpu_dtype)), \\\n",
    "        Variable(input_audio.type(gpu_dtype)),Variable(target.type(gpu_dtype))\n",
    "        # compute output\n",
    "        output = model([input_image_var,input_audio_var])\n",
    "        loss = criterion(output, target_var)\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data[0], input_image.size(0))\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i %  log_freq == 0:\n",
    "            to_log = 'Val/Test: [{0}/{1}]\\t Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t Loss {loss.val:f} ({loss.avg:f})\\n'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses)\n",
    "            log_value(to_log)\n",
    "            print(to_log)\n",
    "                        \n",
    "    return losses.avg, output,input_image,target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: for larger dataset, consider a step function or exponentialdecay\n",
    "def lr_scheduler(optimizer, epoch):\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# can retriecve these latest val results for plotting\n",
    "val_preds,val_imgs,val_targets = None,None,None\n",
    "\n",
    "\n",
    "\n",
    "def train_model(startModel=None, startEpoch=0, numEpochs=10):    \n",
    "    model = AudioVisualLSTM().type(gpu_dtype)\n",
    "    #del model_base\n",
    "\n",
    "    #  changed to l1 loss to reflect competition \n",
    "    criterion = nn.L1Loss().type(gpu_dtype)\n",
    "\n",
    "    #only optimizing the new_fc layer parameters, other pretrained weights are freezedÂ¶\n",
    "    optimizer  = optim.SGD(get_learnable_params(model),lr=5e-2, momentum=0.9,weight_decay=5e-4)\n",
    "\n",
    "    best_loss = 1000 # will get overwritten\n",
    "    \n",
    "    if(startModel != None):\n",
    "        print(\"=> loading checkpoint '{}'\".format(startModel))\n",
    "        checkpoint = torch.load(startModel)\n",
    "        startEpoch = checkpoint['epoch']\n",
    "        best_loss = checkpoint['best_loss'] # for now because old loss is stale\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        # todo - figure out why not working\n",
    "        #optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        # print(optimizer.param_groups)\n",
    "        \n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(startModel, checkpoint['epoch']))\n",
    "    #else:\n",
    "        # benchmark the model \n",
    "        #best_loss = validate(dset_loaders['val'], model, criterion, startEpoch)\n",
    "        \n",
    "    bestModel = model\n",
    "\n",
    "    for epoch in range(startEpoch,startEpoch+numEpochs):\n",
    "        # train for one epoch\n",
    "        train_loss = train(dset_loaders['train'], model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        val_loss,val_preds,val_imgs,val_targets = validate(dset_loaders['val'], model, criterion, epoch)\n",
    "\n",
    "        # log \n",
    "        log_value('Epoch: [{0}]\\t Train Loss: {train_loss:f}  \\t Val Loss: {val_loss:f}\\n'.format(epoch,\\\n",
    "                                        train_loss=train_loss,val_loss=val_loss),'./%s_epoch_log.txt'%exp_name)\n",
    "        print('Epoch: [{0}]\\t Train Loss: {train_loss:f}  \\t Val Loss: {val_loss:f}\\n'.format(epoch,\\\n",
    "                    train_loss=train_loss,val_loss=val_loss))\n",
    "\n",
    "        # remember best loss and save checkpoint\n",
    "        is_best = val_loss <= best_loss\n",
    "        best_loss = min(val_loss, best_loss)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': 'resnet34_only',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print (is_best, best_loss)\n",
    "\n",
    "    print ('Best Loss: ', best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Start to train a new model run this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/384]\t Time 1.140 (1.140)\t Loss 0.083843 (0.083843)\n",
      "\n",
      "Epoch: [0][10/384]\t Time 0.697 (0.737)\t Loss 0.152954 (0.131492)\n",
      "\n",
      "Epoch: [0][20/384]\t Time 0.701 (0.719)\t Loss 0.100186 (0.120348)\n",
      "\n",
      "Epoch: [0][30/384]\t Time 0.690 (0.711)\t Loss 0.145643 (0.119391)\n",
      "\n",
      "Epoch: [0][40/384]\t Time 0.693 (0.707)\t Loss 0.147308 (0.121827)\n",
      "\n",
      "Epoch: [0][50/384]\t Time 0.698 (0.704)\t Loss 0.147699 (0.124720)\n",
      "\n",
      "Epoch: [0][60/384]\t Time 0.699 (0.703)\t Loss 0.110416 (0.123897)\n",
      "\n",
      "Epoch: [0][70/384]\t Time 0.692 (0.702)\t Loss 0.106291 (0.121730)\n",
      "\n",
      "Epoch: [0][80/384]\t Time 0.691 (0.701)\t Loss 0.111205 (0.122481)\n",
      "\n",
      "Epoch: [0][90/384]\t Time 0.697 (0.700)\t Loss 0.119490 (0.121959)\n",
      "\n",
      "Epoch: [0][100/384]\t Time 0.693 (0.699)\t Loss 0.127686 (0.120714)\n",
      "\n",
      "Epoch: [0][110/384]\t Time 0.694 (0.699)\t Loss 0.146491 (0.120203)\n",
      "\n",
      "Epoch: [0][120/384]\t Time 0.702 (0.698)\t Loss 0.144324 (0.120296)\n",
      "\n",
      "Epoch: [0][130/384]\t Time 0.691 (0.698)\t Loss 0.130449 (0.120373)\n",
      "\n",
      "Epoch: [0][140/384]\t Time 0.690 (0.697)\t Loss 0.112652 (0.120734)\n",
      "\n",
      "Epoch: [0][150/384]\t Time 0.691 (0.697)\t Loss 0.104000 (0.120607)\n",
      "\n",
      "Epoch: [0][160/384]\t Time 0.696 (0.697)\t Loss 0.143367 (0.120585)\n",
      "\n",
      "Epoch: [0][170/384]\t Time 0.692 (0.697)\t Loss 0.108553 (0.120116)\n",
      "\n",
      "Epoch: [0][180/384]\t Time 0.688 (0.696)\t Loss 0.099654 (0.120366)\n",
      "\n",
      "Epoch: [0][190/384]\t Time 0.703 (0.696)\t Loss 0.172518 (0.119756)\n",
      "\n",
      "Epoch: [0][200/384]\t Time 0.706 (0.696)\t Loss 0.111603 (0.120045)\n",
      "\n",
      "Epoch: [0][210/384]\t Time 0.699 (0.696)\t Loss 0.101449 (0.119372)\n",
      "\n",
      "Epoch: [0][220/384]\t Time 0.698 (0.697)\t Loss 0.097273 (0.119507)\n",
      "\n",
      "Epoch: [0][230/384]\t Time 0.693 (0.697)\t Loss 0.128660 (0.118937)\n",
      "\n",
      "Epoch: [0][240/384]\t Time 0.697 (0.697)\t Loss 0.117714 (0.118293)\n",
      "\n",
      "Epoch: [0][250/384]\t Time 0.691 (0.696)\t Loss 0.162158 (0.118049)\n",
      "\n",
      "Epoch: [0][260/384]\t Time 0.694 (0.696)\t Loss 0.104959 (0.117732)\n",
      "\n",
      "Epoch: [0][270/384]\t Time 0.699 (0.696)\t Loss 0.112961 (0.117788)\n",
      "\n",
      "Epoch: [0][280/384]\t Time 0.699 (0.696)\t Loss 0.175608 (0.117734)\n",
      "\n",
      "Epoch: [0][290/384]\t Time 0.703 (0.696)\t Loss 0.101081 (0.117623)\n",
      "\n",
      "Epoch: [0][300/384]\t Time 0.701 (0.697)\t Loss 0.104743 (0.117454)\n",
      "\n",
      "Epoch: [0][310/384]\t Time 0.697 (0.697)\t Loss 0.132737 (0.117337)\n",
      "\n",
      "Epoch: [0][320/384]\t Time 0.698 (0.697)\t Loss 0.091351 (0.117377)\n",
      "\n",
      "Epoch: [0][330/384]\t Time 0.694 (0.697)\t Loss 0.113460 (0.117564)\n",
      "\n",
      "Epoch: [0][340/384]\t Time 0.697 (0.697)\t Loss 0.122254 (0.117629)\n",
      "\n",
      "Epoch: [0][350/384]\t Time 0.698 (0.697)\t Loss 0.140947 (0.117589)\n",
      "\n",
      "Epoch: [0][360/384]\t Time 0.702 (0.697)\t Loss 0.074880 (0.117373)\n",
      "\n",
      "Epoch: [0][370/384]\t Time 0.698 (0.697)\t Loss 0.097011 (0.117607)\n",
      "\n",
      "Epoch: [0][380/384]\t Time 0.694 (0.697)\t Loss 0.118862 (0.117775)\n",
      "\n",
      "Val/Test: [0/96]\t Time 1.112 (1.112)\t Loss 0.093284 (0.093284)\n",
      "\n",
      "Val/Test: [10/96]\t Time 0.660 (0.712)\t Loss 0.104147 (0.111341)\n",
      "\n",
      "Val/Test: [20/96]\t Time 0.663 (0.689)\t Loss 0.139780 (0.109908)\n",
      "\n",
      "Val/Test: [30/96]\t Time 0.668 (0.681)\t Loss 0.112432 (0.112544)\n",
      "\n",
      "Val/Test: [40/96]\t Time 0.668 (0.677)\t Loss 0.100922 (0.113793)\n",
      "\n",
      "Val/Test: [50/96]\t Time 0.665 (0.674)\t Loss 0.106488 (0.113372)\n",
      "\n",
      "Val/Test: [60/96]\t Time 1.255 (0.718)\t Loss 0.098957 (0.113708)\n",
      "\n",
      "Val/Test: [70/96]\t Time 1.242 (0.794)\t Loss 0.137708 (0.113424)\n",
      "\n",
      "Val/Test: [80/96]\t Time 1.254 (0.851)\t Loss 0.105601 (0.113265)\n",
      "\n",
      "Val/Test: [90/96]\t Time 1.252 (0.895)\t Loss 0.128402 (0.112947)\n",
      "\n",
      "Epoch: [0]\t Train Loss: 0.117682  \t Val Loss: 0.112675\n",
      " ./avepool_dropout_ResNet34_LSTM_experiment_epoch_log.txt\n",
      "True 0.11267467386399706\n",
      "Best Loss:  0.11267467386399706\n"
     ]
    }
   ],
   "source": [
    "train_model(startModel=None, startEpoch=0, numEpochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train from the latest model checpoint run this cell, specify startepoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "startModel='./resnet_for_rnn/avepool_dropout_ResNet34_LSTM_experiment/checkpoint.pth.tar'\n",
    "train_model(startModel=startModel, startEpoch=1 , numEpochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# visualize the pred and groundtruth and images of the latest val batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "val_pred = (val_preds.data).cpu().numpy()\n",
    "val_gt = (Variable(val_targets).data).cpu().numpy()\n",
    "# select a video in batch to see: idx can be 0-7\n",
    "idx = 7\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(np.arange(5),val_gt[idx])\n",
    "plt.xticks(np.arange(5),['e','n','a','c','o'])\n",
    "plt.gca().set_ylim([0.0,1.0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(np.arange(5),val_pred[idx])\n",
    "plt.xticks(np.arange(5),['e','n','a','c','o'])\n",
    "plt.gca().set_ylim([0.0,1.0])\n",
    "plt.show()\n",
    "val_unflattened_sample = val_imgs.view(-1,3,256,256)\n",
    "# Make a grid from batch\n",
    "plt.figure( figsize=(30, 3))\n",
    "out = torchvision.utils.make_grid(val_unflattened_sample[idx*10:idx*10+10],nrow=10)\n",
    "imshow(out, title='trainning sample')\n",
    "#plt.savefig('train_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_dir = '/home/noa_glaser/proj/data/train-frames/train'\n",
    "val_dataset_dir = '/home/noa_glaser/proj/data/train-frames/val'\n",
    "audio_dataset_dir = '/home/noa_glaser/proj/data/train-audio'\n",
    "label_dataset_dir = '/home/noa_glaser/proj/data/'\n",
    "exp_name = 'changeEval' # roughly 3K videos\n",
    "num_classes = 5 \n",
    "num_partition = 10\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import time\n",
    "import copy\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first, get the Images (path) and their labels (five personality traits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_img_audio_label(dataset_dir,audio_dataset_dir,label_dataset_dir):\n",
    "    \"\"\"Returns a list of np.array(img_paths), np.array(audio_paths),\n",
    "        np.array(labels), np.array(raw_movienames)\n",
    "    Args:\n",
    "    dataset for video, for audio_feats from pyAudioAnalysis, labels\n",
    "    \"\"\"\n",
    " \n",
    "    print(\"processing dataset: \"+ dataset_dir)\n",
    "    img_paths = [] \n",
    "    audio_paths=[]\n",
    "    raw_movienames = []\n",
    "    labels = []\n",
    "\n",
    "    annotaion_filename = label_dataset_dir + \"/annotation_training.pkl\"\n",
    "    \n",
    "    with open(annotaion_filename, 'rb') as f:\n",
    "        label_dicts = pickle.load(f, encoding='latin1') \n",
    "\n",
    "    for movie in os.listdir(dataset_dir):\n",
    "        fileEnding ='_50uniform' #TODO: figure out how to make more general\n",
    "        if fileEnding not in movie: continue #skip non-movie files\n",
    "        raw_moviename = movie.replace(fileEnding,'.mp4')      \n",
    "        big_five = [label_dicts['extraversion'][raw_moviename], \n",
    "                    label_dicts['neuroticism'][raw_moviename],\n",
    "                    label_dicts['agreeableness'][raw_moviename],\n",
    "                    label_dicts['conscientiousness'][raw_moviename],\n",
    "                    label_dicts['openness'][raw_moviename] ]\n",
    "                    #label_dicts['interview'][raw_moviename]]      \n",
    "        movie_path = os.path.join(dataset_dir, movie)\n",
    "        mv_partitions = []\n",
    "        p = 0\n",
    "        all_imgs = os.listdir(movie_path)\n",
    "        assert(len(all_imgs) >= num_partition)\n",
    "        opened = True\n",
    "        for i in range(num_partition):\n",
    "            path = os.path.join(movie_path, all_imgs[i])\n",
    "            try:\n",
    "                open(path)\n",
    "            except:\n",
    "                print('image failed to open',path)\n",
    "                opened = False\n",
    "                \n",
    "            mv_partitions.append(path)\n",
    "        assert(len(mv_partitions)==num_partition)\n",
    "        \n",
    "        \n",
    "        audiofeat_path = os.path.join(audio_dataset_dir,raw_moviename+'.wav.csv')\n",
    "        try:\n",
    "            open(audiofeat_path)\n",
    "        except:\n",
    "            print('audio failed to open',path)\n",
    "            opened = False\n",
    "        if opened :\n",
    "            img_paths.append(mv_partitions)\n",
    "            audio_paths.append(audiofeat_path)\n",
    "            raw_movienames.append(raw_moviename)\n",
    "            labels.append(big_five)\n",
    "            \n",
    "    \n",
    "    return np.array(img_paths),np.array(audio_paths),\\\n",
    "                np.array(labels), np.array(raw_movienames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## use this if we have seperated train/val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dataset: /home/noa_glaser/proj/data/train-frames/train\n",
      "processing dataset: /home/noa_glaser/proj/data/train-frames/val\n"
     ]
    }
   ],
   "source": [
    "train_img_paths,train_audio_paths, train_labels, train_movienames \\\n",
    "        = get_img_audio_label(train_dataset_dir,audio_dataset_dir,label_dataset_dir) \n",
    "val_img_paths,val_audio_paths, val_labels, val_movienames \\\n",
    "= get_img_audio_label(val_dataset_dir,audio_dataset_dir,label_dataset_dir)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Loader. Data is normalized before feeding into model (as required by the pretrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def default_img_loader(img_paths,transform):\n",
    "    ten_img_tensor = []\n",
    "    for path in img_paths:\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if transform is not None:\n",
    "            img = transform(img)\n",
    "        ten_img_tensor.append(img)\n",
    "        \n",
    "    return torch.cat(ten_img_tensor)\n",
    "        \n",
    "\n",
    "def default_audio_loader(path):\n",
    "\treturn np.loadtxt(path,delimiter=',')\n",
    "\n",
    "class VisualAudio(data.Dataset):\n",
    "    def __init__(self,split,img_paths,audio_paths, movie_names,labels,transform=None,\n",
    "                 img_loader=default_img_loader,audio_loader=default_audio_loader):\n",
    "        self.split = split \n",
    "        self.img_paths = img_paths\n",
    "        self.audio_paths = audio_paths\n",
    "        self.movie_names = movie_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.img_loader=img_loader\n",
    "        self.audio_loader= audio_loader\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_paths, audio_paths,target = self.img_paths[index], \\\n",
    "                                        self.audio_paths[index], self.labels[index]\n",
    "        ten_img_tensor = self.img_loader(img_paths,self.transform)\n",
    "        ten_audio = self.audio_loader(audio_paths)\n",
    "        #return 30x224x224 , 10x68, 10 x 5\n",
    "        \n",
    "        assert(ten_img_tensor.size() == (30,256,256))\n",
    "        \n",
    "        return ten_img_tensor, ten_audio[:10,:], target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 4800, 'val': 1200}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import  transforms\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.RandomCrop(256),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "        \n",
    "dsets = {}\n",
    "dsets['train'] = VisualAudio('train',train_img_paths,train_audio_paths,\\\n",
    "                    train_movienames ,train_labels,transform=data_transforms['train'] )\n",
    "dsets['val'] = VisualAudio('val',val_img_paths,val_audio_paths,\\\n",
    "                         val_movienames,val_labels,transform=data_transforms['val'] )\n",
    "\n",
    "dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=1) for x in ['train', 'val']}\n",
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "dset_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Some dataset examples (each batch is 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    \n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "#train_imgsamples,train_audiosamle,train_labelsample = next(iter(dset_loaders['train']))\n",
    "\n",
    "#train_unflattened_sample = train_imgsamples.view(-1,3,256,256)[0:10,:,:,:]\n",
    "## Make a grid from batch\n",
    "#plt.figure( figsize=(100, 20))\n",
    "#out = torchvision.utils.make_grid(train_unflattened_sample,nrow=10)\n",
    "#imshow(out, title='trainning sample')\n",
    "#plt.savefig('train_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#val_imgsamples,val_audiosamle,val_labelsample = next(iter(dset_loaders['val']))\n",
    "#val_unflattened_sample = val_imgsamples.view(-1,3,256,256)\n",
    "### Make a grid from val batch\n",
    "#plt.figure( figsize=(10, 16))\n",
    "#out2 = torchvision.utils.make_grid(val_unflattened_sample,nrow=10)\n",
    "#imshow(out2, title='validation sample')\n",
    "#plt.savefig('val_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AudioVisualLSTM(nn.Module):\n",
    "    NUM_AUDIO_INPUT = 68\n",
    "    NUM_VID_FEATURES = 128\n",
    "    NUM_AUDIO_FEATURES = 32\n",
    "    NUM_LSTM_HIDDEN = 128\n",
    "    NUM_PARTITIONS = 10\n",
    "    NUM_CLASS = 5\n",
    "    NUM_IMG_SIZE = 256\n",
    "    NUM_CHANNEL = 3\n",
    "    \n",
    "    def __init__(self):        \n",
    "        super(AudioVisualLSTM, self).__init__()\n",
    "        self.audioBranch =  nn.Sequential(nn.Linear(self.NUM_AUDIO_INPUT,self.NUM_AUDIO_FEATURES))\n",
    "        self.videoBranch = self._createVideoBranch()\n",
    "        self.video_dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=(self.NUM_VID_FEATURES+self.NUM_AUDIO_FEATURES),\n",
    "            hidden_size=self.NUM_LSTM_HIDDEN,\n",
    "            num_layers=1,\n",
    "            bias=True,\n",
    "            batch_first=True # input and output tensors provided as (batch, seq, feature)\n",
    "            # can add dropout later\n",
    "            )\n",
    "        self.fc = nn.Linear(self.NUM_LSTM_HIDDEN,self.NUM_CLASS)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.avg = nn.AvgPool1d(self.NUM_PARTITIONS,self.NUM_PARTITIONS)\n",
    "\n",
    "    def _createVideoBranch(self):\n",
    "        model_pretrained = torchvision.models.resnet34(pretrained=True)\n",
    "        # All of the parameters are freezed, not to change (newly constructed layers' params won't be influenced)\n",
    "        for param in model_pretrained.parameters():\n",
    "            param.requires_grad = False   \n",
    "        model_pretrained.fc = nn.Linear(model_pretrained.fc.in_features, self.NUM_VID_FEATURES)\n",
    "        return model_pretrained\n",
    "    \n",
    "    def forward(self, x):\n",
    "        videoData = x[0].view(-1,self.NUM_CHANNEL,self.NUM_IMG_SIZE,self.NUM_IMG_SIZE)\n",
    "        audioData = x[1].view(-1,self.NUM_AUDIO_INPUT)\n",
    "        videoProcessed = self.videoBranch(videoData).view(-1,self.NUM_PARTITIONS,self.NUM_VID_FEATURES) # will output a (n x partitions)x 32 tensor\n",
    "        videoProcessed = self.video_dropout(videoProcessed)\n",
    "        audioProcessed = self.audioBranch(audioData).view(-1,self.NUM_PARTITIONS,self.NUM_AUDIO_FEATURES)# will output a (n x partitions)x 128 tensor\n",
    "        x = torch.cat((videoProcessed, audioProcessed), 2).type(gpu_dtype) #(N,10,160)\n",
    "        h0 = Variable(torch.zeros(1, x.size()[0], self.NUM_LSTM_HIDDEN)).type(gpu_dtype)\n",
    "        c0 = Variable(torch.zeros(1, x.size()[0], self.NUM_LSTM_HIDDEN)).type(gpu_dtype)\n",
    "        x,cn = self.lstm(x, (h0, c0))\n",
    "        x = x.contiguous().view(-1,self.NUM_LSTM_HIDDEN)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x) #(N*P,5)\n",
    "        x = self.avg(x.view(-1,self.NUM_PARTITIONS,self.NUM_CLASS).transpose(1,2)).squeeze() #(N,5)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some gpu configs\n",
    "use_gpu = True\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "\n",
    "def get_learnable_params(m,verbose = 0):\n",
    "    ret = []\n",
    "    for l in m.parameters():\n",
    "        if l.requires_grad == True:\n",
    "            ret.append(l)\n",
    "            if verbose == 1:\n",
    "                print (l.size())\n",
    "            if verbose == 2:\n",
    "                print (l)\n",
    "    return ret\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"Saves checkpoint to disk\"\"\"\n",
    "    directory = \"resnet_for_rnn/%s/\"%(exp_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filename = directory + filename\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'resnet_for_rnn/%s/'%(exp_name) + 'model_best.pth.tar')\n",
    "\n",
    "def log_value(to_log, log_path = './log_'+ exp_name + '.txt'):\n",
    "    log_file = open(log_path, 'a+')\n",
    "    log_file.write(to_log)\n",
    "    log_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_freq = 50\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch) :\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        input_image,input_audio, target = data\n",
    "        input_image_var, input_audio_var,target_var = Variable(input_image.type(gpu_dtype)), \\\n",
    "            Variable(input_audio.type(gpu_dtype)),Variable(target.type(gpu_dtype))\n",
    "        # compute output\n",
    "        output = model([input_image_var,input_audio_var])\n",
    "        loss = criterion(output, target_var)\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data[0], input_image.size(0))\n",
    "        # compute gradient and do Adam step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i % log_freq == 0:\n",
    "            to_log = 'Epoch: [{0}][{1}/{2}]\\t Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t Loss {loss.val:f} ({loss.avg:f})\\n'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time, loss=losses)\n",
    "            log_value(to_log)\n",
    "            print(to_log)\n",
    "            \n",
    "       \n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    \"\"\"Perform validation on the validation set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, data in enumerate(val_loader):\n",
    "        input_image,input_audio, target = data\n",
    "        input_image_var, input_audio_var,target_var = Variable(input_image.type(gpu_dtype)), \\\n",
    "        Variable(input_audio.type(gpu_dtype)),Variable(target.type(gpu_dtype))\n",
    "        # compute output\n",
    "        output = model([input_image_var,input_audio_var])\n",
    "        loss = criterion(output, target_var)\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data[0], input_image.size(0))\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i %  log_freq == 0:\n",
    "            \n",
    "            to_log = 'Val/Test: [{0}/{1}]\\t Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t Loss {loss.val:f} ({loss.avg:f})\\n'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses)\n",
    "            log_value(to_log)\n",
    "            print(to_log)\n",
    "                        \n",
    "    return losses.avg, output,input_image,target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: for larger dataset, consider a step function or exponentialdecay\n",
    "def lr_scheduler(optimizer, epoch):\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = AudioVisualLSTM().type(gpu_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val/Test: [0/150]\t Time 1.039 (1.039)\t Loss 0.021047 (0.021047)\t Loss_O 0.026464 (0.026464) \t Loss_C 0.018138 (0.018138)\t Loss_E 0.020867 (0.020867)\t Loss_A 0.010338 (0.010338)\t Loss_N 0.029427 (0.029427)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.615 (0.652)\t Loss 0.030349 (0.026085)\t Loss_O 0.033047 (0.031450) \t Loss_C 0.028545 (0.022124)\t Loss_E 0.036275 (0.029415)\t Loss_A 0.020620 (0.021348)\t Loss_N 0.033257 (0.026088)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.613 (0.634)\t Loss 0.027473 (0.026016)\t Loss_O 0.009325 (0.028321) \t Loss_C 0.039450 (0.024402)\t Loss_E 0.021382 (0.027823)\t Loss_A 0.020253 (0.022151)\t Loss_N 0.046954 (0.027381)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.613 (0.628)\t Loss 0.022194 (0.025755)\t Loss_O 0.022899 (0.029245) \t Loss_C 0.019558 (0.023851)\t Loss_E 0.021355 (0.027148)\t Loss_A 0.022205 (0.021820)\t Loss_N 0.024952 (0.026710)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.617 (0.626)\t Loss 0.022109 (0.025072)\t Loss_O 0.029597 (0.028756) \t Loss_C 0.022428 (0.023855)\t Loss_E 0.023076 (0.026095)\t Loss_A 0.018382 (0.021040)\t Loss_N 0.017062 (0.025612)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.619 (0.624)\t Loss 0.019262 (0.025104)\t Loss_O 0.025411 (0.030181) \t Loss_C 0.015211 (0.023500)\t Loss_E 0.018535 (0.025757)\t Loss_A 0.013901 (0.020723)\t Loss_N 0.023254 (0.025359)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.625 (0.624)\t Loss 0.020017 (0.024730)\t Loss_O 0.019211 (0.029797) \t Loss_C 0.027402 (0.023828)\t Loss_E 0.024294 (0.024906)\t Loss_A 0.014111 (0.019908)\t Loss_N 0.015065 (0.025211)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.624 (0.624)\t Loss 0.027543 (0.024724)\t Loss_O 0.037063 (0.029691) \t Loss_C 0.017813 (0.023916)\t Loss_E 0.026275 (0.024790)\t Loss_A 0.021611 (0.020157)\t Loss_N 0.034952 (0.025065)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.626 (0.624)\t Loss 0.019999 (0.025133)\t Loss_O 0.019165 (0.029753) \t Loss_C 0.020570 (0.024629)\t Loss_E 0.026300 (0.025367)\t Loss_A 0.010223 (0.020442)\t Loss_N 0.023738 (0.025473)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.632 (0.624)\t Loss 0.016694 (0.024861)\t Loss_O 0.015848 (0.029412) \t Loss_C 0.014939 (0.024454)\t Loss_E 0.009316 (0.024597)\t Loss_A 0.028292 (0.020344)\t Loss_N 0.015076 (0.025499)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.625 (0.624)\t Loss 0.025980 (0.024875)\t Loss_O 0.057991 (0.029727) \t Loss_C 0.015613 (0.024187)\t Loss_E 0.017093 (0.024559)\t Loss_A 0.009771 (0.020184)\t Loss_N 0.029432 (0.025717)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.624 (0.624)\t Loss 0.019874 (0.024614)\t Loss_O 0.026847 (0.029512) \t Loss_C 0.014479 (0.023748)\t Loss_E 0.023122 (0.024182)\t Loss_A 0.012512 (0.020001)\t Loss_N 0.022412 (0.025629)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.629 (0.624)\t Loss 0.009197 (0.024299)\t Loss_O 0.006841 (0.028903) \t Loss_C 0.012318 (0.023718)\t Loss_E 0.011803 (0.023863)\t Loss_A 0.012406 (0.019752)\t Loss_N 0.002618 (0.025260)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.626 (0.625)\t Loss 0.036588 (0.024081)\t Loss_O 0.055445 (0.028715) \t Loss_C 0.040715 (0.023641)\t Loss_E 0.018804 (0.023532)\t Loss_A 0.029393 (0.019593)\t Loss_N 0.038583 (0.024924)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.626 (0.625)\t Loss 0.025691 (0.024034)\t Loss_O 0.041582 (0.028342) \t Loss_C 0.020912 (0.023635)\t Loss_E 0.023167 (0.023475)\t Loss_A 0.023484 (0.019806)\t Loss_N 0.019310 (0.024914)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def validate(val_loader, model, criterion, epoch):\n",
    "    \"\"\"Perform validation on the validation set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_E = AverageMeter()\n",
    "    losses_N = AverageMeter()\n",
    "    losses_A= AverageMeter()\n",
    "    losses_C= AverageMeter()\n",
    "    losses_O= AverageMeter()\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, data in enumerate(val_loader):\n",
    "        input_image,input_audio, target = data\n",
    "        input_image_var, input_audio_var,target_var = Variable(input_image.type(gpu_dtype)), \\\n",
    "        Variable(input_audio.type(gpu_dtype)),Variable(target.type(gpu_dtype))\n",
    "        # compute output\n",
    "        output = model([input_image_var,input_audio_var])\n",
    "           \n",
    "        loss = criterion(output, target_var)\n",
    "        loss_E= criterion(output[:,0], target_var[:,0])\n",
    "        loss_N= criterion(output[:,1], target_var[:,1])\n",
    "        loss_A= criterion(output[:,2], target_var[:,2])\n",
    "        loss_C= criterion(output[:,3], target_var[:,3])\n",
    "        loss_O= criterion(output[:,4], target_var[:,4])\n",
    "        \n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data[0], input_image.size(0))\n",
    "        \n",
    "        losses_E.update(loss_E.data[0], input_image.size(0))\n",
    "        losses_N.update(loss_N.data[0], input_image.size(0))\n",
    "        losses_A.update(loss_A.data[0], input_image.size(0))\n",
    "        losses_C.update(loss_C.data[0], input_image.size(0))\n",
    "        losses_O.update(loss_O.data[0], input_image.size(0))\n",
    "        ocean_losses = [losses_O.avg,losses_C.avg,losses_E.avg,losses_A.avg,losses_N.avg]\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i %  log_freq == 0:\n",
    "            \n",
    "            to_log = 'Val/Test: [{0}/{1}]\\t Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t Loss {loss.val:f} ({loss.avg:f})\\t Loss_O {losses_O.val:f} ({losses_O.avg:f}) \\t Loss_C {losses_C.val:f} ({losses_C.avg:f})\\t Loss_E {losses_E.val:f} ({losses_E.avg:f})\\t Loss_A {losses_A.val:f} ({losses_A.avg:f})\\t Loss_N {losses_N.val:f} ({losses_N.avg:f})\\n'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses,losses_O=losses_O,losses_C=losses_C,losses_E=losses_E,losses_A=losses_A,losses_N=losses_N)\n",
    "            log_value(to_log)\n",
    "            print(to_log)\n",
    "                     \n",
    "    return losses.avg, output,input_image,target,ocean_losses\n",
    "\n",
    "val_loss,val_preds,val_imgs,val_targets,val_ocean_loss = validate(dset_loaders['val'], model, nn.MSELoss().type(gpu_dtype), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.028432273725047707, 0.02360273233614862, 0.02359762910598268, 0.0198449419454361, 0.024803201671068868]\n",
      "[ 0.50390458  0.52310342  0.49139604  0.54537439  0.46589938]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# can retriecve these latest val results for plotting\n",
    "val_preds,val_imgs,val_targets = None,None,None\n",
    "\n",
    "def train_model(startModel=None, startEpoch=0, numEpochs=10):    \n",
    "    model = AudioVisualLSTM().type(gpu_dtype)\n",
    "    #del model_base\n",
    "\n",
    "    #  changed to l1 loss to reflect competition \n",
    "    criterion = nn.MSELoss().type(gpu_dtype)\n",
    "\n",
    "    #only optimizing the new_fc layer parameters, other pretrained weights are freezed¶\n",
    "    optimizer  = optim.SGD(get_learnable_params(model),lr=5e-2, momentum=0.9,weight_decay=5e-4)\n",
    "\n",
    "    best_loss = 1000 # will get overwritten\n",
    "    \n",
    "    if(startModel != None):\n",
    "        print(\"=> loading checkpoint '{}'\".format(startModel))\n",
    "        checkpoint = torch.load(startModel)\n",
    "        startEpoch = checkpoint['epoch']\n",
    "        best_loss = checkpoint['best_loss'] # for now because old loss is stale\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        # todo - figure out why not working\n",
    "        #optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        # print(optimizer.param_groups)\n",
    "        \n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(startModel, checkpoint['epoch']))\n",
    "    #else:\n",
    "        # benchmark the model \n",
    "        #best_loss = validate(dset_loaders['val'], model, criterion, startEpoch)\n",
    "        \n",
    "    bestModel = model\n",
    "\n",
    "    for epoch in range(startEpoch,startEpoch+numEpochs):\n",
    "        # train for one epoch\n",
    "        train_loss = train(dset_loaders['train'], model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        val_loss,val_preds,val_imgs,val_targets,val_ocean_loss = validate(dset_loaders['val'], model, criterion, epoch)\n",
    "\n",
    "        # log \n",
    "        log_value('Epoch: [{0}]\\t Train Loss: {train_loss:f}  \\t Val Loss: {val_loss:f} \\t \\n'.format(epoch,\\\n",
    "                                        train_loss=train_loss,val_loss=val_loss),'./%s_epoch_log.txt'%exp_name)\n",
    "        print('Epoch: [{0}]\\t Train Loss: {train_loss:f}  \\t Val Loss: {val_loss:f} Loss_O: {Loss_O:f} Loss_C: {Loss_C:f} Loss_E: {Loss_E:f} Loss_A: {Loss_A:f} Loss_N: {Loss_N:f}\\n'.format(epoch,\\\n",
    "                    train_loss=train_loss,val_loss=val_loss,Loss_O=val_ocean_loss[0],Loss_C=val_ocean_loss[1],Loss_E=val_ocean_loss[2],Loss_A=val_ocean_loss[3],Loss_N=val_ocean_loss[4]))\n",
    "\n",
    "        # remember best loss and save checkpoint\n",
    "        is_best = val_loss <= best_loss\n",
    "        best_loss = min(val_loss, best_loss)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': 'resnet34_only',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print (is_best, best_loss)\n",
    "\n",
    "    print ('Best Loss: ', best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Start to train a new model run this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_model(startModel=None, startEpoch=0, numEpochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train from the latest model checpoint run this cell, specify startepoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "startModel='./resnet_for_rnn/avepool_dropout_ResNet34_LSTM_experiment/checkpoint.pth.tar'\n",
    "train_model(startModel=startModel, startEpoch=1 , numEpochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# visualize the pred and groundtruth and images of the latest val batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "val_pred = (val_preds.data).cpu().numpy()\n",
    "val_gt = (Variable(val_targets).data).cpu().numpy()\n",
    "# select a video in batch to see: idx can be 0-7\n",
    "idx = 7\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(np.arange(5),val_gt[idx])\n",
    "plt.xticks(np.arange(5),['e','n','a','c','o'])\n",
    "plt.gca().set_ylim([0.0,1.0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(np.arange(5),val_pred[idx])\n",
    "plt.xticks(np.arange(5),['e','n','a','c','o'])\n",
    "plt.gca().set_ylim([0.0,1.0])\n",
    "plt.show()\n",
    "val_unflattened_sample = val_imgs.view(-1,3,256,256)\n",
    "# Make a grid from batch\n",
    "plt.figure( figsize=(30, 3))\n",
    "out = torchvision.utils.make_grid(val_unflattened_sample[idx*10:idx*10+10],nrow=10)\n",
    "imshow(out, title='trainning sample')\n",
    "#plt.savefig('train_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

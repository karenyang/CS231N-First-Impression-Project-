{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_dir = '/home/noa_glaser/proj/data/train-frames/train'\n",
    "val_dataset_dir = '/home/noa_glaser/proj/data/train-frames/val'\n",
    "audio_dataset_dir = '/home/noa_glaser/proj/data/train-audio'\n",
    "label_dataset_dir = '/home/noa_glaser/proj/data'\n",
    "exp_name = 'avepool_dropout_ResNet34_LSTM_experiment' # roughly 3K videos\n",
    "num_classes = 5 \n",
    "num_partition = 10\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import time\n",
    "import copy\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first, get the Images (path) and their labels (five personality traits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_img_audio_label(dataset_dir,audio_dataset_dir,label_dataset_dir):\n",
    "    \"\"\"Returns a list of np.array(img_paths), np.array(audio_paths),\n",
    "        np.array(labels), np.array(raw_movienames)\n",
    "    Args:\n",
    "    dataset for video, for audio_feats from pyAudioAnalysis, labels\n",
    "    \"\"\"\n",
    " \n",
    "    print(\"processing dataset: \"+ dataset_dir)\n",
    "    img_paths = [] \n",
    "    audio_paths=[]\n",
    "    raw_movienames = []\n",
    "    labels = []\n",
    "\n",
    "    annotaion_filename = label_dataset_dir + \"/annotation_training.pkl\"\n",
    "    \n",
    "    with open(annotaion_filename, 'rb') as f:\n",
    "        label_dicts = pickle.load(f, encoding='latin1') \n",
    "\n",
    "    for movie in os.listdir(dataset_dir):\n",
    "        fileEnding ='_50uniform' #TODO: figure out how to make more general\n",
    "        if fileEnding not in movie: continue #skip non-movie files\n",
    "        raw_moviename = movie.replace(fileEnding,'.mp4')      \n",
    "        big_five = [label_dicts['extraversion'][raw_moviename], \n",
    "                    label_dicts['neuroticism'][raw_moviename],\n",
    "                    label_dicts['agreeableness'][raw_moviename],\n",
    "                    label_dicts['conscientiousness'][raw_moviename],\n",
    "                    label_dicts['openness'][raw_moviename] ]\n",
    "                    #label_dicts['interview'][raw_moviename]]      \n",
    "        movie_path = os.path.join(dataset_dir, movie)\n",
    "        mv_partitions = []\n",
    "        p = 0\n",
    "        all_imgs = os.listdir(movie_path)\n",
    "        assert(len(all_imgs) >= num_partition)\n",
    "        opened = True\n",
    "        for i in range(num_partition):\n",
    "            path = os.path.join(movie_path, all_imgs[i])\n",
    "            try:\n",
    "                open(path)\n",
    "            except:\n",
    "                print('image failed to open',path)\n",
    "                opened = False\n",
    "                \n",
    "            mv_partitions.append(path)\n",
    "        assert(len(mv_partitions)==num_partition)\n",
    "        \n",
    "        \n",
    "        audiofeat_path = os.path.join(audio_dataset_dir,raw_moviename+'.wav.csv')\n",
    "        try:\n",
    "            open(audiofeat_path)\n",
    "        except:\n",
    "            print('audio failed to open',path)\n",
    "            opened = False\n",
    "        if opened :\n",
    "            img_paths.append(mv_partitions)\n",
    "            audio_paths.append(audiofeat_path)\n",
    "            raw_movienames.append(raw_moviename)\n",
    "            labels.append(big_five)\n",
    "            \n",
    "    \n",
    "    return np.array(img_paths),np.array(audio_paths),\\\n",
    "                np.array(labels), np.array(raw_movienames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## use this if we have seperated train/val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dataset: /home/noa_glaser/proj/data/train-frames/train\n",
      "processing dataset: /home/noa_glaser/proj/data/train-frames/val\n"
     ]
    }
   ],
   "source": [
    "train_img_paths,train_audio_paths, train_labels, train_movienames \\\n",
    "        = get_img_audio_label(train_dataset_dir,audio_dataset_dir,label_dataset_dir) \n",
    "val_img_paths,val_audio_paths, val_labels, val_movienames \\\n",
    "= get_img_audio_label(val_dataset_dir,audio_dataset_dir,label_dataset_dir)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Loader. Data is normalized before feeding into model (as required by the pretrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def default_img_loader(img_paths,transform):\n",
    "    ten_img_tensor = []\n",
    "    for path in img_paths:\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if transform is not None:\n",
    "            img = transform(img)\n",
    "        ten_img_tensor.append(img)\n",
    "        \n",
    "    return torch.cat(ten_img_tensor)\n",
    "        \n",
    "\n",
    "def default_audio_loader(path):\n",
    "    return np.loadtxt(path,delimiter=',')\n",
    "\n",
    "class VisualAudio(data.Dataset):\n",
    "    def __init__(self,split,img_paths,audio_paths, movie_names,labels,transform=None,\n",
    "                 img_loader=default_img_loader,audio_loader=default_audio_loader):\n",
    "        self.split = split \n",
    "        self.img_paths = img_paths\n",
    "        self.audio_paths = audio_paths\n",
    "        self.movie_names = movie_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.img_loader=img_loader\n",
    "        self.audio_loader= audio_loader\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_paths, audio_paths,target = self.img_paths[index], \\\n",
    "                                        self.audio_paths[index], self.labels[index]\n",
    "        ten_img_tensor = self.img_loader(img_paths,self.transform)\n",
    "        ten_audio = self.audio_loader(audio_paths)\n",
    "        #return 30x224x224 , 10x68, 10 x 5\n",
    "        \n",
    "        assert(ten_img_tensor.size() == (30,256,256))\n",
    "        \n",
    "        return ten_img_tensor, ten_audio[:10,:], target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 4800, 'val': 1200}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import  transforms\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.RandomCrop(256),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "        \n",
    "dsets = {}\n",
    "dsets['train'] = VisualAudio('train',train_img_paths,train_audio_paths,\\\n",
    "                    train_movienames ,train_labels,transform=data_transforms['train'] )\n",
    "dsets['val'] = VisualAudio('val',val_img_paths,val_audio_paths,\\\n",
    "                         val_movienames,val_labels,transform=data_transforms['val'] )\n",
    "\n",
    "dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=1) for x in ['train', 'val']}\n",
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "dset_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Some dataset examples (each batch is 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    \n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "#train_imgsamples,train_audiosamle,train_labelsample = next(iter(dset_loaders['train']))\n",
    "\n",
    "#train_unflattened_sample = train_imgsamples.view(-1,3,256,256)\n",
    "# Make a grid from batch\n",
    "#plt.figure( figsize=(10, 16))\n",
    "#out = torchvision.utils.make_grid(train_unflattened_sample,nrow=10)\n",
    "#imshow(out, title='trainning sample')\n",
    "#plt.savefig('train_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#val_imgsamples,val_audiosamle,val_labelsample = next(iter(dset_loaders['val']))\n",
    "#val_unflattened_sample = val_imgsamples.view(-1,3,256,256)\n",
    "### Make a grid from val batch\n",
    "#plt.figure( figsize=(10, 16))\n",
    "#out2 = torchvision.utils.make_grid(val_unflattened_sample,nrow=10)\n",
    "#imshow(out2, title='validation sample')\n",
    "#plt.savefig('val_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AudioVisualLSTM(nn.Module):\n",
    "    NUM_AUDIO_INPUT = 68\n",
    "    NUM_VID_FEATURES = 128\n",
    "    NUM_AUDIO_FEATURES = 32\n",
    "    NUM_LSTM_HIDDEN = 128\n",
    "    NUM_PARTITIONS = 10\n",
    "    NUM_CLASS = 5\n",
    "    NUM_IMG_SIZE = 256\n",
    "    NUM_CHANNEL = 3\n",
    "    \n",
    "    def __init__(self):        \n",
    "        super(AudioVisualLSTM, self).__init__()\n",
    "        self.audioBranch =  nn.Sequential(nn.Linear(self.NUM_AUDIO_INPUT,self.NUM_AUDIO_FEATURES))\n",
    "        self.videoBranch = self._createVideoBranch()\n",
    "        self.video_dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=(self.NUM_VID_FEATURES+self.NUM_AUDIO_FEATURES),\n",
    "            hidden_size=self.NUM_LSTM_HIDDEN,\n",
    "            num_layers=1,\n",
    "            bias=True,\n",
    "            batch_first=True # input and output tensors provided as (batch, seq, feature)\n",
    "            # can add dropout later\n",
    "            )\n",
    "        self.fc = nn.Linear(self.NUM_LSTM_HIDDEN,self.NUM_CLASS)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.avg = nn.AvgPool1d(self.NUM_PARTITIONS,self.NUM_PARTITIONS)\n",
    "\n",
    "    def _createVideoBranch(self):\n",
    "        model_pretrained = torchvision.models.resnet34(pretrained=True)\n",
    "        # All of the parameters are freezed, not to change (newly constructed layers' params won't be influenced)\n",
    "        for param in model_pretrained.parameters():\n",
    "            param.requires_grad = False   \n",
    "        model_pretrained.fc = nn.Linear(model_pretrained.fc.in_features, self.NUM_VID_FEATURES)\n",
    "        return model_pretrained\n",
    "    \n",
    "    def forward(self, x):\n",
    "        videoData = x[0].view(-1,self.NUM_CHANNEL,self.NUM_IMG_SIZE,self.NUM_IMG_SIZE)\n",
    "        audioData = x[1].view(-1,self.NUM_AUDIO_INPUT)\n",
    "        videoProcessed = self.videoBranch(videoData).view(-1,self.NUM_PARTITIONS,self.NUM_VID_FEATURES) # will output a (n x partitions)x 32 tensor\n",
    "        videoProcessed = self.video_dropout(videoProcessed)\n",
    "        audioProcessed = self.audioBranch(audioData).view(-1,self.NUM_PARTITIONS,self.NUM_AUDIO_FEATURES)# will output a (n x partitions)x 128 tensor\n",
    "        x = torch.cat((videoProcessed, audioProcessed), 2).type(gpu_dtype) #(N,10,160)\n",
    "        h0 = Variable(torch.zeros(1, x.size()[0], self.NUM_LSTM_HIDDEN)).type(gpu_dtype)\n",
    "        c0 = Variable(torch.zeros(1, x.size()[0], self.NUM_LSTM_HIDDEN)).type(gpu_dtype)\n",
    "        x,cn = self.lstm(x, (h0, c0))\n",
    "        x = x.contiguous().view(-1,self.NUM_LSTM_HIDDEN)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x) #(N*P,5)\n",
    "        x = self.avg(x.view(-1,self.NUM_PARTITIONS,self.NUM_CLASS).transpose(1,2)).squeeze() #(N,5)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some gpu configs\n",
    "use_gpu = True\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "\n",
    "def get_learnable_params(m,verbose = 0):\n",
    "    ret = []\n",
    "    for l in m.parameters():\n",
    "        if l.requires_grad == True:\n",
    "            ret.append(l)\n",
    "            if verbose == 1:\n",
    "                print (l.size())\n",
    "            if verbose == 2:\n",
    "                print (l)\n",
    "    return ret\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"Saves checkpoint to disk\"\"\"\n",
    "    directory = \"resnet_for_rnn/%s/\"%(exp_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filename = directory + filename\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'resnet_for_rnn/%s/'%(exp_name) + 'model_best.pth.tar')\n",
    "\n",
    "def log_value(to_log, log_path = './log_'+ exp_name + '.txt'):\n",
    "    log_file = open(log_path, 'a+')\n",
    "    log_file.write(to_log)\n",
    "    log_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_freq = 10\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch) :\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        input_image,input_audio, target = data\n",
    "        input_image_var, input_audio_var,target_var = Variable(input_image.type(gpu_dtype)), \\\n",
    "            Variable(input_audio.type(gpu_dtype)),Variable(target.type(gpu_dtype))\n",
    "        # compute output\n",
    "        output = model([input_image_var,input_audio_var])\n",
    "        loss = criterion(output, target_var)\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data[0], input_image.size(0))\n",
    "        # compute gradient and do Adam step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i % log_freq == 0:\n",
    "            to_log = 'Epoch: [{0}][{1}/{2}]\\t Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t Loss {loss.val:f} ({loss.avg:f})\\n'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time, loss=losses)\n",
    "            log_value(to_log)\n",
    "            print(to_log)\n",
    "            \n",
    "       \n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    \"\"\"Perform validation on the validation set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, data in enumerate(val_loader):\n",
    "        input_image,input_audio, target = data\n",
    "        input_image_var, input_audio_var,target_var = Variable(input_image.type(gpu_dtype)), \\\n",
    "        Variable(input_audio.type(gpu_dtype)),Variable(target.type(gpu_dtype))\n",
    "        # compute output\n",
    "        output = model([input_image_var,input_audio_var])\n",
    "        loss = criterion(output, target_var)\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data[0], input_image.size(0))\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i %  log_freq == 0:\n",
    "            to_log = 'Val/Test: [{0}/{1}]\\t Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t Loss {loss.val:f} ({loss.avg:f})\\n'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses)\n",
    "            log_value(to_log)\n",
    "            print(to_log)\n",
    "                        \n",
    "    return losses.avg, output,input_image,target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: for larger dataset, consider a step function or exponentialdecay\n",
    "def lr_scheduler(optimizer, epoch):\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# can retriecve these latest val results for plotting\n",
    "val_preds,val_imgs,val_targets = None,None,None\n",
    "\n",
    "\n",
    "\n",
    "def train_model(startModel=None, startEpoch=0, numEpochs=10):    \n",
    "    model = AudioVisualLSTM().type(gpu_dtype)\n",
    "    #del model_base\n",
    "\n",
    "    #  changed to l1 loss to reflect competition \n",
    "    criterion = nn.L1Loss().type(gpu_dtype)\n",
    "\n",
    "    #only optimizing the new_fc layer parameters, other pretrained weights are freezedÂ¶\n",
    "    optimizer  = optim.SGD(get_learnable_params(model),lr=2e-5, momentum=0.9,weight_decay=5e-4)\n",
    "\n",
    "    best_loss = 1000 # will get overwritten\n",
    "    \n",
    "    if(startModel != None):\n",
    "        print(\"=> loading checkpoint '{}'\".format(startModel))\n",
    "        checkpoint = torch.load(startModel)\n",
    "        startEpoch = checkpoint['epoch']\n",
    "        best_loss = checkpoint['best_loss'] # for now because old loss is stale\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        # todo - figure out why not working\n",
    "        #optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        # print(optimizer.param_groups)\n",
    "        \n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(startModel, checkpoint['epoch']))\n",
    "    #else:\n",
    "        # benchmark the model \n",
    "        #best_loss = validate(dset_loaders['val'], model, criterion, startEpoch)\n",
    "        \n",
    "    bestModel = model\n",
    "\n",
    "    for epoch in range(startEpoch,startEpoch+numEpochs):\n",
    "        # train for one epoch\n",
    "        train_loss = train(dset_loaders['train'], model, criterion, lr_scheduler(optimizer, epoch), epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        val_loss,val_preds,val_imgs,val_targets = validate(dset_loaders['val'], model, criterion, epoch)\n",
    "\n",
    "        # log \n",
    "        log_value('Epoch: [{0}]\\t Train Loss: {train_loss:f}  \\t Val Loss: {val_loss:f}\\n'.format(epoch,\\\n",
    "                                        train_loss=train_loss,val_loss=val_loss),'./%s_epoch_log.txt'%exp_name)\n",
    "        print('Epoch: [{0}]\\t Train Loss: {train_loss:f}  \\t Val Loss: {val_loss:f}\\n'.format(epoch,\\\n",
    "                    train_loss=train_loss,val_loss=val_loss))\n",
    "\n",
    "        # remember best loss and save checkpoint\n",
    "        is_best = val_loss <= best_loss\n",
    "        best_loss = min(val_loss, best_loss)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': 'resnet34_only',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print (is_best, best_loss)\n",
    "\n",
    "    print ('Best Loss: ', best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Start to train a new model run this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/384]\t Time 1.688 (1.688)\t Loss 0.132156 (0.132156)\n",
      "\n",
      "Epoch: [0][10/384]\t Time 0.633 (0.732)\t Loss 0.143759 (0.129876)\n",
      "\n",
      "Epoch: [0][20/384]\t Time 0.627 (0.683)\t Loss 0.116172 (0.128264)\n",
      "\n",
      "Epoch: [0][30/384]\t Time 0.630 (0.666)\t Loss 0.109504 (0.126049)\n",
      "\n",
      "Epoch: [0][40/384]\t Time 0.631 (0.658)\t Loss 0.084700 (0.125912)\n",
      "\n",
      "Epoch: [0][50/384]\t Time 0.634 (0.653)\t Loss 0.127153 (0.124309)\n",
      "\n",
      "Epoch: [0][60/384]\t Time 0.633 (0.650)\t Loss 0.135271 (0.123641)\n",
      "\n",
      "Epoch: [0][70/384]\t Time 0.639 (0.648)\t Loss 0.102599 (0.125125)\n",
      "\n",
      "Epoch: [0][80/384]\t Time 0.632 (0.647)\t Loss 0.100294 (0.124166)\n",
      "\n",
      "Epoch: [0][90/384]\t Time 0.633 (0.646)\t Loss 0.094928 (0.123659)\n",
      "\n",
      "Epoch: [0][100/384]\t Time 0.633 (0.645)\t Loss 0.090074 (0.123659)\n",
      "\n",
      "Epoch: [0][110/384]\t Time 0.632 (0.644)\t Loss 0.140440 (0.123298)\n",
      "\n",
      "Epoch: [0][120/384]\t Time 0.641 (0.644)\t Loss 0.104139 (0.122031)\n",
      "\n",
      "Epoch: [0][130/384]\t Time 0.638 (0.643)\t Loss 0.108146 (0.121005)\n",
      "\n",
      "Epoch: [0][140/384]\t Time 0.638 (0.643)\t Loss 0.114645 (0.121147)\n",
      "\n",
      "Epoch: [0][150/384]\t Time 0.638 (0.642)\t Loss 0.172408 (0.120470)\n",
      "\n",
      "Epoch: [0][160/384]\t Time 0.641 (0.642)\t Loss 0.137000 (0.120008)\n",
      "\n",
      "Epoch: [0][170/384]\t Time 0.639 (0.642)\t Loss 0.114404 (0.118705)\n",
      "\n",
      "Epoch: [0][180/384]\t Time 0.636 (0.642)\t Loss 0.098957 (0.118278)\n",
      "\n",
      "Epoch: [0][190/384]\t Time 0.636 (0.641)\t Loss 0.111962 (0.119041)\n",
      "\n",
      "Epoch: [0][200/384]\t Time 0.636 (0.641)\t Loss 0.115179 (0.119436)\n",
      "\n",
      "Epoch: [0][210/384]\t Time 0.640 (0.641)\t Loss 0.128245 (0.119547)\n",
      "\n",
      "Epoch: [0][220/384]\t Time 0.636 (0.641)\t Loss 0.145749 (0.119644)\n",
      "\n",
      "Epoch: [0][230/384]\t Time 0.639 (0.641)\t Loss 0.112831 (0.119787)\n",
      "\n",
      "Epoch: [0][240/384]\t Time 0.634 (0.641)\t Loss 0.113056 (0.120306)\n",
      "\n",
      "Epoch: [0][250/384]\t Time 0.636 (0.640)\t Loss 0.087505 (0.119702)\n",
      "\n",
      "Epoch: [0][260/384]\t Time 0.633 (0.640)\t Loss 0.116098 (0.119343)\n",
      "\n",
      "Epoch: [0][270/384]\t Time 0.635 (0.640)\t Loss 0.118805 (0.119487)\n",
      "\n",
      "Epoch: [0][280/384]\t Time 0.640 (0.640)\t Loss 0.120208 (0.119194)\n",
      "\n",
      "Epoch: [0][290/384]\t Time 0.634 (0.640)\t Loss 0.093490 (0.119050)\n",
      "\n",
      "Epoch: [0][300/384]\t Time 0.643 (0.640)\t Loss 0.137350 (0.118872)\n",
      "\n",
      "Epoch: [0][310/384]\t Time 0.635 (0.640)\t Loss 0.088768 (0.118457)\n",
      "\n",
      "Epoch: [0][320/384]\t Time 0.641 (0.640)\t Loss 0.108040 (0.118212)\n",
      "\n",
      "Epoch: [0][330/384]\t Time 0.634 (0.640)\t Loss 0.115164 (0.118125)\n",
      "\n",
      "Epoch: [0][340/384]\t Time 0.644 (0.640)\t Loss 0.093834 (0.117906)\n",
      "\n",
      "Epoch: [0][350/384]\t Time 0.639 (0.640)\t Loss 0.126879 (0.117738)\n",
      "\n",
      "Epoch: [0][360/384]\t Time 0.641 (0.640)\t Loss 0.114608 (0.117966)\n",
      "\n",
      "Epoch: [0][370/384]\t Time 0.633 (0.640)\t Loss 0.115488 (0.118090)\n",
      "\n",
      "Epoch: [0][380/384]\t Time 0.637 (0.639)\t Loss 0.160581 (0.118263)\n",
      "\n",
      "Val/Test: [0/96]\t Time 1.014 (1.014)\t Loss 0.080043 (0.080043)\n",
      "\n",
      "Val/Test: [10/96]\t Time 0.610 (0.644)\t Loss 0.095998 (0.096837)\n",
      "\n",
      "Val/Test: [20/96]\t Time 0.609 (0.626)\t Loss 0.113810 (0.109165)\n",
      "\n",
      "Val/Test: [30/96]\t Time 0.609 (0.619)\t Loss 0.091777 (0.105809)\n",
      "\n",
      "Val/Test: [40/96]\t Time 0.610 (0.616)\t Loss 0.101847 (0.108692)\n",
      "\n",
      "Val/Test: [50/96]\t Time 0.601 (0.614)\t Loss 0.100736 (0.109417)\n",
      "\n",
      "Val/Test: [60/96]\t Time 0.604 (0.613)\t Loss 0.090478 (0.110538)\n",
      "\n",
      "Val/Test: [70/96]\t Time 0.608 (0.612)\t Loss 0.131087 (0.112432)\n",
      "\n",
      "Val/Test: [80/96]\t Time 0.611 (0.611)\t Loss 0.123158 (0.111879)\n",
      "\n",
      "Val/Test: [90/96]\t Time 0.606 (0.611)\t Loss 0.089778 (0.111922)\n",
      "\n",
      "Epoch: [0]\t Train Loss: 0.118321  \t Val Loss: 0.112106\n",
      "\n",
      "True 0.11210570096348722\n",
      "Best Loss:  0.11210570096348722\n"
     ]
    }
   ],
   "source": [
    "train_model(startModel=None, startEpoch=0, numEpochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train from the latest model checpoint run this cell, specify startepoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint './resnet_for_rnn/avepool_dropout_ResNet34_LSTM_experiment/checkpoint.pth.tar'\n",
      "=> loaded checkpoint './resnet_for_rnn/avepool_dropout_ResNet34_LSTM_experiment/checkpoint.pth.tar' (epoch 79)\n",
      "Epoch: [79][0/600]\t Time 9.691 (9.691)\t Loss 0.088571 (0.088571)\n",
      "\n",
      "Epoch: [79][10/600]\t Time 0.921 (1.601)\t Loss 0.114275 (0.102261)\n",
      "\n",
      "Epoch: [79][20/600]\t Time 0.810 (1.242)\t Loss 0.096658 (0.104088)\n",
      "\n",
      "Epoch: [79][30/600]\t Time 0.901 (1.101)\t Loss 0.112861 (0.103271)\n",
      "\n",
      "Epoch: [79][40/600]\t Time 0.719 (1.029)\t Loss 0.073495 (0.103476)\n",
      "\n",
      "Epoch: [79][50/600]\t Time 0.828 (0.996)\t Loss 0.099429 (0.103542)\n",
      "\n",
      "Epoch: [79][60/600]\t Time 1.045 (0.971)\t Loss 0.098747 (0.103084)\n",
      "\n",
      "Epoch: [79][70/600]\t Time 0.972 (0.946)\t Loss 0.084229 (0.101462)\n",
      "\n",
      "Epoch: [79][80/600]\t Time 0.978 (0.939)\t Loss 0.085738 (0.102585)\n",
      "\n",
      "Epoch: [79][90/600]\t Time 0.778 (0.926)\t Loss 0.105083 (0.102393)\n",
      "\n",
      "Epoch: [79][100/600]\t Time 0.877 (0.919)\t Loss 0.115041 (0.102013)\n",
      "\n",
      "Epoch: [79][110/600]\t Time 0.826 (0.907)\t Loss 0.098927 (0.101822)\n",
      "\n",
      "Epoch: [79][120/600]\t Time 0.785 (0.902)\t Loss 0.066847 (0.102017)\n",
      "\n",
      "Epoch: [79][130/600]\t Time 0.903 (0.895)\t Loss 0.067441 (0.101936)\n",
      "\n",
      "Epoch: [79][140/600]\t Time 0.693 (0.888)\t Loss 0.127184 (0.102399)\n",
      "\n",
      "Epoch: [79][150/600]\t Time 0.743 (0.882)\t Loss 0.137889 (0.102761)\n",
      "\n",
      "Epoch: [79][160/600]\t Time 0.762 (0.878)\t Loss 0.104357 (0.102972)\n",
      "\n",
      "Epoch: [79][170/600]\t Time 0.854 (0.874)\t Loss 0.099997 (0.102818)\n",
      "\n",
      "Epoch: [79][180/600]\t Time 0.982 (0.872)\t Loss 0.102528 (0.103127)\n",
      "\n",
      "Epoch: [79][190/600]\t Time 0.864 (0.870)\t Loss 0.093443 (0.102837)\n",
      "\n",
      "Epoch: [79][200/600]\t Time 0.746 (0.864)\t Loss 0.081031 (0.102605)\n",
      "\n",
      "Epoch: [79][210/600]\t Time 0.786 (0.863)\t Loss 0.090912 (0.102511)\n",
      "\n",
      "Epoch: [79][220/600]\t Time 0.907 (0.860)\t Loss 0.057181 (0.102197)\n",
      "\n",
      "Epoch: [79][230/600]\t Time 0.786 (0.858)\t Loss 0.109651 (0.102412)\n",
      "\n",
      "Epoch: [79][240/600]\t Time 0.746 (0.856)\t Loss 0.095939 (0.102173)\n",
      "\n",
      "Epoch: [79][250/600]\t Time 0.885 (0.856)\t Loss 0.111172 (0.102436)\n",
      "\n",
      "Epoch: [79][260/600]\t Time 0.828 (0.854)\t Loss 0.104687 (0.102679)\n",
      "\n",
      "Epoch: [79][270/600]\t Time 0.893 (0.854)\t Loss 0.097319 (0.102807)\n",
      "\n",
      "Epoch: [79][280/600]\t Time 0.859 (0.852)\t Loss 0.088330 (0.102841)\n",
      "\n",
      "Epoch: [79][290/600]\t Time 0.857 (0.850)\t Loss 0.111119 (0.102659)\n",
      "\n",
      "Epoch: [79][300/600]\t Time 0.729 (0.848)\t Loss 0.111273 (0.102672)\n",
      "\n",
      "Epoch: [79][310/600]\t Time 0.682 (0.846)\t Loss 0.077149 (0.102784)\n",
      "\n",
      "Epoch: [79][320/600]\t Time 0.829 (0.845)\t Loss 0.125146 (0.102944)\n",
      "\n",
      "Epoch: [79][330/600]\t Time 0.680 (0.844)\t Loss 0.107163 (0.103182)\n",
      "\n",
      "Epoch: [79][340/600]\t Time 0.793 (0.842)\t Loss 0.063917 (0.103219)\n",
      "\n",
      "Epoch: [79][350/600]\t Time 0.938 (0.842)\t Loss 0.082616 (0.103002)\n",
      "\n",
      "Epoch: [79][360/600]\t Time 0.761 (0.840)\t Loss 0.131072 (0.103053)\n",
      "\n",
      "Epoch: [79][370/600]\t Time 0.679 (0.839)\t Loss 0.133103 (0.103459)\n",
      "\n",
      "Epoch: [79][380/600]\t Time 0.821 (0.839)\t Loss 0.095626 (0.103491)\n",
      "\n",
      "Epoch: [79][390/600]\t Time 0.673 (0.837)\t Loss 0.083992 (0.103650)\n",
      "\n",
      "Epoch: [79][400/600]\t Time 0.748 (0.835)\t Loss 0.114158 (0.103584)\n",
      "\n",
      "Epoch: [79][410/600]\t Time 0.786 (0.834)\t Loss 0.104928 (0.103675)\n",
      "\n",
      "Epoch: [79][420/600]\t Time 0.791 (0.832)\t Loss 0.113323 (0.103751)\n",
      "\n",
      "Epoch: [79][430/600]\t Time 0.770 (0.831)\t Loss 0.089029 (0.103761)\n",
      "\n",
      "Epoch: [79][440/600]\t Time 0.783 (0.829)\t Loss 0.087051 (0.103670)\n",
      "\n",
      "Epoch: [79][450/600]\t Time 0.758 (0.828)\t Loss 0.087472 (0.103883)\n",
      "\n",
      "Epoch: [79][460/600]\t Time 0.824 (0.827)\t Loss 0.123317 (0.103930)\n",
      "\n",
      "Epoch: [79][470/600]\t Time 0.666 (0.825)\t Loss 0.143390 (0.103723)\n",
      "\n",
      "Epoch: [79][480/600]\t Time 0.787 (0.825)\t Loss 0.122244 (0.103817)\n",
      "\n",
      "Epoch: [79][490/600]\t Time 0.826 (0.825)\t Loss 0.100456 (0.103632)\n",
      "\n",
      "Epoch: [79][500/600]\t Time 0.941 (0.826)\t Loss 0.110625 (0.103867)\n",
      "\n",
      "Epoch: [79][510/600]\t Time 0.801 (0.826)\t Loss 0.093588 (0.103985)\n",
      "\n",
      "Epoch: [79][520/600]\t Time 0.699 (0.826)\t Loss 0.083431 (0.103887)\n",
      "\n",
      "Epoch: [79][530/600]\t Time 0.964 (0.826)\t Loss 0.131955 (0.103963)\n",
      "\n",
      "Epoch: [79][540/600]\t Time 0.838 (0.826)\t Loss 0.081582 (0.103813)\n",
      "\n",
      "Epoch: [79][550/600]\t Time 0.696 (0.825)\t Loss 0.096538 (0.103680)\n",
      "\n",
      "Epoch: [79][560/600]\t Time 0.690 (0.823)\t Loss 0.080109 (0.103517)\n",
      "\n",
      "Epoch: [79][570/600]\t Time 0.712 (0.822)\t Loss 0.091419 (0.103571)\n",
      "\n",
      "Epoch: [79][580/600]\t Time 0.857 (0.822)\t Loss 0.101486 (0.103716)\n",
      "\n",
      "Epoch: [79][590/600]\t Time 0.675 (0.821)\t Loss 0.095892 (0.103508)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.671 (1.671)\t Loss 0.081842 (0.081842)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.690 (0.842)\t Loss 0.101862 (0.100383)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.683 (0.765)\t Loss 0.089570 (0.103763)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.641 (0.726)\t Loss 0.133189 (0.106914)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.699 (0.725)\t Loss 0.080225 (0.106438)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.711 (0.716)\t Loss 0.087812 (0.106449)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.810 (0.722)\t Loss 0.094117 (0.104994)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.643 (0.723)\t Loss 0.089502 (0.105483)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.904 (0.720)\t Loss 0.070773 (0.104229)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.653 (0.718)\t Loss 0.123710 (0.103343)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.645 (0.720)\t Loss 0.096079 (0.103809)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.644 (0.720)\t Loss 0.112779 (0.104550)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.699 (0.724)\t Loss 0.112955 (0.103564)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.852 (0.730)\t Loss 0.105341 (0.104133)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.852 (0.730)\t Loss 0.089062 (0.104417)\n",
      "\n",
      "Epoch: [79]\t Train Loss: 0.103513  \t Val Loss: 0.103859\n",
      "\n",
      "False 0.10384097491701444\n",
      "Epoch: [80][0/600]\t Time 1.037 (1.037)\t Loss 0.075220 (0.075220)\n",
      "\n",
      "Epoch: [80][10/600]\t Time 0.672 (0.706)\t Loss 0.109720 (0.100805)\n",
      "\n",
      "Epoch: [80][20/600]\t Time 0.681 (0.691)\t Loss 0.113561 (0.104833)\n",
      "\n",
      "Epoch: [80][30/600]\t Time 0.682 (0.686)\t Loss 0.096824 (0.104002)\n",
      "\n",
      "Epoch: [80][40/600]\t Time 0.669 (0.683)\t Loss 0.115523 (0.102034)\n",
      "\n",
      "Epoch: [80][50/600]\t Time 0.675 (0.682)\t Loss 0.093976 (0.102028)\n",
      "\n",
      "Epoch: [80][60/600]\t Time 0.681 (0.680)\t Loss 0.068006 (0.100039)\n",
      "\n",
      "Epoch: [80][70/600]\t Time 0.676 (0.680)\t Loss 0.135479 (0.101474)\n",
      "\n",
      "Epoch: [80][80/600]\t Time 0.672 (0.679)\t Loss 0.130822 (0.101717)\n",
      "\n",
      "Epoch: [80][90/600]\t Time 0.682 (0.679)\t Loss 0.098633 (0.101149)\n",
      "\n",
      "Epoch: [80][100/600]\t Time 0.676 (0.679)\t Loss 0.096980 (0.101794)\n",
      "\n",
      "Epoch: [80][110/600]\t Time 0.666 (0.678)\t Loss 0.096972 (0.101576)\n",
      "\n",
      "Epoch: [80][120/600]\t Time 0.676 (0.678)\t Loss 0.105136 (0.101805)\n",
      "\n",
      "Epoch: [80][130/600]\t Time 0.676 (0.678)\t Loss 0.111443 (0.101663)\n",
      "\n",
      "Epoch: [80][140/600]\t Time 0.668 (0.678)\t Loss 0.075723 (0.101884)\n",
      "\n",
      "Epoch: [80][150/600]\t Time 0.678 (0.678)\t Loss 0.096314 (0.102104)\n",
      "\n",
      "Epoch: [80][160/600]\t Time 0.676 (0.678)\t Loss 0.086355 (0.101641)\n",
      "\n",
      "Epoch: [80][170/600]\t Time 0.680 (0.678)\t Loss 0.115414 (0.101332)\n",
      "\n",
      "Epoch: [80][180/600]\t Time 0.674 (0.678)\t Loss 0.102318 (0.101215)\n",
      "\n",
      "Epoch: [80][190/600]\t Time 0.678 (0.678)\t Loss 0.103693 (0.101344)\n",
      "\n",
      "Epoch: [80][200/600]\t Time 0.674 (0.678)\t Loss 0.100251 (0.101941)\n",
      "\n",
      "Epoch: [80][210/600]\t Time 0.687 (0.678)\t Loss 0.080049 (0.101764)\n",
      "\n",
      "Epoch: [80][220/600]\t Time 0.680 (0.678)\t Loss 0.086907 (0.101485)\n",
      "\n",
      "Epoch: [80][230/600]\t Time 0.673 (0.678)\t Loss 0.151916 (0.101618)\n",
      "\n",
      "Epoch: [80][240/600]\t Time 0.685 (0.678)\t Loss 0.086547 (0.101617)\n",
      "\n",
      "Epoch: [80][250/600]\t Time 0.674 (0.678)\t Loss 0.077870 (0.101525)\n",
      "\n",
      "Epoch: [80][260/600]\t Time 0.672 (0.678)\t Loss 0.076056 (0.101229)\n",
      "\n",
      "Epoch: [80][270/600]\t Time 0.679 (0.678)\t Loss 0.074243 (0.101085)\n",
      "\n",
      "Epoch: [80][280/600]\t Time 0.675 (0.678)\t Loss 0.127107 (0.101663)\n",
      "\n",
      "Epoch: [80][290/600]\t Time 0.668 (0.677)\t Loss 0.108995 (0.101235)\n",
      "\n",
      "Epoch: [80][300/600]\t Time 0.680 (0.677)\t Loss 0.100143 (0.101277)\n",
      "\n",
      "Epoch: [80][310/600]\t Time 0.678 (0.677)\t Loss 0.108788 (0.101605)\n",
      "\n",
      "Epoch: [80][320/600]\t Time 0.669 (0.677)\t Loss 0.118311 (0.101871)\n",
      "\n",
      "Epoch: [80][330/600]\t Time 0.671 (0.677)\t Loss 0.119431 (0.101872)\n",
      "\n",
      "Epoch: [80][340/600]\t Time 0.675 (0.677)\t Loss 0.080556 (0.102008)\n",
      "\n",
      "Epoch: [80][350/600]\t Time 0.676 (0.677)\t Loss 0.064449 (0.101679)\n",
      "\n",
      "Epoch: [80][360/600]\t Time 0.674 (0.677)\t Loss 0.089757 (0.101593)\n",
      "\n",
      "Epoch: [80][370/600]\t Time 0.674 (0.677)\t Loss 0.123004 (0.101435)\n",
      "\n",
      "Epoch: [80][380/600]\t Time 0.673 (0.677)\t Loss 0.105978 (0.101599)\n",
      "\n",
      "Epoch: [80][390/600]\t Time 0.674 (0.677)\t Loss 0.079966 (0.101371)\n",
      "\n",
      "Epoch: [80][400/600]\t Time 0.675 (0.677)\t Loss 0.106372 (0.101655)\n",
      "\n",
      "Epoch: [80][410/600]\t Time 0.678 (0.677)\t Loss 0.129278 (0.101670)\n",
      "\n",
      "Epoch: [80][420/600]\t Time 0.676 (0.677)\t Loss 0.133282 (0.101897)\n",
      "\n",
      "Epoch: [80][430/600]\t Time 0.678 (0.677)\t Loss 0.100408 (0.101812)\n",
      "\n",
      "Epoch: [80][440/600]\t Time 0.680 (0.677)\t Loss 0.092166 (0.101565)\n",
      "\n",
      "Epoch: [80][450/600]\t Time 0.674 (0.677)\t Loss 0.091354 (0.101607)\n",
      "\n",
      "Epoch: [80][460/600]\t Time 0.686 (0.677)\t Loss 0.133341 (0.101817)\n",
      "\n",
      "Epoch: [80][470/600]\t Time 0.672 (0.677)\t Loss 0.116657 (0.101955)\n",
      "\n",
      "Epoch: [80][480/600]\t Time 0.671 (0.677)\t Loss 0.121418 (0.102039)\n",
      "\n",
      "Epoch: [80][490/600]\t Time 0.678 (0.677)\t Loss 0.079602 (0.101982)\n",
      "\n",
      "Epoch: [80][500/600]\t Time 0.677 (0.677)\t Loss 0.068380 (0.102105)\n",
      "\n",
      "Epoch: [80][510/600]\t Time 0.663 (0.677)\t Loss 0.084329 (0.102159)\n",
      "\n",
      "Epoch: [80][520/600]\t Time 0.677 (0.677)\t Loss 0.091437 (0.102191)\n",
      "\n",
      "Epoch: [80][530/600]\t Time 0.677 (0.677)\t Loss 0.074867 (0.102307)\n",
      "\n",
      "Epoch: [80][540/600]\t Time 0.674 (0.677)\t Loss 0.074246 (0.102324)\n",
      "\n",
      "Epoch: [80][550/600]\t Time 0.679 (0.677)\t Loss 0.110007 (0.102460)\n",
      "\n",
      "Epoch: [80][560/600]\t Time 0.675 (0.677)\t Loss 0.110831 (0.102561)\n",
      "\n",
      "Epoch: [80][570/600]\t Time 0.675 (0.677)\t Loss 0.118121 (0.102640)\n",
      "\n",
      "Epoch: [80][580/600]\t Time 0.672 (0.677)\t Loss 0.097534 (0.102504)\n",
      "\n",
      "Epoch: [80][590/600]\t Time 0.673 (0.677)\t Loss 0.113588 (0.102634)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.039 (1.039)\t Loss 0.117371 (0.117371)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.651 (0.690)\t Loss 0.082960 (0.095615)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.645 (0.670)\t Loss 0.096996 (0.101100)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.644 (0.662)\t Loss 0.107503 (0.100735)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.640 (0.659)\t Loss 0.137636 (0.102389)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.638 (0.656)\t Loss 0.084648 (0.101656)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.646 (0.654)\t Loss 0.114526 (0.101560)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.644 (0.653)\t Loss 0.104487 (0.102064)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.646 (0.652)\t Loss 0.113323 (0.101923)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.647 (0.652)\t Loss 0.118385 (0.102750)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.650 (0.651)\t Loss 0.074406 (0.103455)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.640 (0.651)\t Loss 0.116236 (0.103461)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.649 (0.650)\t Loss 0.116451 (0.103242)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.646 (0.650)\t Loss 0.073131 (0.102466)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.643 (0.650)\t Loss 0.108829 (0.103573)\n",
      "\n",
      "Epoch: [80]\t Train Loss: 0.102569  \t Val Loss: 0.103741\n",
      "\n",
      "True 0.10374114113549392\n",
      "Epoch: [81][0/600]\t Time 1.031 (1.031)\t Loss 0.136350 (0.136350)\n",
      "\n",
      "Epoch: [81][10/600]\t Time 0.676 (0.710)\t Loss 0.094626 (0.096830)\n",
      "\n",
      "Epoch: [81][20/600]\t Time 0.673 (0.694)\t Loss 0.069917 (0.093383)\n",
      "\n",
      "Epoch: [81][30/600]\t Time 0.677 (0.687)\t Loss 0.116504 (0.097633)\n",
      "\n",
      "Epoch: [81][40/600]\t Time 0.667 (0.684)\t Loss 0.131714 (0.100321)\n",
      "\n",
      "Epoch: [81][50/600]\t Time 0.676 (0.683)\t Loss 0.108783 (0.100793)\n",
      "\n",
      "Epoch: [81][60/600]\t Time 0.674 (0.682)\t Loss 0.098935 (0.099758)\n",
      "\n",
      "Epoch: [81][70/600]\t Time 0.666 (0.681)\t Loss 0.131446 (0.100552)\n",
      "\n",
      "Epoch: [81][80/600]\t Time 0.676 (0.680)\t Loss 0.105913 (0.101930)\n",
      "\n",
      "Epoch: [81][90/600]\t Time 0.676 (0.680)\t Loss 0.110302 (0.102777)\n",
      "\n",
      "Epoch: [81][100/600]\t Time 0.666 (0.679)\t Loss 0.102949 (0.102710)\n",
      "\n",
      "Epoch: [81][110/600]\t Time 0.678 (0.679)\t Loss 0.076857 (0.102627)\n",
      "\n",
      "Epoch: [81][120/600]\t Time 0.676 (0.679)\t Loss 0.083833 (0.102459)\n",
      "\n",
      "Epoch: [81][130/600]\t Time 0.673 (0.678)\t Loss 0.087048 (0.102236)\n",
      "\n",
      "Epoch: [81][140/600]\t Time 0.679 (0.678)\t Loss 0.091874 (0.102741)\n",
      "\n",
      "Epoch: [81][150/600]\t Time 0.676 (0.678)\t Loss 0.124889 (0.102902)\n",
      "\n",
      "Epoch: [81][160/600]\t Time 0.675 (0.678)\t Loss 0.088687 (0.103292)\n",
      "\n",
      "Epoch: [81][170/600]\t Time 0.679 (0.678)\t Loss 0.127107 (0.103302)\n",
      "\n",
      "Epoch: [81][180/600]\t Time 0.673 (0.678)\t Loss 0.077634 (0.103438)\n",
      "\n",
      "Epoch: [81][190/600]\t Time 0.682 (0.678)\t Loss 0.108684 (0.103138)\n",
      "\n",
      "Epoch: [81][200/600]\t Time 0.675 (0.678)\t Loss 0.114455 (0.103244)\n",
      "\n",
      "Epoch: [81][210/600]\t Time 0.679 (0.678)\t Loss 0.123721 (0.103453)\n",
      "\n",
      "Epoch: [81][220/600]\t Time 0.674 (0.678)\t Loss 0.076566 (0.103022)\n",
      "\n",
      "Epoch: [81][230/600]\t Time 0.676 (0.677)\t Loss 0.103783 (0.102854)\n",
      "\n",
      "Epoch: [81][240/600]\t Time 0.679 (0.677)\t Loss 0.093344 (0.102777)\n",
      "\n",
      "Epoch: [81][250/600]\t Time 0.677 (0.677)\t Loss 0.123312 (0.102487)\n",
      "\n",
      "Epoch: [81][260/600]\t Time 0.674 (0.677)\t Loss 0.110564 (0.102438)\n",
      "\n",
      "Epoch: [81][270/600]\t Time 0.675 (0.677)\t Loss 0.086787 (0.102663)\n",
      "\n",
      "Epoch: [81][280/600]\t Time 0.678 (0.677)\t Loss 0.089730 (0.102841)\n",
      "\n",
      "Epoch: [81][290/600]\t Time 0.674 (0.677)\t Loss 0.091867 (0.102897)\n",
      "\n",
      "Epoch: [81][300/600]\t Time 0.685 (0.677)\t Loss 0.105733 (0.103140)\n",
      "\n",
      "Epoch: [81][310/600]\t Time 0.682 (0.677)\t Loss 0.121182 (0.103319)\n",
      "\n",
      "Epoch: [81][320/600]\t Time 0.673 (0.677)\t Loss 0.107088 (0.103247)\n",
      "\n",
      "Epoch: [81][330/600]\t Time 0.686 (0.677)\t Loss 0.121044 (0.103551)\n",
      "\n",
      "Epoch: [81][340/600]\t Time 0.680 (0.677)\t Loss 0.104956 (0.103485)\n",
      "\n",
      "Epoch: [81][350/600]\t Time 0.675 (0.677)\t Loss 0.093668 (0.103651)\n",
      "\n",
      "Epoch: [81][360/600]\t Time 0.679 (0.677)\t Loss 0.076796 (0.103340)\n",
      "\n",
      "Epoch: [81][370/600]\t Time 0.672 (0.677)\t Loss 0.074332 (0.103371)\n",
      "\n",
      "Epoch: [81][380/600]\t Time 0.667 (0.677)\t Loss 0.087708 (0.103532)\n",
      "\n",
      "Epoch: [81][390/600]\t Time 0.668 (0.677)\t Loss 0.089290 (0.103475)\n",
      "\n",
      "Epoch: [81][400/600]\t Time 0.673 (0.677)\t Loss 0.065985 (0.103449)\n",
      "\n",
      "Epoch: [81][410/600]\t Time 0.674 (0.677)\t Loss 0.091546 (0.103238)\n",
      "\n",
      "Epoch: [81][420/600]\t Time 0.668 (0.677)\t Loss 0.113415 (0.103212)\n",
      "\n",
      "Epoch: [81][430/600]\t Time 0.676 (0.677)\t Loss 0.134809 (0.103379)\n",
      "\n",
      "Epoch: [81][440/600]\t Time 0.676 (0.677)\t Loss 0.121595 (0.103411)\n",
      "\n",
      "Epoch: [81][450/600]\t Time 0.668 (0.677)\t Loss 0.114183 (0.103366)\n",
      "\n",
      "Epoch: [81][460/600]\t Time 0.679 (0.677)\t Loss 0.130053 (0.103363)\n",
      "\n",
      "Epoch: [81][470/600]\t Time 0.678 (0.677)\t Loss 0.105932 (0.103253)\n",
      "\n",
      "Epoch: [81][480/600]\t Time 0.670 (0.677)\t Loss 0.081010 (0.102983)\n",
      "\n",
      "Epoch: [81][490/600]\t Time 0.661 (0.677)\t Loss 0.132638 (0.103026)\n",
      "\n",
      "Epoch: [81][500/600]\t Time 0.678 (0.677)\t Loss 0.105797 (0.103054)\n",
      "\n",
      "Epoch: [81][510/600]\t Time 0.672 (0.676)\t Loss 0.104869 (0.103065)\n",
      "\n",
      "Epoch: [81][520/600]\t Time 0.665 (0.676)\t Loss 0.114127 (0.103285)\n",
      "\n",
      "Epoch: [81][530/600]\t Time 0.677 (0.676)\t Loss 0.103592 (0.103385)\n",
      "\n",
      "Epoch: [81][540/600]\t Time 0.676 (0.676)\t Loss 0.102003 (0.103409)\n",
      "\n",
      "Epoch: [81][550/600]\t Time 0.667 (0.676)\t Loss 0.087622 (0.103570)\n",
      "\n",
      "Epoch: [81][560/600]\t Time 0.675 (0.676)\t Loss 0.117616 (0.103418)\n",
      "\n",
      "Epoch: [81][570/600]\t Time 0.676 (0.676)\t Loss 0.103484 (0.103178)\n",
      "\n",
      "Epoch: [81][580/600]\t Time 0.664 (0.676)\t Loss 0.105461 (0.103245)\n",
      "\n",
      "Epoch: [81][590/600]\t Time 0.676 (0.676)\t Loss 0.089571 (0.103092)\n",
      "\n",
      "Val/Test: [0/150]\t Time 0.988 (0.988)\t Loss 0.107021 (0.107021)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.646 (0.677)\t Loss 0.122646 (0.100475)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.640 (0.662)\t Loss 0.092273 (0.100665)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.652 (0.656)\t Loss 0.135232 (0.103177)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.643 (0.653)\t Loss 0.099588 (0.101249)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.649 (0.652)\t Loss 0.105908 (0.104667)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.647 (0.651)\t Loss 0.105842 (0.103005)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.639 (0.650)\t Loss 0.077500 (0.101366)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.640 (0.649)\t Loss 0.077206 (0.100885)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.649 (0.649)\t Loss 0.081363 (0.102208)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.639 (0.648)\t Loss 0.096491 (0.101963)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.634 (0.648)\t Loss 0.131276 (0.102635)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.645 (0.648)\t Loss 0.146166 (0.103130)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.653 (0.648)\t Loss 0.124613 (0.103206)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.647 (0.648)\t Loss 0.118737 (0.103303)\n",
      "\n",
      "Epoch: [81]\t Train Loss: 0.103194  \t Val Loss: 0.103835\n",
      "\n",
      "False 0.10374114113549392\n",
      "Epoch: [82][0/600]\t Time 1.043 (1.043)\t Loss 0.117390 (0.117390)\n",
      "\n",
      "Epoch: [82][10/600]\t Time 0.676 (0.709)\t Loss 0.120059 (0.101933)\n",
      "\n",
      "Epoch: [82][20/600]\t Time 0.669 (0.693)\t Loss 0.089775 (0.100155)\n",
      "\n",
      "Epoch: [82][30/600]\t Time 0.679 (0.687)\t Loss 0.102196 (0.102568)\n",
      "\n",
      "Epoch: [82][40/600]\t Time 0.675 (0.684)\t Loss 0.126350 (0.105925)\n",
      "\n",
      "Epoch: [82][50/600]\t Time 0.672 (0.683)\t Loss 0.118108 (0.106768)\n",
      "\n",
      "Epoch: [82][60/600]\t Time 0.677 (0.681)\t Loss 0.101235 (0.105980)\n",
      "\n",
      "Epoch: [82][70/600]\t Time 0.675 (0.680)\t Loss 0.074449 (0.105424)\n",
      "\n",
      "Epoch: [82][80/600]\t Time 0.673 (0.680)\t Loss 0.116608 (0.105711)\n",
      "\n",
      "Epoch: [82][90/600]\t Time 0.677 (0.679)\t Loss 0.102228 (0.105288)\n",
      "\n",
      "Epoch: [82][100/600]\t Time 0.685 (0.679)\t Loss 0.139936 (0.105258)\n",
      "\n",
      "Epoch: [82][110/600]\t Time 0.676 (0.679)\t Loss 0.074636 (0.104892)\n",
      "\n",
      "Epoch: [82][120/600]\t Time 0.682 (0.679)\t Loss 0.119649 (0.104443)\n",
      "\n",
      "Epoch: [82][130/600]\t Time 0.683 (0.678)\t Loss 0.088335 (0.104457)\n",
      "\n",
      "Epoch: [82][140/600]\t Time 0.674 (0.678)\t Loss 0.077347 (0.104704)\n",
      "\n",
      "Epoch: [82][150/600]\t Time 0.669 (0.678)\t Loss 0.129331 (0.104460)\n",
      "\n",
      "Epoch: [82][160/600]\t Time 0.678 (0.678)\t Loss 0.086190 (0.103871)\n",
      "\n",
      "Epoch: [82][170/600]\t Time 0.679 (0.678)\t Loss 0.090713 (0.103612)\n",
      "\n",
      "Epoch: [82][180/600]\t Time 0.677 (0.678)\t Loss 0.142555 (0.103331)\n",
      "\n",
      "Epoch: [82][190/600]\t Time 0.677 (0.678)\t Loss 0.124316 (0.103659)\n",
      "\n",
      "Epoch: [82][200/600]\t Time 0.677 (0.678)\t Loss 0.081008 (0.103302)\n",
      "\n",
      "Epoch: [82][210/600]\t Time 0.674 (0.678)\t Loss 0.113744 (0.103738)\n",
      "\n",
      "Epoch: [82][220/600]\t Time 0.677 (0.678)\t Loss 0.110123 (0.104540)\n",
      "\n",
      "Epoch: [82][230/600]\t Time 0.674 (0.678)\t Loss 0.092134 (0.104291)\n",
      "\n",
      "Epoch: [82][240/600]\t Time 0.679 (0.678)\t Loss 0.092846 (0.104208)\n",
      "\n",
      "Epoch: [82][250/600]\t Time 0.675 (0.678)\t Loss 0.082693 (0.104298)\n",
      "\n",
      "Epoch: [82][260/600]\t Time 0.676 (0.677)\t Loss 0.098084 (0.104355)\n",
      "\n",
      "Epoch: [82][270/600]\t Time 0.682 (0.677)\t Loss 0.103890 (0.104394)\n",
      "\n",
      "Epoch: [82][280/600]\t Time 0.671 (0.677)\t Loss 0.117273 (0.104129)\n",
      "\n",
      "Epoch: [82][290/600]\t Time 0.679 (0.677)\t Loss 0.107785 (0.104124)\n",
      "\n",
      "Epoch: [82][300/600]\t Time 0.677 (0.677)\t Loss 0.102810 (0.104446)\n",
      "\n",
      "Epoch: [82][310/600]\t Time 0.672 (0.677)\t Loss 0.102887 (0.104999)\n",
      "\n",
      "Epoch: [82][320/600]\t Time 0.671 (0.677)\t Loss 0.120997 (0.105057)\n",
      "\n",
      "Epoch: [82][330/600]\t Time 0.678 (0.677)\t Loss 0.128879 (0.105114)\n",
      "\n",
      "Epoch: [82][340/600]\t Time 0.675 (0.677)\t Loss 0.092131 (0.104693)\n",
      "\n",
      "Epoch: [82][350/600]\t Time 0.672 (0.677)\t Loss 0.113279 (0.104615)\n",
      "\n",
      "Epoch: [82][360/600]\t Time 0.673 (0.677)\t Loss 0.084913 (0.104887)\n",
      "\n",
      "Epoch: [82][370/600]\t Time 0.675 (0.677)\t Loss 0.138200 (0.104725)\n",
      "\n",
      "Epoch: [82][380/600]\t Time 0.674 (0.677)\t Loss 0.116506 (0.104464)\n",
      "\n",
      "Epoch: [82][390/600]\t Time 0.671 (0.677)\t Loss 0.075858 (0.104281)\n",
      "\n",
      "Epoch: [82][400/600]\t Time 0.679 (0.677)\t Loss 0.108220 (0.104262)\n",
      "\n",
      "Epoch: [82][410/600]\t Time 0.676 (0.677)\t Loss 0.084191 (0.104108)\n",
      "\n",
      "Epoch: [82][420/600]\t Time 0.678 (0.677)\t Loss 0.086913 (0.103979)\n",
      "\n",
      "Epoch: [82][430/600]\t Time 0.675 (0.677)\t Loss 0.092249 (0.103927)\n",
      "\n",
      "Epoch: [82][440/600]\t Time 0.683 (0.677)\t Loss 0.065452 (0.103775)\n",
      "\n",
      "Epoch: [82][450/600]\t Time 0.683 (0.677)\t Loss 0.082302 (0.103676)\n",
      "\n",
      "Epoch: [82][460/600]\t Time 0.672 (0.677)\t Loss 0.086920 (0.103403)\n",
      "\n",
      "Epoch: [82][470/600]\t Time 0.682 (0.677)\t Loss 0.140736 (0.103279)\n",
      "\n",
      "Epoch: [82][480/600]\t Time 0.683 (0.677)\t Loss 0.108553 (0.103176)\n",
      "\n",
      "Epoch: [82][490/600]\t Time 0.670 (0.677)\t Loss 0.094388 (0.103071)\n",
      "\n",
      "Epoch: [82][500/600]\t Time 0.684 (0.677)\t Loss 0.079158 (0.103201)\n",
      "\n",
      "Epoch: [82][510/600]\t Time 0.679 (0.677)\t Loss 0.090580 (0.103020)\n",
      "\n",
      "Epoch: [82][520/600]\t Time 0.671 (0.677)\t Loss 0.086282 (0.102839)\n",
      "\n",
      "Epoch: [82][530/600]\t Time 0.671 (0.677)\t Loss 0.104982 (0.102886)\n",
      "\n",
      "Epoch: [82][540/600]\t Time 0.679 (0.677)\t Loss 0.119437 (0.103229)\n",
      "\n",
      "Epoch: [82][550/600]\t Time 0.674 (0.677)\t Loss 0.167062 (0.103162)\n",
      "\n",
      "Epoch: [82][560/600]\t Time 0.671 (0.676)\t Loss 0.089746 (0.103085)\n",
      "\n",
      "Epoch: [82][570/600]\t Time 0.679 (0.676)\t Loss 0.133117 (0.103059)\n",
      "\n",
      "Epoch: [82][580/600]\t Time 0.675 (0.676)\t Loss 0.106391 (0.102972)\n",
      "\n",
      "Epoch: [82][590/600]\t Time 0.669 (0.676)\t Loss 0.073041 (0.102868)\n",
      "\n",
      "Val/Test: [0/150]\t Time 0.982 (0.982)\t Loss 0.084424 (0.084424)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.645 (0.678)\t Loss 0.122971 (0.106989)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.651 (0.662)\t Loss 0.086161 (0.109855)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.646 (0.657)\t Loss 0.103798 (0.107731)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.648 (0.655)\t Loss 0.142939 (0.106500)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.653 (0.653)\t Loss 0.093880 (0.105375)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.644 (0.652)\t Loss 0.079035 (0.104467)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.644 (0.652)\t Loss 0.085709 (0.104245)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.646 (0.651)\t Loss 0.113115 (0.104762)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.650 (0.651)\t Loss 0.117800 (0.104721)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.651 (0.650)\t Loss 0.079814 (0.104580)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.642 (0.650)\t Loss 0.110445 (0.104574)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.649 (0.650)\t Loss 0.107902 (0.104747)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.653 (0.650)\t Loss 0.104757 (0.104797)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.644 (0.649)\t Loss 0.080180 (0.104337)\n",
      "\n",
      "Epoch: [82]\t Train Loss: 0.102843  \t Val Loss: 0.103880\n",
      "\n",
      "False 0.10374114113549392\n",
      "Epoch: [83][0/600]\t Time 1.051 (1.051)\t Loss 0.100109 (0.100109)\n",
      "\n",
      "Epoch: [83][10/600]\t Time 0.676 (0.711)\t Loss 0.086737 (0.101955)\n",
      "\n",
      "Epoch: [83][20/600]\t Time 0.673 (0.694)\t Loss 0.119780 (0.102685)\n",
      "\n",
      "Epoch: [83][30/600]\t Time 0.672 (0.688)\t Loss 0.116028 (0.104510)\n",
      "\n",
      "Epoch: [83][40/600]\t Time 0.677 (0.685)\t Loss 0.103452 (0.103825)\n",
      "\n",
      "Epoch: [83][50/600]\t Time 0.677 (0.684)\t Loss 0.122337 (0.103248)\n",
      "\n",
      "Epoch: [83][60/600]\t Time 0.680 (0.683)\t Loss 0.083923 (0.101784)\n",
      "\n",
      "Epoch: [83][70/600]\t Time 0.672 (0.682)\t Loss 0.085460 (0.102874)\n",
      "\n",
      "Epoch: [83][80/600]\t Time 0.688 (0.681)\t Loss 0.093711 (0.100804)\n",
      "\n",
      "Epoch: [83][90/600]\t Time 0.678 (0.681)\t Loss 0.109327 (0.101367)\n",
      "\n",
      "Epoch: [83][100/600]\t Time 0.673 (0.680)\t Loss 0.096023 (0.101264)\n",
      "\n",
      "Epoch: [83][110/600]\t Time 0.685 (0.680)\t Loss 0.134569 (0.101856)\n",
      "\n",
      "Epoch: [83][120/600]\t Time 0.675 (0.680)\t Loss 0.103033 (0.102201)\n",
      "\n",
      "Epoch: [83][130/600]\t Time 0.667 (0.679)\t Loss 0.089379 (0.103308)\n",
      "\n",
      "Epoch: [83][140/600]\t Time 0.676 (0.679)\t Loss 0.093470 (0.102780)\n",
      "\n",
      "Epoch: [83][150/600]\t Time 0.675 (0.679)\t Loss 0.127551 (0.102952)\n",
      "\n",
      "Epoch: [83][160/600]\t Time 0.675 (0.679)\t Loss 0.106566 (0.102921)\n",
      "\n",
      "Epoch: [83][170/600]\t Time 0.672 (0.679)\t Loss 0.095159 (0.102826)\n",
      "\n",
      "Epoch: [83][180/600]\t Time 0.676 (0.679)\t Loss 0.079874 (0.102516)\n",
      "\n",
      "Epoch: [83][190/600]\t Time 0.672 (0.678)\t Loss 0.093470 (0.102660)\n",
      "\n",
      "Epoch: [83][200/600]\t Time 0.680 (0.678)\t Loss 0.080351 (0.102122)\n",
      "\n",
      "Epoch: [83][210/600]\t Time 0.673 (0.678)\t Loss 0.095213 (0.102350)\n",
      "\n",
      "Epoch: [83][220/600]\t Time 0.671 (0.678)\t Loss 0.134852 (0.102235)\n",
      "\n",
      "Epoch: [83][230/600]\t Time 0.679 (0.678)\t Loss 0.101557 (0.102474)\n",
      "\n",
      "Epoch: [83][240/600]\t Time 0.672 (0.678)\t Loss 0.129213 (0.102499)\n",
      "\n",
      "Epoch: [83][250/600]\t Time 0.675 (0.678)\t Loss 0.117790 (0.102347)\n",
      "\n",
      "Epoch: [83][260/600]\t Time 0.671 (0.678)\t Loss 0.089029 (0.102329)\n",
      "\n",
      "Epoch: [83][270/600]\t Time 0.676 (0.678)\t Loss 0.113947 (0.102336)\n",
      "\n",
      "Epoch: [83][280/600]\t Time 0.679 (0.677)\t Loss 0.108785 (0.102910)\n",
      "\n",
      "Epoch: [83][290/600]\t Time 0.675 (0.677)\t Loss 0.102720 (0.102800)\n",
      "\n",
      "Epoch: [83][300/600]\t Time 0.675 (0.677)\t Loss 0.092180 (0.103050)\n",
      "\n",
      "Epoch: [83][310/600]\t Time 0.674 (0.677)\t Loss 0.084908 (0.102811)\n",
      "\n",
      "Epoch: [83][320/600]\t Time 0.672 (0.677)\t Loss 0.093254 (0.103217)\n",
      "\n",
      "Epoch: [83][330/600]\t Time 0.673 (0.677)\t Loss 0.101111 (0.103226)\n",
      "\n",
      "Epoch: [83][340/600]\t Time 0.677 (0.677)\t Loss 0.095096 (0.103368)\n",
      "\n",
      "Epoch: [83][350/600]\t Time 0.675 (0.677)\t Loss 0.083885 (0.103517)\n",
      "\n",
      "Epoch: [83][360/600]\t Time 0.675 (0.677)\t Loss 0.106602 (0.103342)\n",
      "\n",
      "Epoch: [83][370/600]\t Time 0.672 (0.677)\t Loss 0.121170 (0.103202)\n",
      "\n",
      "Epoch: [83][380/600]\t Time 0.676 (0.677)\t Loss 0.121067 (0.103157)\n",
      "\n",
      "Epoch: [83][390/600]\t Time 0.675 (0.677)\t Loss 0.078546 (0.103055)\n",
      "\n",
      "Epoch: [83][400/600]\t Time 0.666 (0.677)\t Loss 0.099602 (0.103020)\n",
      "\n",
      "Epoch: [83][410/600]\t Time 0.676 (0.677)\t Loss 0.097305 (0.102937)\n",
      "\n",
      "Epoch: [83][420/600]\t Time 0.674 (0.677)\t Loss 0.113174 (0.102890)\n",
      "\n",
      "Epoch: [83][430/600]\t Time 0.667 (0.677)\t Loss 0.091760 (0.102749)\n",
      "\n",
      "Epoch: [83][440/600]\t Time 0.676 (0.677)\t Loss 0.083824 (0.102826)\n",
      "\n",
      "Epoch: [83][450/600]\t Time 0.673 (0.677)\t Loss 0.093747 (0.102735)\n",
      "\n",
      "Epoch: [83][460/600]\t Time 0.673 (0.676)\t Loss 0.072700 (0.102598)\n",
      "\n",
      "Epoch: [83][470/600]\t Time 0.677 (0.677)\t Loss 0.078880 (0.102492)\n",
      "\n",
      "Epoch: [83][480/600]\t Time 0.675 (0.676)\t Loss 0.137298 (0.102429)\n",
      "\n",
      "Epoch: [83][490/600]\t Time 0.674 (0.676)\t Loss 0.120534 (0.102416)\n",
      "\n",
      "Epoch: [83][500/600]\t Time 0.671 (0.676)\t Loss 0.084478 (0.102223)\n",
      "\n",
      "Epoch: [83][510/600]\t Time 0.674 (0.676)\t Loss 0.104685 (0.102063)\n",
      "\n",
      "Epoch: [83][520/600]\t Time 0.674 (0.676)\t Loss 0.092243 (0.102331)\n",
      "\n",
      "Epoch: [83][530/600]\t Time 0.672 (0.676)\t Loss 0.093426 (0.102470)\n",
      "\n",
      "Epoch: [83][540/600]\t Time 0.678 (0.676)\t Loss 0.101952 (0.102333)\n",
      "\n",
      "Epoch: [83][550/600]\t Time 0.676 (0.676)\t Loss 0.113867 (0.102317)\n",
      "\n",
      "Epoch: [83][560/600]\t Time 0.674 (0.676)\t Loss 0.092820 (0.102435)\n",
      "\n",
      "Epoch: [83][570/600]\t Time 0.678 (0.676)\t Loss 0.121635 (0.102507)\n",
      "\n",
      "Epoch: [83][580/600]\t Time 0.673 (0.676)\t Loss 0.099340 (0.102313)\n",
      "\n",
      "Epoch: [83][590/600]\t Time 0.687 (0.676)\t Loss 0.105637 (0.102472)\n",
      "\n",
      "Val/Test: [0/150]\t Time 0.989 (0.989)\t Loss 0.127539 (0.127539)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.645 (0.679)\t Loss 0.120193 (0.108021)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.646 (0.664)\t Loss 0.134757 (0.104409)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.643 (0.658)\t Loss 0.104641 (0.103672)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.646 (0.656)\t Loss 0.066865 (0.103827)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.645 (0.654)\t Loss 0.107185 (0.104196)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.649 (0.653)\t Loss 0.109946 (0.102642)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.653 (0.652)\t Loss 0.123080 (0.102957)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.640 (0.651)\t Loss 0.072264 (0.102571)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.651 (0.651)\t Loss 0.096293 (0.102218)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.652 (0.650)\t Loss 0.077953 (0.103079)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.645 (0.650)\t Loss 0.100933 (0.102946)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.653 (0.650)\t Loss 0.103681 (0.102457)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.643 (0.650)\t Loss 0.139892 (0.102615)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.651 (0.649)\t Loss 0.087213 (0.103439)\n",
      "\n",
      "Epoch: [83]\t Train Loss: 0.102415  \t Val Loss: 0.103403\n",
      "\n",
      "True 0.10340322797497113\n",
      "Epoch: [84][0/600]\t Time 1.032 (1.032)\t Loss 0.100642 (0.100642)\n",
      "\n",
      "Epoch: [84][10/600]\t Time 0.678 (0.707)\t Loss 0.115672 (0.123254)\n",
      "\n",
      "Epoch: [84][20/600]\t Time 0.680 (0.693)\t Loss 0.115688 (0.113862)\n",
      "\n",
      "Epoch: [84][30/600]\t Time 0.669 (0.687)\t Loss 0.097730 (0.110164)\n",
      "\n",
      "Epoch: [84][40/600]\t Time 0.682 (0.684)\t Loss 0.107346 (0.106936)\n",
      "\n",
      "Epoch: [84][50/600]\t Time 0.675 (0.683)\t Loss 0.068386 (0.103793)\n",
      "\n",
      "Epoch: [84][60/600]\t Time 0.673 (0.682)\t Loss 0.098275 (0.103921)\n",
      "\n",
      "Epoch: [84][70/600]\t Time 0.679 (0.681)\t Loss 0.113543 (0.103516)\n",
      "\n",
      "Epoch: [84][80/600]\t Time 0.677 (0.680)\t Loss 0.076870 (0.102333)\n",
      "\n",
      "Epoch: [84][90/600]\t Time 0.671 (0.679)\t Loss 0.097022 (0.102100)\n",
      "\n",
      "Epoch: [84][100/600]\t Time 0.672 (0.679)\t Loss 0.108330 (0.101202)\n",
      "\n",
      "Epoch: [84][110/600]\t Time 0.675 (0.678)\t Loss 0.081265 (0.100804)\n",
      "\n",
      "Epoch: [84][120/600]\t Time 0.678 (0.678)\t Loss 0.114204 (0.101794)\n",
      "\n",
      "Epoch: [84][130/600]\t Time 0.675 (0.678)\t Loss 0.102231 (0.101924)\n",
      "\n",
      "Epoch: [84][140/600]\t Time 0.673 (0.678)\t Loss 0.103953 (0.102246)\n",
      "\n",
      "Epoch: [84][150/600]\t Time 0.673 (0.677)\t Loss 0.104543 (0.101589)\n",
      "\n",
      "Epoch: [84][160/600]\t Time 0.678 (0.677)\t Loss 0.093809 (0.101840)\n",
      "\n",
      "Epoch: [84][170/600]\t Time 0.672 (0.677)\t Loss 0.076064 (0.101733)\n",
      "\n",
      "Epoch: [84][180/600]\t Time 0.674 (0.677)\t Loss 0.079958 (0.101432)\n",
      "\n",
      "Epoch: [84][190/600]\t Time 0.673 (0.677)\t Loss 0.113422 (0.101529)\n",
      "\n",
      "Epoch: [84][200/600]\t Time 0.675 (0.677)\t Loss 0.122403 (0.101678)\n",
      "\n",
      "Epoch: [84][210/600]\t Time 0.679 (0.677)\t Loss 0.082385 (0.101373)\n",
      "\n",
      "Epoch: [84][220/600]\t Time 0.671 (0.677)\t Loss 0.100831 (0.101374)\n",
      "\n",
      "Epoch: [84][230/600]\t Time 0.684 (0.677)\t Loss 0.100529 (0.101306)\n",
      "\n",
      "Epoch: [84][240/600]\t Time 0.675 (0.676)\t Loss 0.129744 (0.101693)\n",
      "\n",
      "Epoch: [84][250/600]\t Time 0.679 (0.677)\t Loss 0.095849 (0.101311)\n",
      "\n",
      "Epoch: [84][260/600]\t Time 0.679 (0.677)\t Loss 0.074623 (0.101709)\n",
      "\n",
      "Epoch: [84][270/600]\t Time 0.673 (0.677)\t Loss 0.103216 (0.101979)\n",
      "\n",
      "Epoch: [84][280/600]\t Time 0.678 (0.676)\t Loss 0.092554 (0.102050)\n",
      "\n",
      "Epoch: [84][290/600]\t Time 0.677 (0.676)\t Loss 0.086130 (0.102078)\n",
      "\n",
      "Epoch: [84][300/600]\t Time 0.673 (0.676)\t Loss 0.079299 (0.102110)\n",
      "\n",
      "Epoch: [84][310/600]\t Time 0.675 (0.676)\t Loss 0.103333 (0.102046)\n",
      "\n",
      "Epoch: [84][320/600]\t Time 0.679 (0.676)\t Loss 0.090417 (0.101941)\n",
      "\n",
      "Epoch: [84][330/600]\t Time 0.671 (0.676)\t Loss 0.104776 (0.102262)\n",
      "\n",
      "Epoch: [84][340/600]\t Time 0.676 (0.676)\t Loss 0.075588 (0.102227)\n",
      "\n",
      "Epoch: [84][350/600]\t Time 0.678 (0.676)\t Loss 0.075990 (0.102131)\n",
      "\n",
      "Epoch: [84][360/600]\t Time 0.673 (0.676)\t Loss 0.087316 (0.102432)\n",
      "\n",
      "Epoch: [84][370/600]\t Time 0.681 (0.676)\t Loss 0.092588 (0.102197)\n",
      "\n",
      "Epoch: [84][380/600]\t Time 0.684 (0.676)\t Loss 0.094121 (0.102250)\n",
      "\n",
      "Epoch: [84][390/600]\t Time 0.672 (0.676)\t Loss 0.118009 (0.102198)\n",
      "\n",
      "Epoch: [84][400/600]\t Time 0.682 (0.676)\t Loss 0.128135 (0.102193)\n",
      "\n",
      "Epoch: [84][410/600]\t Time 0.682 (0.676)\t Loss 0.085763 (0.102090)\n",
      "\n",
      "Epoch: [84][420/600]\t Time 0.679 (0.676)\t Loss 0.074238 (0.101953)\n",
      "\n",
      "Epoch: [84][430/600]\t Time 0.672 (0.676)\t Loss 0.128716 (0.102025)\n",
      "\n",
      "Epoch: [84][440/600]\t Time 0.677 (0.676)\t Loss 0.080377 (0.102176)\n",
      "\n",
      "Epoch: [84][450/600]\t Time 0.676 (0.676)\t Loss 0.081135 (0.102161)\n",
      "\n",
      "Epoch: [84][460/600]\t Time 0.664 (0.676)\t Loss 0.091287 (0.102163)\n",
      "\n",
      "Epoch: [84][470/600]\t Time 0.678 (0.676)\t Loss 0.072521 (0.102297)\n",
      "\n",
      "Epoch: [84][480/600]\t Time 0.674 (0.676)\t Loss 0.095611 (0.102512)\n",
      "\n",
      "Epoch: [84][490/600]\t Time 0.676 (0.676)\t Loss 0.102598 (0.102534)\n",
      "\n",
      "Epoch: [84][500/600]\t Time 0.669 (0.676)\t Loss 0.112475 (0.102638)\n",
      "\n",
      "Epoch: [84][510/600]\t Time 0.671 (0.676)\t Loss 0.114468 (0.102566)\n",
      "\n",
      "Epoch: [84][520/600]\t Time 0.675 (0.676)\t Loss 0.095810 (0.102820)\n",
      "\n",
      "Epoch: [84][530/600]\t Time 0.673 (0.676)\t Loss 0.108850 (0.102783)\n",
      "\n",
      "Epoch: [84][540/600]\t Time 0.674 (0.676)\t Loss 0.086908 (0.102805)\n",
      "\n",
      "Epoch: [84][550/600]\t Time 0.672 (0.676)\t Loss 0.092206 (0.102794)\n",
      "\n",
      "Epoch: [84][560/600]\t Time 0.667 (0.676)\t Loss 0.091062 (0.102676)\n",
      "\n",
      "Epoch: [84][570/600]\t Time 0.677 (0.676)\t Loss 0.089555 (0.102334)\n",
      "\n",
      "Epoch: [84][580/600]\t Time 0.675 (0.676)\t Loss 0.093186 (0.102267)\n",
      "\n",
      "Epoch: [84][590/600]\t Time 0.673 (0.676)\t Loss 0.106990 (0.102329)\n",
      "\n",
      "Val/Test: [0/150]\t Time 0.993 (0.993)\t Loss 0.131467 (0.131467)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.647 (0.684)\t Loss 0.117009 (0.103475)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.651 (0.666)\t Loss 0.110807 (0.105959)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.643 (0.659)\t Loss 0.105939 (0.106445)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.636 (0.655)\t Loss 0.085315 (0.103668)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.647 (0.653)\t Loss 0.142713 (0.104576)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.653 (0.653)\t Loss 0.083694 (0.104903)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.648 (0.651)\t Loss 0.126550 (0.103976)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.644 (0.651)\t Loss 0.091373 (0.104248)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.640 (0.650)\t Loss 0.082673 (0.103144)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.652 (0.650)\t Loss 0.094108 (0.103373)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.645 (0.649)\t Loss 0.125388 (0.103150)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.650 (0.649)\t Loss 0.086986 (0.103095)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.641 (0.649)\t Loss 0.104861 (0.103068)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.644 (0.648)\t Loss 0.082374 (0.103666)\n",
      "\n",
      "Epoch: [84]\t Train Loss: 0.102315  \t Val Loss: 0.103356\n",
      "\n",
      "True 0.10335628673434258\n",
      "Epoch: [85][0/600]\t Time 1.021 (1.021)\t Loss 0.130939 (0.130939)\n",
      "\n",
      "Epoch: [85][10/600]\t Time 0.675 (0.710)\t Loss 0.076164 (0.105282)\n",
      "\n",
      "Epoch: [85][20/600]\t Time 0.665 (0.693)\t Loss 0.111331 (0.104535)\n",
      "\n",
      "Epoch: [85][30/600]\t Time 0.679 (0.688)\t Loss 0.096292 (0.101972)\n",
      "\n",
      "Epoch: [85][40/600]\t Time 0.676 (0.685)\t Loss 0.142970 (0.105690)\n",
      "\n",
      "Epoch: [85][50/600]\t Time 0.673 (0.683)\t Loss 0.108723 (0.106533)\n",
      "\n",
      "Epoch: [85][60/600]\t Time 0.677 (0.682)\t Loss 0.078166 (0.104669)\n",
      "\n",
      "Epoch: [85][70/600]\t Time 0.677 (0.681)\t Loss 0.084062 (0.104009)\n",
      "\n",
      "Epoch: [85][80/600]\t Time 0.673 (0.680)\t Loss 0.105824 (0.103504)\n",
      "\n",
      "Epoch: [85][90/600]\t Time 0.673 (0.680)\t Loss 0.106150 (0.104040)\n",
      "\n",
      "Epoch: [85][100/600]\t Time 0.674 (0.680)\t Loss 0.113743 (0.103879)\n",
      "\n",
      "Epoch: [85][110/600]\t Time 0.679 (0.679)\t Loss 0.096210 (0.103504)\n",
      "\n",
      "Epoch: [85][120/600]\t Time 0.676 (0.679)\t Loss 0.129808 (0.103634)\n",
      "\n",
      "Epoch: [85][130/600]\t Time 0.674 (0.679)\t Loss 0.100211 (0.103265)\n",
      "\n",
      "Epoch: [85][140/600]\t Time 0.683 (0.679)\t Loss 0.092453 (0.103435)\n",
      "\n",
      "Epoch: [85][150/600]\t Time 0.673 (0.678)\t Loss 0.112496 (0.103729)\n",
      "\n",
      "Epoch: [85][160/600]\t Time 0.680 (0.678)\t Loss 0.083605 (0.103088)\n",
      "\n",
      "Epoch: [85][170/600]\t Time 0.678 (0.678)\t Loss 0.147069 (0.102307)\n",
      "\n",
      "Epoch: [85][180/600]\t Time 0.675 (0.678)\t Loss 0.134927 (0.102292)\n",
      "\n",
      "Epoch: [85][190/600]\t Time 0.679 (0.678)\t Loss 0.088072 (0.102181)\n",
      "\n",
      "Epoch: [85][200/600]\t Time 0.673 (0.678)\t Loss 0.101179 (0.102171)\n",
      "\n",
      "Epoch: [85][210/600]\t Time 0.674 (0.678)\t Loss 0.109660 (0.102380)\n",
      "\n",
      "Epoch: [85][220/600]\t Time 0.688 (0.678)\t Loss 0.114521 (0.103023)\n",
      "\n",
      "Epoch: [85][230/600]\t Time 0.677 (0.678)\t Loss 0.076939 (0.102797)\n",
      "\n",
      "Epoch: [85][240/600]\t Time 0.673 (0.678)\t Loss 0.115838 (0.102413)\n",
      "\n",
      "Epoch: [85][250/600]\t Time 0.687 (0.677)\t Loss 0.107053 (0.102696)\n",
      "\n",
      "Epoch: [85][260/600]\t Time 0.682 (0.677)\t Loss 0.102991 (0.103158)\n",
      "\n",
      "Epoch: [85][270/600]\t Time 0.672 (0.677)\t Loss 0.136390 (0.103374)\n",
      "\n",
      "Epoch: [85][280/600]\t Time 0.684 (0.677)\t Loss 0.075920 (0.103429)\n",
      "\n",
      "Epoch: [85][290/600]\t Time 0.677 (0.677)\t Loss 0.133408 (0.103575)\n",
      "\n",
      "Epoch: [85][300/600]\t Time 0.670 (0.677)\t Loss 0.065634 (0.103570)\n",
      "\n",
      "Epoch: [85][310/600]\t Time 0.680 (0.677)\t Loss 0.108394 (0.103634)\n",
      "\n",
      "Epoch: [85][320/600]\t Time 0.676 (0.677)\t Loss 0.150273 (0.103859)\n",
      "\n",
      "Epoch: [85][330/600]\t Time 0.675 (0.677)\t Loss 0.067658 (0.103798)\n",
      "\n",
      "Epoch: [85][340/600]\t Time 0.668 (0.677)\t Loss 0.089087 (0.103556)\n",
      "\n",
      "Epoch: [85][350/600]\t Time 0.676 (0.677)\t Loss 0.130902 (0.103541)\n",
      "\n",
      "Epoch: [85][360/600]\t Time 0.674 (0.677)\t Loss 0.119820 (0.103878)\n",
      "\n",
      "Epoch: [85][370/600]\t Time 0.676 (0.677)\t Loss 0.079233 (0.103828)\n",
      "\n",
      "Epoch: [85][380/600]\t Time 0.678 (0.677)\t Loss 0.119596 (0.103661)\n",
      "\n",
      "Epoch: [85][390/600]\t Time 0.676 (0.677)\t Loss 0.086510 (0.103610)\n",
      "\n",
      "Epoch: [85][400/600]\t Time 0.678 (0.677)\t Loss 0.108641 (0.103386)\n",
      "\n",
      "Epoch: [85][410/600]\t Time 0.674 (0.677)\t Loss 0.090797 (0.103224)\n",
      "\n",
      "Epoch: [85][420/600]\t Time 0.670 (0.677)\t Loss 0.128838 (0.103209)\n",
      "\n",
      "Epoch: [85][430/600]\t Time 0.677 (0.677)\t Loss 0.140889 (0.103144)\n",
      "\n",
      "Epoch: [85][440/600]\t Time 0.675 (0.677)\t Loss 0.108464 (0.103307)\n",
      "\n",
      "Epoch: [85][450/600]\t Time 0.673 (0.677)\t Loss 0.126956 (0.103280)\n",
      "\n",
      "Epoch: [85][460/600]\t Time 0.675 (0.677)\t Loss 0.108776 (0.103363)\n",
      "\n",
      "Epoch: [85][470/600]\t Time 0.672 (0.677)\t Loss 0.099447 (0.103362)\n",
      "\n",
      "Epoch: [85][480/600]\t Time 0.679 (0.677)\t Loss 0.077381 (0.103264)\n",
      "\n",
      "Epoch: [85][490/600]\t Time 0.677 (0.677)\t Loss 0.101873 (0.103206)\n",
      "\n",
      "Epoch: [85][500/600]\t Time 0.681 (0.677)\t Loss 0.129576 (0.103112)\n",
      "\n",
      "Epoch: [85][510/600]\t Time 0.679 (0.677)\t Loss 0.069251 (0.102868)\n",
      "\n",
      "Epoch: [85][520/600]\t Time 0.673 (0.677)\t Loss 0.132776 (0.102967)\n",
      "\n",
      "Epoch: [85][530/600]\t Time 0.688 (0.677)\t Loss 0.104555 (0.103118)\n",
      "\n",
      "Epoch: [85][540/600]\t Time 0.675 (0.677)\t Loss 0.096783 (0.103223)\n",
      "\n",
      "Epoch: [85][550/600]\t Time 0.668 (0.677)\t Loss 0.081776 (0.103078)\n",
      "\n",
      "Epoch: [85][560/600]\t Time 0.675 (0.677)\t Loss 0.085580 (0.103002)\n",
      "\n",
      "Epoch: [85][570/600]\t Time 0.679 (0.677)\t Loss 0.071778 (0.102876)\n",
      "\n",
      "Epoch: [85][580/600]\t Time 0.680 (0.677)\t Loss 0.091545 (0.102814)\n",
      "\n",
      "Epoch: [85][590/600]\t Time 0.673 (0.677)\t Loss 0.081419 (0.102653)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.013 (1.013)\t Loss 0.105083 (0.105083)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.642 (0.682)\t Loss 0.089615 (0.109098)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.648 (0.665)\t Loss 0.082298 (0.107762)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.647 (0.659)\t Loss 0.086763 (0.106322)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.651 (0.656)\t Loss 0.120049 (0.105210)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.642 (0.653)\t Loss 0.121617 (0.108193)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.651 (0.652)\t Loss 0.099662 (0.107313)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.642 (0.651)\t Loss 0.157485 (0.105767)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.643 (0.651)\t Loss 0.068139 (0.103881)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.646 (0.650)\t Loss 0.100176 (0.104816)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.642 (0.650)\t Loss 0.109652 (0.104991)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.643 (0.649)\t Loss 0.114948 (0.105121)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.642 (0.649)\t Loss 0.075531 (0.104026)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.652 (0.649)\t Loss 0.120433 (0.103075)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.649 (0.649)\t Loss 0.100254 (0.103706)\n",
      "\n",
      "Epoch: [85]\t Train Loss: 0.102474  \t Val Loss: 0.103411\n",
      "\n",
      "False 0.10335628673434258\n",
      "Epoch: [86][0/600]\t Time 1.044 (1.044)\t Loss 0.089298 (0.089298)\n",
      "\n",
      "Epoch: [86][10/600]\t Time 0.671 (0.710)\t Loss 0.069617 (0.104381)\n",
      "\n",
      "Epoch: [86][20/600]\t Time 0.680 (0.694)\t Loss 0.113085 (0.100894)\n",
      "\n",
      "Epoch: [86][30/600]\t Time 0.681 (0.687)\t Loss 0.131115 (0.104614)\n",
      "\n",
      "Epoch: [86][40/600]\t Time 0.680 (0.685)\t Loss 0.111348 (0.105098)\n",
      "\n",
      "Epoch: [86][50/600]\t Time 0.672 (0.683)\t Loss 0.091821 (0.103371)\n",
      "\n",
      "Epoch: [86][60/600]\t Time 0.686 (0.681)\t Loss 0.077786 (0.102081)\n",
      "\n",
      "Epoch: [86][70/600]\t Time 0.682 (0.681)\t Loss 0.095995 (0.102196)\n",
      "\n",
      "Epoch: [86][80/600]\t Time 0.673 (0.680)\t Loss 0.107751 (0.102948)\n",
      "\n",
      "Epoch: [86][90/600]\t Time 0.681 (0.679)\t Loss 0.110587 (0.102769)\n",
      "\n",
      "Epoch: [86][100/600]\t Time 0.677 (0.679)\t Loss 0.121400 (0.102453)\n",
      "\n",
      "Epoch: [86][110/600]\t Time 0.674 (0.679)\t Loss 0.072770 (0.102694)\n",
      "\n",
      "Epoch: [86][120/600]\t Time 0.680 (0.678)\t Loss 0.109766 (0.103282)\n",
      "\n",
      "Epoch: [86][130/600]\t Time 0.686 (0.678)\t Loss 0.061077 (0.102853)\n",
      "\n",
      "Epoch: [86][140/600]\t Time 0.667 (0.678)\t Loss 0.107442 (0.102488)\n",
      "\n",
      "Epoch: [86][150/600]\t Time 0.673 (0.678)\t Loss 0.084831 (0.102088)\n",
      "\n",
      "Epoch: [86][160/600]\t Time 0.683 (0.678)\t Loss 0.092285 (0.102145)\n",
      "\n",
      "Epoch: [86][170/600]\t Time 0.677 (0.678)\t Loss 0.088167 (0.102584)\n",
      "\n",
      "Epoch: [86][180/600]\t Time 0.670 (0.678)\t Loss 0.121066 (0.102711)\n",
      "\n",
      "Epoch: [86][190/600]\t Time 0.686 (0.677)\t Loss 0.122072 (0.102842)\n",
      "\n",
      "Epoch: [86][200/600]\t Time 0.678 (0.677)\t Loss 0.112396 (0.103228)\n",
      "\n",
      "Epoch: [86][210/600]\t Time 0.674 (0.677)\t Loss 0.087564 (0.103289)\n",
      "\n",
      "Epoch: [86][220/600]\t Time 0.677 (0.677)\t Loss 0.091285 (0.102846)\n",
      "\n",
      "Epoch: [86][230/600]\t Time 0.677 (0.677)\t Loss 0.083208 (0.102444)\n",
      "\n",
      "Epoch: [86][240/600]\t Time 0.666 (0.677)\t Loss 0.083626 (0.102592)\n",
      "\n",
      "Epoch: [86][250/600]\t Time 0.675 (0.677)\t Loss 0.089464 (0.102565)\n",
      "\n",
      "Epoch: [86][260/600]\t Time 0.677 (0.677)\t Loss 0.104724 (0.102288)\n",
      "\n",
      "Epoch: [86][270/600]\t Time 0.675 (0.677)\t Loss 0.092134 (0.102165)\n",
      "\n",
      "Epoch: [86][280/600]\t Time 0.679 (0.677)\t Loss 0.088019 (0.102051)\n",
      "\n",
      "Epoch: [86][290/600]\t Time 0.675 (0.677)\t Loss 0.110624 (0.102080)\n",
      "\n",
      "Epoch: [86][300/600]\t Time 0.678 (0.677)\t Loss 0.109891 (0.101856)\n",
      "\n",
      "Epoch: [86][310/600]\t Time 0.673 (0.677)\t Loss 0.101640 (0.101703)\n",
      "\n",
      "Epoch: [86][320/600]\t Time 0.678 (0.677)\t Loss 0.111203 (0.101731)\n",
      "\n",
      "Epoch: [86][330/600]\t Time 0.672 (0.677)\t Loss 0.085493 (0.101458)\n",
      "\n",
      "Epoch: [86][340/600]\t Time 0.677 (0.677)\t Loss 0.070999 (0.101490)\n",
      "\n",
      "Epoch: [86][350/600]\t Time 0.685 (0.677)\t Loss 0.113709 (0.101854)\n",
      "\n",
      "Epoch: [86][360/600]\t Time 0.674 (0.677)\t Loss 0.112248 (0.101788)\n",
      "\n",
      "Epoch: [86][370/600]\t Time 0.671 (0.677)\t Loss 0.118666 (0.102020)\n",
      "\n",
      "Epoch: [86][380/600]\t Time 0.674 (0.677)\t Loss 0.114287 (0.101873)\n",
      "\n",
      "Epoch: [86][390/600]\t Time 0.674 (0.677)\t Loss 0.086575 (0.101839)\n",
      "\n",
      "Epoch: [86][400/600]\t Time 0.675 (0.677)\t Loss 0.083105 (0.101853)\n",
      "\n",
      "Epoch: [86][410/600]\t Time 0.680 (0.677)\t Loss 0.070629 (0.101719)\n",
      "\n",
      "Epoch: [86][420/600]\t Time 0.681 (0.677)\t Loss 0.082127 (0.101704)\n",
      "\n",
      "Epoch: [86][430/600]\t Time 0.673 (0.677)\t Loss 0.103957 (0.101721)\n",
      "\n",
      "Epoch: [86][440/600]\t Time 0.679 (0.677)\t Loss 0.101392 (0.101876)\n",
      "\n",
      "Epoch: [86][450/600]\t Time 0.676 (0.677)\t Loss 0.114813 (0.101850)\n",
      "\n",
      "Epoch: [86][460/600]\t Time 0.667 (0.677)\t Loss 0.116415 (0.101770)\n",
      "\n",
      "Epoch: [86][470/600]\t Time 0.678 (0.677)\t Loss 0.073770 (0.101830)\n",
      "\n",
      "Epoch: [86][480/600]\t Time 0.678 (0.677)\t Loss 0.100321 (0.101855)\n",
      "\n",
      "Epoch: [86][490/600]\t Time 0.666 (0.677)\t Loss 0.113771 (0.101914)\n",
      "\n",
      "Epoch: [86][500/600]\t Time 0.679 (0.677)\t Loss 0.116518 (0.101743)\n",
      "\n",
      "Epoch: [86][510/600]\t Time 0.677 (0.677)\t Loss 0.073808 (0.101592)\n",
      "\n",
      "Epoch: [86][520/600]\t Time 0.672 (0.677)\t Loss 0.131069 (0.101807)\n",
      "\n",
      "Epoch: [86][530/600]\t Time 0.675 (0.677)\t Loss 0.115783 (0.101724)\n",
      "\n",
      "Epoch: [86][540/600]\t Time 0.677 (0.677)\t Loss 0.070685 (0.102000)\n",
      "\n",
      "Epoch: [86][550/600]\t Time 0.676 (0.677)\t Loss 0.097674 (0.102037)\n",
      "\n",
      "Epoch: [86][560/600]\t Time 0.682 (0.677)\t Loss 0.113427 (0.102161)\n",
      "\n",
      "Epoch: [86][570/600]\t Time 0.680 (0.677)\t Loss 0.125539 (0.102365)\n",
      "\n",
      "Epoch: [86][580/600]\t Time 0.674 (0.677)\t Loss 0.107666 (0.102386)\n",
      "\n",
      "Epoch: [86][590/600]\t Time 0.686 (0.677)\t Loss 0.120977 (0.102309)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.034 (1.034)\t Loss 0.072122 (0.072122)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.645 (0.686)\t Loss 0.101558 (0.095175)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.649 (0.667)\t Loss 0.113972 (0.099237)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.644 (0.661)\t Loss 0.119236 (0.103232)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.648 (0.657)\t Loss 0.133871 (0.103456)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.651 (0.655)\t Loss 0.081295 (0.103708)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.644 (0.653)\t Loss 0.059422 (0.102616)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.642 (0.652)\t Loss 0.117503 (0.102597)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.637 (0.651)\t Loss 0.107002 (0.103252)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.643 (0.650)\t Loss 0.121962 (0.103166)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.654 (0.650)\t Loss 0.099187 (0.103817)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.652 (0.650)\t Loss 0.099785 (0.104235)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.645 (0.649)\t Loss 0.101119 (0.103538)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.652 (0.649)\t Loss 0.122123 (0.103773)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.645 (0.649)\t Loss 0.069601 (0.104197)\n",
      "\n",
      "Epoch: [86]\t Train Loss: 0.102363  \t Val Loss: 0.103784\n",
      "\n",
      "False 0.10335628673434258\n",
      "Epoch: [87][0/600]\t Time 1.046 (1.046)\t Loss 0.105664 (0.105664)\n",
      "\n",
      "Epoch: [87][10/600]\t Time 0.679 (0.713)\t Loss 0.092975 (0.100135)\n",
      "\n",
      "Epoch: [87][20/600]\t Time 0.675 (0.695)\t Loss 0.153223 (0.106468)\n",
      "\n",
      "Epoch: [87][30/600]\t Time 0.678 (0.689)\t Loss 0.103534 (0.106440)\n",
      "\n",
      "Epoch: [87][40/600]\t Time 0.670 (0.686)\t Loss 0.117796 (0.105521)\n",
      "\n",
      "Epoch: [87][50/600]\t Time 0.672 (0.684)\t Loss 0.089061 (0.104094)\n",
      "\n",
      "Epoch: [87][60/600]\t Time 0.676 (0.683)\t Loss 0.088133 (0.103593)\n",
      "\n",
      "Epoch: [87][70/600]\t Time 0.676 (0.682)\t Loss 0.126889 (0.102923)\n",
      "\n",
      "Epoch: [87][80/600]\t Time 0.686 (0.681)\t Loss 0.100693 (0.102347)\n",
      "\n",
      "Epoch: [87][90/600]\t Time 0.676 (0.681)\t Loss 0.112399 (0.102151)\n",
      "\n",
      "Epoch: [87][100/600]\t Time 0.677 (0.680)\t Loss 0.090151 (0.101613)\n",
      "\n",
      "Epoch: [87][110/600]\t Time 0.687 (0.680)\t Loss 0.100063 (0.102026)\n",
      "\n",
      "Epoch: [87][120/600]\t Time 0.671 (0.679)\t Loss 0.125883 (0.102758)\n",
      "\n",
      "Epoch: [87][130/600]\t Time 0.682 (0.679)\t Loss 0.103674 (0.102252)\n",
      "\n",
      "Epoch: [87][140/600]\t Time 0.685 (0.679)\t Loss 0.064617 (0.101834)\n",
      "\n",
      "Epoch: [87][150/600]\t Time 0.674 (0.679)\t Loss 0.089436 (0.101201)\n",
      "\n",
      "Epoch: [87][160/600]\t Time 0.671 (0.679)\t Loss 0.086225 (0.101221)\n",
      "\n",
      "Epoch: [87][170/600]\t Time 0.681 (0.678)\t Loss 0.117208 (0.101595)\n",
      "\n",
      "Epoch: [87][180/600]\t Time 0.674 (0.678)\t Loss 0.114111 (0.102110)\n",
      "\n",
      "Epoch: [87][190/600]\t Time 0.675 (0.678)\t Loss 0.111092 (0.101952)\n",
      "\n",
      "Epoch: [87][200/600]\t Time 0.685 (0.678)\t Loss 0.094213 (0.101862)\n",
      "\n",
      "Epoch: [87][210/600]\t Time 0.675 (0.678)\t Loss 0.125081 (0.102020)\n",
      "\n",
      "Epoch: [87][220/600]\t Time 0.674 (0.678)\t Loss 0.154581 (0.102346)\n",
      "\n",
      "Epoch: [87][230/600]\t Time 0.677 (0.678)\t Loss 0.109926 (0.102376)\n",
      "\n",
      "Epoch: [87][240/600]\t Time 0.676 (0.678)\t Loss 0.103151 (0.103030)\n",
      "\n",
      "Epoch: [87][250/600]\t Time 0.669 (0.678)\t Loss 0.097161 (0.103216)\n",
      "\n",
      "Epoch: [87][260/600]\t Time 0.665 (0.677)\t Loss 0.088405 (0.103208)\n",
      "\n",
      "Epoch: [87][270/600]\t Time 0.678 (0.677)\t Loss 0.111846 (0.103471)\n",
      "\n",
      "Epoch: [87][280/600]\t Time 0.674 (0.677)\t Loss 0.091087 (0.103320)\n",
      "\n",
      "Epoch: [87][290/600]\t Time 0.666 (0.677)\t Loss 0.094299 (0.103372)\n",
      "\n",
      "Epoch: [87][300/600]\t Time 0.676 (0.677)\t Loss 0.103597 (0.103293)\n",
      "\n",
      "Epoch: [87][310/600]\t Time 0.674 (0.677)\t Loss 0.075622 (0.103419)\n",
      "\n",
      "Epoch: [87][320/600]\t Time 0.665 (0.677)\t Loss 0.113509 (0.103208)\n",
      "\n",
      "Epoch: [87][330/600]\t Time 0.676 (0.677)\t Loss 0.144484 (0.103289)\n",
      "\n",
      "Epoch: [87][340/600]\t Time 0.678 (0.677)\t Loss 0.097040 (0.103063)\n",
      "\n",
      "Epoch: [87][350/600]\t Time 0.668 (0.677)\t Loss 0.136617 (0.103232)\n",
      "\n",
      "Epoch: [87][360/600]\t Time 0.677 (0.677)\t Loss 0.090160 (0.103266)\n",
      "\n",
      "Epoch: [87][370/600]\t Time 0.678 (0.677)\t Loss 0.115109 (0.103335)\n",
      "\n",
      "Epoch: [87][380/600]\t Time 0.672 (0.677)\t Loss 0.078397 (0.103135)\n",
      "\n",
      "Epoch: [87][390/600]\t Time 0.678 (0.677)\t Loss 0.103200 (0.103189)\n",
      "\n",
      "Epoch: [87][400/600]\t Time 0.675 (0.677)\t Loss 0.092523 (0.103315)\n",
      "\n",
      "Epoch: [87][410/600]\t Time 0.683 (0.677)\t Loss 0.096578 (0.103270)\n",
      "\n",
      "Epoch: [87][420/600]\t Time 0.676 (0.677)\t Loss 0.090355 (0.103111)\n",
      "\n",
      "Epoch: [87][430/600]\t Time 0.685 (0.677)\t Loss 0.127730 (0.103211)\n",
      "\n",
      "Epoch: [87][440/600]\t Time 0.672 (0.677)\t Loss 0.085401 (0.103204)\n",
      "\n",
      "Epoch: [87][450/600]\t Time 0.687 (0.677)\t Loss 0.106605 (0.103274)\n",
      "\n",
      "Epoch: [87][460/600]\t Time 0.684 (0.677)\t Loss 0.078808 (0.103168)\n",
      "\n",
      "Epoch: [87][470/600]\t Time 0.673 (0.677)\t Loss 0.064309 (0.103069)\n",
      "\n",
      "Epoch: [87][480/600]\t Time 0.679 (0.677)\t Loss 0.127885 (0.102908)\n",
      "\n",
      "Epoch: [87][490/600]\t Time 0.676 (0.677)\t Loss 0.121378 (0.102748)\n",
      "\n",
      "Epoch: [87][500/600]\t Time 0.668 (0.677)\t Loss 0.107275 (0.102609)\n",
      "\n",
      "Epoch: [87][510/600]\t Time 0.676 (0.677)\t Loss 0.103056 (0.102574)\n",
      "\n",
      "Epoch: [87][520/600]\t Time 0.678 (0.677)\t Loss 0.073611 (0.102437)\n",
      "\n",
      "Epoch: [87][530/600]\t Time 0.671 (0.677)\t Loss 0.085284 (0.102530)\n",
      "\n",
      "Epoch: [87][540/600]\t Time 0.680 (0.677)\t Loss 0.107357 (0.102583)\n",
      "\n",
      "Epoch: [87][550/600]\t Time 0.672 (0.677)\t Loss 0.117084 (0.102496)\n",
      "\n",
      "Epoch: [87][560/600]\t Time 0.680 (0.677)\t Loss 0.133617 (0.102509)\n",
      "\n",
      "Epoch: [87][570/600]\t Time 0.677 (0.677)\t Loss 0.075143 (0.102220)\n",
      "\n",
      "Epoch: [87][580/600]\t Time 0.679 (0.677)\t Loss 0.124096 (0.102249)\n",
      "\n",
      "Epoch: [87][590/600]\t Time 0.675 (0.677)\t Loss 0.087772 (0.102351)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.009 (1.009)\t Loss 0.093460 (0.093460)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.647 (0.686)\t Loss 0.100199 (0.095503)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.643 (0.668)\t Loss 0.117721 (0.098524)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.651 (0.661)\t Loss 0.116252 (0.103481)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.642 (0.657)\t Loss 0.091788 (0.103750)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.643 (0.655)\t Loss 0.126260 (0.105680)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.643 (0.653)\t Loss 0.123637 (0.105973)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.641 (0.652)\t Loss 0.107075 (0.105556)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.637 (0.651)\t Loss 0.104362 (0.104674)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.646 (0.650)\t Loss 0.130211 (0.105171)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.641 (0.649)\t Loss 0.081303 (0.103645)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.637 (0.649)\t Loss 0.125831 (0.104050)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.647 (0.649)\t Loss 0.105200 (0.103535)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.649 (0.648)\t Loss 0.126031 (0.103531)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.653 (0.648)\t Loss 0.075853 (0.103173)\n",
      "\n",
      "Epoch: [87]\t Train Loss: 0.102356  \t Val Loss: 0.103347\n",
      "\n",
      "True 0.10334676213562488\n",
      "Epoch: [88][0/600]\t Time 1.077 (1.077)\t Loss 0.102318 (0.102318)\n",
      "\n",
      "Epoch: [88][10/600]\t Time 0.675 (0.715)\t Loss 0.088635 (0.102363)\n",
      "\n",
      "Epoch: [88][20/600]\t Time 0.676 (0.696)\t Loss 0.084724 (0.105852)\n",
      "\n",
      "Epoch: [88][30/600]\t Time 0.672 (0.690)\t Loss 0.100012 (0.103459)\n",
      "\n",
      "Epoch: [88][40/600]\t Time 0.678 (0.686)\t Loss 0.106109 (0.103568)\n",
      "\n",
      "Epoch: [88][50/600]\t Time 0.677 (0.684)\t Loss 0.088234 (0.103121)\n",
      "\n",
      "Epoch: [88][60/600]\t Time 0.667 (0.682)\t Loss 0.094699 (0.104002)\n",
      "\n",
      "Epoch: [88][70/600]\t Time 0.676 (0.682)\t Loss 0.116682 (0.103409)\n",
      "\n",
      "Epoch: [88][80/600]\t Time 0.679 (0.681)\t Loss 0.088561 (0.102283)\n",
      "\n",
      "Epoch: [88][90/600]\t Time 0.676 (0.681)\t Loss 0.117367 (0.101553)\n",
      "\n",
      "Epoch: [88][100/600]\t Time 0.679 (0.680)\t Loss 0.079663 (0.101372)\n",
      "\n",
      "Epoch: [88][110/600]\t Time 0.673 (0.680)\t Loss 0.087401 (0.101739)\n",
      "\n",
      "Epoch: [88][120/600]\t Time 0.677 (0.680)\t Loss 0.063061 (0.101741)\n",
      "\n",
      "Epoch: [88][130/600]\t Time 0.677 (0.679)\t Loss 0.066350 (0.101086)\n",
      "\n",
      "Epoch: [88][140/600]\t Time 0.677 (0.679)\t Loss 0.131881 (0.101102)\n",
      "\n",
      "Epoch: [88][150/600]\t Time 0.678 (0.679)\t Loss 0.111275 (0.101046)\n",
      "\n",
      "Epoch: [88][160/600]\t Time 0.677 (0.679)\t Loss 0.092893 (0.101303)\n",
      "\n",
      "Epoch: [88][170/600]\t Time 0.685 (0.679)\t Loss 0.095819 (0.100762)\n",
      "\n",
      "Epoch: [88][180/600]\t Time 0.672 (0.679)\t Loss 0.106808 (0.101015)\n",
      "\n",
      "Epoch: [88][190/600]\t Time 0.679 (0.679)\t Loss 0.101682 (0.101203)\n",
      "\n",
      "Epoch: [88][200/600]\t Time 0.676 (0.678)\t Loss 0.106105 (0.101342)\n",
      "\n",
      "Epoch: [88][210/600]\t Time 0.669 (0.678)\t Loss 0.122480 (0.101513)\n",
      "\n",
      "Epoch: [88][220/600]\t Time 0.672 (0.678)\t Loss 0.092344 (0.101493)\n",
      "\n",
      "Epoch: [88][230/600]\t Time 0.676 (0.678)\t Loss 0.094964 (0.101724)\n",
      "\n",
      "Epoch: [88][240/600]\t Time 0.674 (0.678)\t Loss 0.083284 (0.101099)\n",
      "\n",
      "Epoch: [88][250/600]\t Time 0.674 (0.678)\t Loss 0.075560 (0.101098)\n",
      "\n",
      "Epoch: [88][260/600]\t Time 0.678 (0.678)\t Loss 0.127254 (0.101264)\n",
      "\n",
      "Epoch: [88][270/600]\t Time 0.676 (0.678)\t Loss 0.093141 (0.101407)\n",
      "\n",
      "Epoch: [88][280/600]\t Time 0.679 (0.678)\t Loss 0.098521 (0.101348)\n",
      "\n",
      "Epoch: [88][290/600]\t Time 0.675 (0.678)\t Loss 0.120213 (0.101469)\n",
      "\n",
      "Epoch: [88][300/600]\t Time 0.677 (0.678)\t Loss 0.076793 (0.100919)\n",
      "\n",
      "Epoch: [88][310/600]\t Time 0.679 (0.678)\t Loss 0.147091 (0.101210)\n",
      "\n",
      "Epoch: [88][320/600]\t Time 0.679 (0.678)\t Loss 0.130093 (0.101156)\n",
      "\n",
      "Epoch: [88][330/600]\t Time 0.673 (0.678)\t Loss 0.090433 (0.101151)\n",
      "\n",
      "Epoch: [88][340/600]\t Time 0.684 (0.678)\t Loss 0.093414 (0.101296)\n",
      "\n",
      "Epoch: [88][350/600]\t Time 0.673 (0.678)\t Loss 0.123533 (0.101446)\n",
      "\n",
      "Epoch: [88][360/600]\t Time 0.673 (0.678)\t Loss 0.068432 (0.101225)\n",
      "\n",
      "Epoch: [88][370/600]\t Time 0.679 (0.678)\t Loss 0.074716 (0.101466)\n",
      "\n",
      "Epoch: [88][380/600]\t Time 0.676 (0.678)\t Loss 0.092586 (0.101626)\n",
      "\n",
      "Epoch: [88][390/600]\t Time 0.675 (0.678)\t Loss 0.086683 (0.101564)\n",
      "\n",
      "Epoch: [88][400/600]\t Time 0.662 (0.678)\t Loss 0.102565 (0.101646)\n",
      "\n",
      "Epoch: [88][410/600]\t Time 0.677 (0.678)\t Loss 0.094600 (0.101804)\n",
      "\n",
      "Epoch: [88][420/600]\t Time 0.678 (0.677)\t Loss 0.149001 (0.102045)\n",
      "\n",
      "Epoch: [88][430/600]\t Time 0.672 (0.677)\t Loss 0.125216 (0.102049)\n",
      "\n",
      "Epoch: [88][440/600]\t Time 0.680 (0.677)\t Loss 0.099042 (0.101929)\n",
      "\n",
      "Epoch: [88][450/600]\t Time 0.674 (0.677)\t Loss 0.111688 (0.101951)\n",
      "\n",
      "Epoch: [88][460/600]\t Time 0.672 (0.677)\t Loss 0.154349 (0.102289)\n",
      "\n",
      "Epoch: [88][470/600]\t Time 0.678 (0.677)\t Loss 0.104487 (0.102339)\n",
      "\n",
      "Epoch: [88][480/600]\t Time 0.675 (0.677)\t Loss 0.102246 (0.102415)\n",
      "\n",
      "Epoch: [88][490/600]\t Time 0.667 (0.677)\t Loss 0.102656 (0.102330)\n",
      "\n",
      "Epoch: [88][500/600]\t Time 0.679 (0.677)\t Loss 0.091640 (0.102318)\n",
      "\n",
      "Epoch: [88][510/600]\t Time 0.676 (0.677)\t Loss 0.064363 (0.102211)\n",
      "\n",
      "Epoch: [88][520/600]\t Time 0.678 (0.677)\t Loss 0.117923 (0.102165)\n",
      "\n",
      "Epoch: [88][530/600]\t Time 0.675 (0.677)\t Loss 0.134007 (0.102348)\n",
      "\n",
      "Epoch: [88][540/600]\t Time 0.672 (0.677)\t Loss 0.107015 (0.102289)\n",
      "\n",
      "Epoch: [88][550/600]\t Time 0.672 (0.677)\t Loss 0.085603 (0.102192)\n",
      "\n",
      "Epoch: [88][560/600]\t Time 0.677 (0.677)\t Loss 0.116637 (0.102058)\n",
      "\n",
      "Epoch: [88][570/600]\t Time 0.674 (0.677)\t Loss 0.107035 (0.102187)\n",
      "\n",
      "Epoch: [88][580/600]\t Time 0.680 (0.677)\t Loss 0.111894 (0.102332)\n",
      "\n",
      "Epoch: [88][590/600]\t Time 0.678 (0.677)\t Loss 0.109490 (0.102290)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.023 (1.023)\t Loss 0.118852 (0.118852)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.642 (0.685)\t Loss 0.074293 (0.112551)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.646 (0.667)\t Loss 0.095345 (0.105977)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.644 (0.660)\t Loss 0.168125 (0.104683)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.648 (0.657)\t Loss 0.088874 (0.103450)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.650 (0.655)\t Loss 0.113465 (0.102079)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.641 (0.653)\t Loss 0.112940 (0.102050)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.643 (0.652)\t Loss 0.090334 (0.102030)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.641 (0.652)\t Loss 0.111813 (0.103047)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.641 (0.651)\t Loss 0.089569 (0.102324)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.654 (0.651)\t Loss 0.119560 (0.102124)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.653 (0.651)\t Loss 0.121914 (0.102323)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.646 (0.651)\t Loss 0.082918 (0.103321)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.649 (0.650)\t Loss 0.124688 (0.104050)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.652 (0.650)\t Loss 0.077806 (0.103666)\n",
      "\n",
      "Epoch: [88]\t Train Loss: 0.102334  \t Val Loss: 0.103682\n",
      "\n",
      "False 0.10334676213562488\n",
      "Epoch: [89][0/600]\t Time 1.051 (1.051)\t Loss 0.082674 (0.082674)\n",
      "\n",
      "Epoch: [89][10/600]\t Time 0.686 (0.715)\t Loss 0.112216 (0.099144)\n",
      "\n",
      "Epoch: [89][20/600]\t Time 0.670 (0.697)\t Loss 0.109202 (0.102673)\n",
      "\n",
      "Epoch: [89][30/600]\t Time 0.675 (0.689)\t Loss 0.122474 (0.103279)\n",
      "\n",
      "Epoch: [89][40/600]\t Time 0.671 (0.685)\t Loss 0.083184 (0.101415)\n",
      "\n",
      "Epoch: [89][50/600]\t Time 0.675 (0.683)\t Loss 0.092328 (0.100943)\n",
      "\n",
      "Epoch: [89][60/600]\t Time 0.676 (0.682)\t Loss 0.121383 (0.103427)\n",
      "\n",
      "Epoch: [89][70/600]\t Time 0.677 (0.681)\t Loss 0.077972 (0.101739)\n",
      "\n",
      "Epoch: [89][80/600]\t Time 0.676 (0.680)\t Loss 0.095907 (0.101047)\n",
      "\n",
      "Epoch: [89][90/600]\t Time 0.678 (0.679)\t Loss 0.056361 (0.101225)\n",
      "\n",
      "Epoch: [89][100/600]\t Time 0.675 (0.678)\t Loss 0.063416 (0.101342)\n",
      "\n",
      "Epoch: [89][110/600]\t Time 0.675 (0.679)\t Loss 0.092333 (0.101671)\n",
      "\n",
      "Epoch: [89][120/600]\t Time 0.671 (0.678)\t Loss 0.069774 (0.101604)\n",
      "\n",
      "Epoch: [89][130/600]\t Time 0.679 (0.678)\t Loss 0.135755 (0.101539)\n",
      "\n",
      "Epoch: [89][140/600]\t Time 0.673 (0.678)\t Loss 0.105670 (0.102200)\n",
      "\n",
      "Epoch: [89][150/600]\t Time 0.675 (0.678)\t Loss 0.108391 (0.102401)\n",
      "\n",
      "Epoch: [89][160/600]\t Time 0.676 (0.678)\t Loss 0.100816 (0.102681)\n",
      "\n",
      "Epoch: [89][170/600]\t Time 0.675 (0.678)\t Loss 0.087892 (0.102282)\n",
      "\n",
      "Epoch: [89][180/600]\t Time 0.671 (0.677)\t Loss 0.105933 (0.101934)\n",
      "\n",
      "Epoch: [89][190/600]\t Time 0.678 (0.677)\t Loss 0.111695 (0.101679)\n",
      "\n",
      "Epoch: [89][200/600]\t Time 0.677 (0.677)\t Loss 0.120940 (0.101959)\n",
      "\n",
      "Epoch: [89][210/600]\t Time 0.674 (0.677)\t Loss 0.073514 (0.101338)\n",
      "\n",
      "Epoch: [89][220/600]\t Time 0.680 (0.677)\t Loss 0.086381 (0.101438)\n",
      "\n",
      "Epoch: [89][230/600]\t Time 0.672 (0.677)\t Loss 0.093975 (0.101428)\n",
      "\n",
      "Epoch: [89][240/600]\t Time 0.675 (0.677)\t Loss 0.102060 (0.101658)\n",
      "\n",
      "Epoch: [89][250/600]\t Time 0.681 (0.677)\t Loss 0.114062 (0.101662)\n",
      "\n",
      "Epoch: [89][260/600]\t Time 0.674 (0.677)\t Loss 0.101605 (0.101557)\n",
      "\n",
      "Epoch: [89][270/600]\t Time 0.680 (0.677)\t Loss 0.114168 (0.101576)\n",
      "\n",
      "Epoch: [89][280/600]\t Time 0.676 (0.677)\t Loss 0.098325 (0.101405)\n",
      "\n",
      "Epoch: [89][290/600]\t Time 0.673 (0.677)\t Loss 0.091466 (0.101144)\n",
      "\n",
      "Epoch: [89][300/600]\t Time 0.687 (0.677)\t Loss 0.109471 (0.100762)\n",
      "\n",
      "Epoch: [89][310/600]\t Time 0.682 (0.677)\t Loss 0.114957 (0.100908)\n",
      "\n",
      "Epoch: [89][320/600]\t Time 0.673 (0.677)\t Loss 0.107943 (0.100933)\n",
      "\n",
      "Epoch: [89][330/600]\t Time 0.683 (0.677)\t Loss 0.110742 (0.101099)\n",
      "\n",
      "Epoch: [89][340/600]\t Time 0.677 (0.677)\t Loss 0.084502 (0.101061)\n",
      "\n",
      "Epoch: [89][350/600]\t Time 0.667 (0.677)\t Loss 0.127213 (0.100868)\n",
      "\n",
      "Epoch: [89][360/600]\t Time 0.670 (0.677)\t Loss 0.068612 (0.100603)\n",
      "\n",
      "Epoch: [89][370/600]\t Time 0.678 (0.677)\t Loss 0.062570 (0.100488)\n",
      "\n",
      "Epoch: [89][380/600]\t Time 0.672 (0.677)\t Loss 0.118089 (0.100693)\n",
      "\n",
      "Epoch: [89][390/600]\t Time 0.676 (0.677)\t Loss 0.097373 (0.100769)\n",
      "\n",
      "Epoch: [89][400/600]\t Time 0.679 (0.677)\t Loss 0.104694 (0.100808)\n",
      "\n",
      "Epoch: [89][410/600]\t Time 0.677 (0.677)\t Loss 0.107142 (0.100678)\n",
      "\n",
      "Epoch: [89][420/600]\t Time 0.677 (0.677)\t Loss 0.124242 (0.100942)\n",
      "\n",
      "Epoch: [89][430/600]\t Time 0.674 (0.677)\t Loss 0.103850 (0.101002)\n",
      "\n",
      "Epoch: [89][440/600]\t Time 0.678 (0.677)\t Loss 0.094961 (0.101316)\n",
      "\n",
      "Epoch: [89][450/600]\t Time 0.674 (0.677)\t Loss 0.125655 (0.101320)\n",
      "\n",
      "Epoch: [89][460/600]\t Time 0.681 (0.677)\t Loss 0.126309 (0.101534)\n",
      "\n",
      "Epoch: [89][470/600]\t Time 0.672 (0.677)\t Loss 0.096886 (0.101683)\n",
      "\n",
      "Epoch: [89][480/600]\t Time 0.682 (0.677)\t Loss 0.092909 (0.101759)\n",
      "\n",
      "Epoch: [89][490/600]\t Time 0.675 (0.677)\t Loss 0.178259 (0.101951)\n",
      "\n",
      "Epoch: [89][500/600]\t Time 0.673 (0.677)\t Loss 0.122281 (0.102170)\n",
      "\n",
      "Epoch: [89][510/600]\t Time 0.665 (0.677)\t Loss 0.150527 (0.101982)\n",
      "\n",
      "Epoch: [89][520/600]\t Time 0.676 (0.677)\t Loss 0.124187 (0.102162)\n",
      "\n",
      "Epoch: [89][530/600]\t Time 0.676 (0.677)\t Loss 0.101795 (0.101957)\n",
      "\n",
      "Epoch: [89][540/600]\t Time 0.671 (0.677)\t Loss 0.088907 (0.101806)\n",
      "\n",
      "Epoch: [89][550/600]\t Time 0.680 (0.677)\t Loss 0.084495 (0.101976)\n",
      "\n",
      "Epoch: [89][560/600]\t Time 0.674 (0.677)\t Loss 0.124171 (0.102120)\n",
      "\n",
      "Epoch: [89][570/600]\t Time 0.681 (0.677)\t Loss 0.078965 (0.102008)\n",
      "\n",
      "Epoch: [89][580/600]\t Time 0.677 (0.677)\t Loss 0.078724 (0.101944)\n",
      "\n",
      "Epoch: [89][590/600]\t Time 0.679 (0.677)\t Loss 0.124583 (0.102138)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.005 (1.005)\t Loss 0.083416 (0.083416)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.649 (0.682)\t Loss 0.136850 (0.095251)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.651 (0.666)\t Loss 0.089617 (0.095656)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.650 (0.659)\t Loss 0.134120 (0.095895)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.646 (0.656)\t Loss 0.118402 (0.099947)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.650 (0.654)\t Loss 0.076440 (0.100412)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.642 (0.653)\t Loss 0.138655 (0.102801)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.642 (0.652)\t Loss 0.104456 (0.102961)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.639 (0.652)\t Loss 0.093825 (0.102920)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.648 (0.651)\t Loss 0.111187 (0.102657)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.654 (0.651)\t Loss 0.103456 (0.102941)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.657 (0.651)\t Loss 0.128331 (0.102851)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.640 (0.651)\t Loss 0.107683 (0.102404)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.649 (0.650)\t Loss 0.084656 (0.103019)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.650 (0.650)\t Loss 0.111631 (0.103148)\n",
      "\n",
      "Epoch: [89]\t Train Loss: 0.102192  \t Val Loss: 0.103644\n",
      "\n",
      "False 0.10334676213562488\n",
      "Epoch: [90][0/600]\t Time 1.058 (1.058)\t Loss 0.081695 (0.081695)\n",
      "\n",
      "Epoch: [90][10/600]\t Time 0.675 (0.713)\t Loss 0.095468 (0.096164)\n",
      "\n",
      "Epoch: [90][20/600]\t Time 0.684 (0.696)\t Loss 0.133698 (0.103200)\n",
      "\n",
      "Epoch: [90][30/600]\t Time 0.672 (0.689)\t Loss 0.085323 (0.101537)\n",
      "\n",
      "Epoch: [90][40/600]\t Time 0.681 (0.686)\t Loss 0.108501 (0.101242)\n",
      "\n",
      "Epoch: [90][50/600]\t Time 0.685 (0.684)\t Loss 0.077962 (0.099335)\n",
      "\n",
      "Epoch: [90][60/600]\t Time 0.675 (0.683)\t Loss 0.075015 (0.099571)\n",
      "\n",
      "Epoch: [90][70/600]\t Time 0.670 (0.682)\t Loss 0.092070 (0.099269)\n",
      "\n",
      "Epoch: [90][80/600]\t Time 0.675 (0.680)\t Loss 0.082381 (0.100751)\n",
      "\n",
      "Epoch: [90][90/600]\t Time 0.684 (0.680)\t Loss 0.076363 (0.100122)\n",
      "\n",
      "Epoch: [90][100/600]\t Time 0.671 (0.679)\t Loss 0.077501 (0.100080)\n",
      "\n",
      "Epoch: [90][110/600]\t Time 0.675 (0.679)\t Loss 0.129302 (0.100812)\n",
      "\n",
      "Epoch: [90][120/600]\t Time 0.672 (0.678)\t Loss 0.125045 (0.101206)\n",
      "\n",
      "Epoch: [90][130/600]\t Time 0.675 (0.678)\t Loss 0.089133 (0.101208)\n",
      "\n",
      "Epoch: [90][140/600]\t Time 0.674 (0.678)\t Loss 0.116202 (0.101604)\n",
      "\n",
      "Epoch: [90][150/600]\t Time 0.673 (0.678)\t Loss 0.067285 (0.101888)\n",
      "\n",
      "Epoch: [90][160/600]\t Time 0.675 (0.677)\t Loss 0.106598 (0.101796)\n",
      "\n",
      "Epoch: [90][170/600]\t Time 0.672 (0.677)\t Loss 0.069345 (0.101979)\n",
      "\n",
      "Epoch: [90][180/600]\t Time 0.673 (0.677)\t Loss 0.115676 (0.101891)\n",
      "\n",
      "Epoch: [90][190/600]\t Time 0.674 (0.677)\t Loss 0.098960 (0.102146)\n",
      "\n",
      "Epoch: [90][200/600]\t Time 0.678 (0.677)\t Loss 0.096133 (0.102543)\n",
      "\n",
      "Epoch: [90][210/600]\t Time 0.678 (0.677)\t Loss 0.096778 (0.102629)\n",
      "\n",
      "Epoch: [90][220/600]\t Time 0.678 (0.677)\t Loss 0.095279 (0.102306)\n",
      "\n",
      "Epoch: [90][230/600]\t Time 0.679 (0.677)\t Loss 0.118209 (0.102021)\n",
      "\n",
      "Epoch: [90][240/600]\t Time 0.671 (0.677)\t Loss 0.119581 (0.102096)\n",
      "\n",
      "Epoch: [90][250/600]\t Time 0.680 (0.677)\t Loss 0.086011 (0.102441)\n",
      "\n",
      "Epoch: [90][260/600]\t Time 0.675 (0.677)\t Loss 0.096983 (0.102538)\n",
      "\n",
      "Epoch: [90][270/600]\t Time 0.672 (0.677)\t Loss 0.078471 (0.102560)\n",
      "\n",
      "Epoch: [90][280/600]\t Time 0.678 (0.677)\t Loss 0.078702 (0.102159)\n",
      "\n",
      "Epoch: [90][290/600]\t Time 0.681 (0.677)\t Loss 0.096539 (0.102171)\n",
      "\n",
      "Epoch: [90][300/600]\t Time 0.672 (0.677)\t Loss 0.101258 (0.102177)\n",
      "\n",
      "Epoch: [90][310/600]\t Time 0.687 (0.677)\t Loss 0.124397 (0.102036)\n",
      "\n",
      "Epoch: [90][320/600]\t Time 0.682 (0.677)\t Loss 0.069961 (0.101978)\n",
      "\n",
      "Epoch: [90][330/600]\t Time 0.668 (0.677)\t Loss 0.076360 (0.101755)\n",
      "\n",
      "Epoch: [90][340/600]\t Time 0.683 (0.677)\t Loss 0.098425 (0.101915)\n",
      "\n",
      "Epoch: [90][350/600]\t Time 0.677 (0.677)\t Loss 0.095697 (0.101605)\n",
      "\n",
      "Epoch: [90][360/600]\t Time 0.666 (0.677)\t Loss 0.086519 (0.101941)\n",
      "\n",
      "Epoch: [90][370/600]\t Time 0.680 (0.677)\t Loss 0.110417 (0.101770)\n",
      "\n",
      "Epoch: [90][380/600]\t Time 0.672 (0.677)\t Loss 0.073799 (0.101661)\n",
      "\n",
      "Epoch: [90][390/600]\t Time 0.684 (0.677)\t Loss 0.119784 (0.101810)\n",
      "\n",
      "Epoch: [90][400/600]\t Time 0.677 (0.677)\t Loss 0.109015 (0.101969)\n",
      "\n",
      "Epoch: [90][410/600]\t Time 0.680 (0.677)\t Loss 0.104077 (0.102052)\n",
      "\n",
      "Epoch: [90][420/600]\t Time 0.678 (0.677)\t Loss 0.103435 (0.102059)\n",
      "\n",
      "Epoch: [90][430/600]\t Time 0.681 (0.677)\t Loss 0.100185 (0.102161)\n",
      "\n",
      "Epoch: [90][440/600]\t Time 0.680 (0.677)\t Loss 0.097266 (0.102111)\n",
      "\n",
      "Epoch: [90][450/600]\t Time 0.667 (0.677)\t Loss 0.076775 (0.101932)\n",
      "\n",
      "Epoch: [90][460/600]\t Time 0.677 (0.677)\t Loss 0.093311 (0.101987)\n",
      "\n",
      "Epoch: [90][470/600]\t Time 0.673 (0.677)\t Loss 0.113411 (0.102158)\n",
      "\n",
      "Epoch: [90][480/600]\t Time 0.675 (0.677)\t Loss 0.072323 (0.102351)\n",
      "\n",
      "Epoch: [90][490/600]\t Time 0.677 (0.677)\t Loss 0.096059 (0.102149)\n",
      "\n",
      "Epoch: [90][500/600]\t Time 0.676 (0.677)\t Loss 0.097054 (0.102340)\n",
      "\n",
      "Epoch: [90][510/600]\t Time 0.676 (0.677)\t Loss 0.098796 (0.102124)\n",
      "\n",
      "Epoch: [90][520/600]\t Time 0.680 (0.677)\t Loss 0.107973 (0.102079)\n",
      "\n",
      "Epoch: [90][530/600]\t Time 0.674 (0.677)\t Loss 0.150864 (0.102312)\n",
      "\n",
      "Epoch: [90][540/600]\t Time 0.684 (0.677)\t Loss 0.085183 (0.102191)\n",
      "\n",
      "Epoch: [90][550/600]\t Time 0.677 (0.677)\t Loss 0.120659 (0.102357)\n",
      "\n",
      "Epoch: [90][560/600]\t Time 0.666 (0.677)\t Loss 0.084990 (0.102340)\n",
      "\n",
      "Epoch: [90][570/600]\t Time 0.677 (0.677)\t Loss 0.089087 (0.102282)\n",
      "\n",
      "Epoch: [90][580/600]\t Time 0.677 (0.677)\t Loss 0.109872 (0.102480)\n",
      "\n",
      "Epoch: [90][590/600]\t Time 0.675 (0.677)\t Loss 0.095371 (0.102358)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.039 (1.039)\t Loss 0.122065 (0.122065)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.640 (0.688)\t Loss 0.079537 (0.101230)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.649 (0.669)\t Loss 0.090161 (0.103744)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.652 (0.662)\t Loss 0.119845 (0.106012)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.651 (0.658)\t Loss 0.109458 (0.106106)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.649 (0.656)\t Loss 0.086371 (0.104404)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.651 (0.655)\t Loss 0.072870 (0.102992)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.649 (0.653)\t Loss 0.098584 (0.102642)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.639 (0.652)\t Loss 0.090588 (0.103009)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.638 (0.651)\t Loss 0.090019 (0.102425)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.652 (0.651)\t Loss 0.125574 (0.102538)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.647 (0.651)\t Loss 0.106720 (0.102723)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.645 (0.650)\t Loss 0.084294 (0.102386)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.644 (0.650)\t Loss 0.072758 (0.103195)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.651 (0.650)\t Loss 0.083955 (0.103110)\n",
      "\n",
      "Epoch: [90]\t Train Loss: 0.102559  \t Val Loss: 0.103763\n",
      "\n",
      "False 0.10334676213562488\n",
      "Epoch: [91][0/600]\t Time 1.038 (1.038)\t Loss 0.102764 (0.102764)\n",
      "\n",
      "Epoch: [91][10/600]\t Time 0.676 (0.710)\t Loss 0.092230 (0.115555)\n",
      "\n",
      "Epoch: [91][20/600]\t Time 0.674 (0.693)\t Loss 0.072734 (0.108663)\n",
      "\n",
      "Epoch: [91][30/600]\t Time 0.674 (0.689)\t Loss 0.128182 (0.106390)\n",
      "\n",
      "Epoch: [91][40/600]\t Time 0.672 (0.685)\t Loss 0.116266 (0.105703)\n",
      "\n",
      "Epoch: [91][50/600]\t Time 0.674 (0.683)\t Loss 0.096100 (0.104534)\n",
      "\n",
      "Epoch: [91][60/600]\t Time 0.677 (0.682)\t Loss 0.089195 (0.102830)\n",
      "\n",
      "Epoch: [91][70/600]\t Time 0.680 (0.681)\t Loss 0.131038 (0.101518)\n",
      "\n",
      "Epoch: [91][80/600]\t Time 0.676 (0.680)\t Loss 0.078926 (0.102244)\n",
      "\n",
      "Epoch: [91][90/600]\t Time 0.685 (0.680)\t Loss 0.134089 (0.102717)\n",
      "\n",
      "Epoch: [91][100/600]\t Time 0.673 (0.680)\t Loss 0.067522 (0.102812)\n",
      "\n",
      "Epoch: [91][110/600]\t Time 0.685 (0.679)\t Loss 0.141224 (0.103162)\n",
      "\n",
      "Epoch: [91][120/600]\t Time 0.675 (0.679)\t Loss 0.105436 (0.104573)\n",
      "\n",
      "Epoch: [91][130/600]\t Time 0.677 (0.679)\t Loss 0.136493 (0.104462)\n",
      "\n",
      "Epoch: [91][140/600]\t Time 0.688 (0.679)\t Loss 0.104536 (0.104007)\n",
      "\n",
      "Epoch: [91][150/600]\t Time 0.678 (0.679)\t Loss 0.089146 (0.103584)\n",
      "\n",
      "Epoch: [91][160/600]\t Time 0.671 (0.678)\t Loss 0.084162 (0.103744)\n",
      "\n",
      "Epoch: [91][170/600]\t Time 0.684 (0.678)\t Loss 0.122691 (0.103636)\n",
      "\n",
      "Epoch: [91][180/600]\t Time 0.679 (0.678)\t Loss 0.086869 (0.103461)\n",
      "\n",
      "Epoch: [91][190/600]\t Time 0.671 (0.678)\t Loss 0.103852 (0.103438)\n",
      "\n",
      "Epoch: [91][200/600]\t Time 0.673 (0.678)\t Loss 0.100312 (0.103465)\n",
      "\n",
      "Epoch: [91][210/600]\t Time 0.677 (0.677)\t Loss 0.097482 (0.103220)\n",
      "\n",
      "Epoch: [91][220/600]\t Time 0.675 (0.677)\t Loss 0.120964 (0.103320)\n",
      "\n",
      "Epoch: [91][230/600]\t Time 0.675 (0.677)\t Loss 0.106545 (0.103302)\n",
      "\n",
      "Epoch: [91][240/600]\t Time 0.676 (0.677)\t Loss 0.087740 (0.103077)\n",
      "\n",
      "Epoch: [91][250/600]\t Time 0.684 (0.677)\t Loss 0.149704 (0.103583)\n",
      "\n",
      "Epoch: [91][260/600]\t Time 0.677 (0.677)\t Loss 0.101257 (0.103530)\n",
      "\n",
      "Epoch: [91][270/600]\t Time 0.677 (0.677)\t Loss 0.112270 (0.103321)\n",
      "\n",
      "Epoch: [91][280/600]\t Time 0.677 (0.677)\t Loss 0.091067 (0.102992)\n",
      "\n",
      "Epoch: [91][290/600]\t Time 0.666 (0.677)\t Loss 0.089396 (0.102869)\n",
      "\n",
      "Epoch: [91][300/600]\t Time 0.680 (0.677)\t Loss 0.100772 (0.102607)\n",
      "\n",
      "Epoch: [91][310/600]\t Time 0.675 (0.677)\t Loss 0.079185 (0.102520)\n",
      "\n",
      "Epoch: [91][320/600]\t Time 0.667 (0.677)\t Loss 0.146192 (0.102340)\n",
      "\n",
      "Epoch: [91][330/600]\t Time 0.679 (0.677)\t Loss 0.127666 (0.102689)\n",
      "\n",
      "Epoch: [91][340/600]\t Time 0.679 (0.677)\t Loss 0.089239 (0.102919)\n",
      "\n",
      "Epoch: [91][350/600]\t Time 0.667 (0.677)\t Loss 0.093336 (0.102986)\n",
      "\n",
      "Epoch: [91][360/600]\t Time 0.676 (0.677)\t Loss 0.127596 (0.102907)\n",
      "\n",
      "Epoch: [91][370/600]\t Time 0.678 (0.677)\t Loss 0.099105 (0.102510)\n",
      "\n",
      "Epoch: [91][380/600]\t Time 0.671 (0.677)\t Loss 0.122424 (0.102521)\n",
      "\n",
      "Epoch: [91][390/600]\t Time 0.680 (0.677)\t Loss 0.088319 (0.102506)\n",
      "\n",
      "Epoch: [91][400/600]\t Time 0.675 (0.677)\t Loss 0.118588 (0.102642)\n",
      "\n",
      "Epoch: [91][410/600]\t Time 0.673 (0.677)\t Loss 0.102999 (0.102694)\n",
      "\n",
      "Epoch: [91][420/600]\t Time 0.676 (0.677)\t Loss 0.152288 (0.102789)\n",
      "\n",
      "Epoch: [91][430/600]\t Time 0.685 (0.677)\t Loss 0.127301 (0.102781)\n",
      "\n",
      "Epoch: [91][440/600]\t Time 0.674 (0.677)\t Loss 0.118344 (0.102861)\n",
      "\n",
      "Epoch: [91][450/600]\t Time 0.687 (0.677)\t Loss 0.069668 (0.102752)\n",
      "\n",
      "Epoch: [91][460/600]\t Time 0.676 (0.677)\t Loss 0.088338 (0.102555)\n",
      "\n",
      "Epoch: [91][470/600]\t Time 0.667 (0.677)\t Loss 0.120118 (0.102591)\n",
      "\n",
      "Epoch: [91][480/600]\t Time 0.676 (0.677)\t Loss 0.106494 (0.102720)\n",
      "\n",
      "Epoch: [91][490/600]\t Time 0.673 (0.677)\t Loss 0.085509 (0.102817)\n",
      "\n",
      "Epoch: [91][500/600]\t Time 0.670 (0.677)\t Loss 0.087706 (0.102805)\n",
      "\n",
      "Epoch: [91][510/600]\t Time 0.671 (0.677)\t Loss 0.103879 (0.102796)\n",
      "\n",
      "Epoch: [91][520/600]\t Time 0.677 (0.677)\t Loss 0.095832 (0.102910)\n",
      "\n",
      "Epoch: [91][530/600]\t Time 0.673 (0.677)\t Loss 0.091158 (0.102905)\n",
      "\n",
      "Epoch: [91][540/600]\t Time 0.682 (0.677)\t Loss 0.083583 (0.102984)\n",
      "\n",
      "Epoch: [91][550/600]\t Time 0.675 (0.677)\t Loss 0.080343 (0.102998)\n",
      "\n",
      "Epoch: [91][560/600]\t Time 0.679 (0.677)\t Loss 0.089037 (0.102787)\n",
      "\n",
      "Epoch: [91][570/600]\t Time 0.676 (0.677)\t Loss 0.084531 (0.102668)\n",
      "\n",
      "Epoch: [91][580/600]\t Time 0.679 (0.677)\t Loss 0.102868 (0.102589)\n",
      "\n",
      "Epoch: [91][590/600]\t Time 0.674 (0.677)\t Loss 0.061033 (0.102344)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.011 (1.011)\t Loss 0.061842 (0.061842)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.649 (0.687)\t Loss 0.127148 (0.096211)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.640 (0.668)\t Loss 0.114101 (0.098722)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.652 (0.661)\t Loss 0.087247 (0.100731)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.642 (0.658)\t Loss 0.096293 (0.100804)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.643 (0.655)\t Loss 0.124257 (0.100710)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.654 (0.654)\t Loss 0.098943 (0.100990)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.644 (0.653)\t Loss 0.098029 (0.101135)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.649 (0.652)\t Loss 0.130164 (0.102154)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.639 (0.652)\t Loss 0.110088 (0.102352)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.650 (0.651)\t Loss 0.111792 (0.102037)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.647 (0.651)\t Loss 0.104021 (0.102919)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.647 (0.650)\t Loss 0.107189 (0.103411)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.649 (0.650)\t Loss 0.125902 (0.104113)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.648 (0.650)\t Loss 0.106454 (0.103995)\n",
      "\n",
      "Epoch: [91]\t Train Loss: 0.102212  \t Val Loss: 0.103531\n",
      "\n",
      "False 0.10334676213562488\n",
      "Epoch: [92][0/600]\t Time 1.052 (1.052)\t Loss 0.085674 (0.085674)\n",
      "\n",
      "Epoch: [92][10/600]\t Time 0.688 (0.715)\t Loss 0.117901 (0.103060)\n",
      "\n",
      "Epoch: [92][20/600]\t Time 0.675 (0.697)\t Loss 0.062560 (0.100111)\n",
      "\n",
      "Epoch: [92][30/600]\t Time 0.667 (0.690)\t Loss 0.120661 (0.103605)\n",
      "\n",
      "Epoch: [92][40/600]\t Time 0.676 (0.687)\t Loss 0.099562 (0.103322)\n",
      "\n",
      "Epoch: [92][50/600]\t Time 0.676 (0.685)\t Loss 0.093260 (0.100769)\n",
      "\n",
      "Epoch: [92][60/600]\t Time 0.666 (0.683)\t Loss 0.110110 (0.100938)\n",
      "\n",
      "Epoch: [92][70/600]\t Time 0.679 (0.683)\t Loss 0.095591 (0.102044)\n",
      "\n",
      "Epoch: [92][80/600]\t Time 0.678 (0.682)\t Loss 0.124221 (0.101719)\n",
      "\n",
      "Epoch: [92][90/600]\t Time 0.685 (0.682)\t Loss 0.092256 (0.102050)\n",
      "\n",
      "Epoch: [92][100/600]\t Time 0.676 (0.681)\t Loss 0.095161 (0.102212)\n",
      "\n",
      "Epoch: [92][110/600]\t Time 0.677 (0.681)\t Loss 0.118787 (0.102029)\n",
      "\n",
      "Epoch: [92][120/600]\t Time 0.674 (0.680)\t Loss 0.077505 (0.102219)\n",
      "\n",
      "Epoch: [92][130/600]\t Time 0.678 (0.680)\t Loss 0.138721 (0.102434)\n",
      "\n",
      "Epoch: [92][140/600]\t Time 0.680 (0.680)\t Loss 0.090578 (0.101995)\n",
      "\n",
      "Epoch: [92][150/600]\t Time 0.671 (0.680)\t Loss 0.081212 (0.101958)\n",
      "\n",
      "Epoch: [92][160/600]\t Time 0.688 (0.679)\t Loss 0.094054 (0.101448)\n",
      "\n",
      "Epoch: [92][170/600]\t Time 0.674 (0.679)\t Loss 0.093721 (0.101328)\n",
      "\n",
      "Epoch: [92][180/600]\t Time 0.673 (0.679)\t Loss 0.067323 (0.101528)\n",
      "\n",
      "Epoch: [92][190/600]\t Time 0.682 (0.679)\t Loss 0.153177 (0.102182)\n",
      "\n",
      "Epoch: [92][200/600]\t Time 0.674 (0.679)\t Loss 0.110087 (0.102537)\n",
      "\n",
      "Epoch: [92][210/600]\t Time 0.672 (0.679)\t Loss 0.089744 (0.102416)\n",
      "\n",
      "Epoch: [92][220/600]\t Time 0.672 (0.678)\t Loss 0.103295 (0.102102)\n",
      "\n",
      "Epoch: [92][230/600]\t Time 0.684 (0.678)\t Loss 0.135506 (0.101830)\n",
      "\n",
      "Epoch: [92][240/600]\t Time 0.679 (0.678)\t Loss 0.069328 (0.101654)\n",
      "\n",
      "Epoch: [92][250/600]\t Time 0.674 (0.678)\t Loss 0.089534 (0.101563)\n",
      "\n",
      "Epoch: [92][260/600]\t Time 0.672 (0.678)\t Loss 0.091347 (0.101619)\n",
      "\n",
      "Epoch: [92][270/600]\t Time 0.673 (0.678)\t Loss 0.087570 (0.101437)\n",
      "\n",
      "Epoch: [92][280/600]\t Time 0.674 (0.677)\t Loss 0.070277 (0.101554)\n",
      "\n",
      "Epoch: [92][290/600]\t Time 0.676 (0.677)\t Loss 0.114406 (0.101392)\n",
      "\n",
      "Epoch: [92][300/600]\t Time 0.674 (0.677)\t Loss 0.081210 (0.101230)\n",
      "\n",
      "Epoch: [92][310/600]\t Time 0.681 (0.677)\t Loss 0.092519 (0.101379)\n",
      "\n",
      "Epoch: [92][320/600]\t Time 0.673 (0.677)\t Loss 0.077561 (0.101288)\n",
      "\n",
      "Epoch: [92][330/600]\t Time 0.682 (0.677)\t Loss 0.101450 (0.100999)\n",
      "\n",
      "Epoch: [92][340/600]\t Time 0.684 (0.677)\t Loss 0.076623 (0.101086)\n",
      "\n",
      "Epoch: [92][350/600]\t Time 0.673 (0.677)\t Loss 0.095950 (0.101218)\n",
      "\n",
      "Epoch: [92][360/600]\t Time 0.684 (0.677)\t Loss 0.086191 (0.101384)\n",
      "\n",
      "Epoch: [92][370/600]\t Time 0.674 (0.677)\t Loss 0.126738 (0.101233)\n",
      "\n",
      "Epoch: [92][380/600]\t Time 0.668 (0.677)\t Loss 0.139971 (0.101419)\n",
      "\n",
      "Epoch: [92][390/600]\t Time 0.683 (0.677)\t Loss 0.140765 (0.101731)\n",
      "\n",
      "Epoch: [92][400/600]\t Time 0.678 (0.677)\t Loss 0.089753 (0.101577)\n",
      "\n",
      "Epoch: [92][410/600]\t Time 0.668 (0.677)\t Loss 0.088757 (0.101568)\n",
      "\n",
      "Epoch: [92][420/600]\t Time 0.679 (0.677)\t Loss 0.115729 (0.101558)\n",
      "\n",
      "Epoch: [92][430/600]\t Time 0.679 (0.677)\t Loss 0.089613 (0.101929)\n",
      "\n",
      "Epoch: [92][440/600]\t Time 0.675 (0.677)\t Loss 0.105836 (0.101896)\n",
      "\n",
      "Epoch: [92][450/600]\t Time 0.679 (0.677)\t Loss 0.074870 (0.101819)\n",
      "\n",
      "Epoch: [92][460/600]\t Time 0.676 (0.677)\t Loss 0.097558 (0.101787)\n",
      "\n",
      "Epoch: [92][470/600]\t Time 0.679 (0.677)\t Loss 0.089910 (0.101892)\n",
      "\n",
      "Epoch: [92][480/600]\t Time 0.681 (0.677)\t Loss 0.117849 (0.102125)\n",
      "\n",
      "Epoch: [92][490/600]\t Time 0.675 (0.677)\t Loss 0.111492 (0.101955)\n",
      "\n",
      "Epoch: [92][500/600]\t Time 0.674 (0.677)\t Loss 0.093973 (0.101771)\n",
      "\n",
      "Epoch: [92][510/600]\t Time 0.673 (0.677)\t Loss 0.154509 (0.101811)\n",
      "\n",
      "Epoch: [92][520/600]\t Time 0.680 (0.677)\t Loss 0.101143 (0.101804)\n",
      "\n",
      "Epoch: [92][530/600]\t Time 0.671 (0.677)\t Loss 0.091640 (0.101787)\n",
      "\n",
      "Epoch: [92][540/600]\t Time 0.675 (0.677)\t Loss 0.105346 (0.101791)\n",
      "\n",
      "Epoch: [92][550/600]\t Time 0.682 (0.677)\t Loss 0.133122 (0.101840)\n",
      "\n",
      "Epoch: [92][560/600]\t Time 0.674 (0.677)\t Loss 0.122094 (0.101741)\n",
      "\n",
      "Epoch: [92][570/600]\t Time 0.677 (0.677)\t Loss 0.091760 (0.101696)\n",
      "\n",
      "Epoch: [92][580/600]\t Time 0.678 (0.677)\t Loss 0.086271 (0.101753)\n",
      "\n",
      "Epoch: [92][590/600]\t Time 0.676 (0.677)\t Loss 0.067038 (0.101855)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.014 (1.014)\t Loss 0.094940 (0.094940)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.642 (0.680)\t Loss 0.134777 (0.103104)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.652 (0.665)\t Loss 0.172105 (0.107908)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.641 (0.659)\t Loss 0.133417 (0.105621)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.651 (0.656)\t Loss 0.102987 (0.106595)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.654 (0.654)\t Loss 0.070282 (0.107000)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.647 (0.653)\t Loss 0.093242 (0.106839)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.651 (0.652)\t Loss 0.088579 (0.105631)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.651 (0.652)\t Loss 0.093295 (0.105423)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.643 (0.651)\t Loss 0.081670 (0.104951)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.652 (0.651)\t Loss 0.067861 (0.104486)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.640 (0.651)\t Loss 0.080498 (0.102937)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.644 (0.650)\t Loss 0.089543 (0.104056)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.650 (0.650)\t Loss 0.094356 (0.103766)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.648 (0.650)\t Loss 0.074036 (0.103737)\n",
      "\n",
      "Epoch: [92]\t Train Loss: 0.101794  \t Val Loss: 0.103644\n",
      "\n",
      "False 0.10334676213562488\n",
      "Epoch: [93][0/600]\t Time 1.056 (1.056)\t Loss 0.102611 (0.102611)\n",
      "\n",
      "Epoch: [93][10/600]\t Time 0.673 (0.713)\t Loss 0.127227 (0.094358)\n",
      "\n",
      "Epoch: [93][20/600]\t Time 0.689 (0.695)\t Loss 0.096730 (0.096047)\n",
      "\n",
      "Epoch: [93][30/600]\t Time 0.677 (0.690)\t Loss 0.106631 (0.097279)\n",
      "\n",
      "Epoch: [93][40/600]\t Time 0.674 (0.686)\t Loss 0.129452 (0.099096)\n",
      "\n",
      "Epoch: [93][50/600]\t Time 0.682 (0.685)\t Loss 0.122171 (0.102836)\n",
      "\n",
      "Epoch: [93][60/600]\t Time 0.677 (0.683)\t Loss 0.067393 (0.101963)\n",
      "\n",
      "Epoch: [93][70/600]\t Time 0.666 (0.682)\t Loss 0.111645 (0.103056)\n",
      "\n",
      "Epoch: [93][80/600]\t Time 0.679 (0.682)\t Loss 0.090137 (0.102280)\n",
      "\n",
      "Epoch: [93][90/600]\t Time 0.675 (0.681)\t Loss 0.095854 (0.102037)\n",
      "\n",
      "Epoch: [93][100/600]\t Time 0.683 (0.681)\t Loss 0.098644 (0.101888)\n",
      "\n",
      "Epoch: [93][110/600]\t Time 0.670 (0.681)\t Loss 0.099660 (0.101882)\n",
      "\n",
      "Epoch: [93][120/600]\t Time 0.680 (0.680)\t Loss 0.069990 (0.100712)\n",
      "\n",
      "Epoch: [93][130/600]\t Time 0.676 (0.680)\t Loss 0.092941 (0.100548)\n",
      "\n",
      "Epoch: [93][140/600]\t Time 0.684 (0.680)\t Loss 0.086524 (0.100736)\n",
      "\n",
      "Epoch: [93][150/600]\t Time 0.676 (0.680)\t Loss 0.083435 (0.101042)\n",
      "\n",
      "Epoch: [93][160/600]\t Time 0.673 (0.680)\t Loss 0.106898 (0.101336)\n",
      "\n",
      "Epoch: [93][170/600]\t Time 0.681 (0.680)\t Loss 0.149270 (0.101356)\n",
      "\n",
      "Epoch: [93][180/600]\t Time 0.680 (0.679)\t Loss 0.108580 (0.101033)\n",
      "\n",
      "Epoch: [93][190/600]\t Time 0.666 (0.679)\t Loss 0.116776 (0.100739)\n",
      "\n",
      "Epoch: [93][200/600]\t Time 0.678 (0.679)\t Loss 0.075898 (0.100196)\n",
      "\n",
      "Epoch: [93][210/600]\t Time 0.671 (0.679)\t Loss 0.134802 (0.100356)\n",
      "\n",
      "Epoch: [93][220/600]\t Time 0.675 (0.679)\t Loss 0.097672 (0.100203)\n",
      "\n",
      "Epoch: [93][230/600]\t Time 0.676 (0.679)\t Loss 0.105961 (0.100637)\n",
      "\n",
      "Epoch: [93][240/600]\t Time 0.675 (0.679)\t Loss 0.097233 (0.100080)\n",
      "\n",
      "Epoch: [93][250/600]\t Time 0.680 (0.679)\t Loss 0.074837 (0.100278)\n",
      "\n",
      "Epoch: [93][260/600]\t Time 0.675 (0.679)\t Loss 0.087276 (0.100305)\n",
      "\n",
      "Epoch: [93][270/600]\t Time 0.679 (0.678)\t Loss 0.107836 (0.100457)\n",
      "\n",
      "Epoch: [93][280/600]\t Time 0.674 (0.678)\t Loss 0.139809 (0.100629)\n",
      "\n",
      "Epoch: [93][290/600]\t Time 0.669 (0.678)\t Loss 0.094379 (0.100654)\n",
      "\n",
      "Epoch: [93][300/600]\t Time 0.675 (0.678)\t Loss 0.114932 (0.100582)\n",
      "\n",
      "Epoch: [93][310/600]\t Time 0.671 (0.678)\t Loss 0.147434 (0.101067)\n",
      "\n",
      "Epoch: [93][320/600]\t Time 0.678 (0.678)\t Loss 0.076812 (0.101048)\n",
      "\n",
      "Epoch: [93][330/600]\t Time 0.675 (0.678)\t Loss 0.139672 (0.101060)\n",
      "\n",
      "Epoch: [93][340/600]\t Time 0.676 (0.678)\t Loss 0.097040 (0.101086)\n",
      "\n",
      "Epoch: [93][350/600]\t Time 0.682 (0.678)\t Loss 0.100709 (0.101159)\n",
      "\n",
      "Epoch: [93][360/600]\t Time 0.675 (0.678)\t Loss 0.106274 (0.101108)\n",
      "\n",
      "Epoch: [93][370/600]\t Time 0.679 (0.678)\t Loss 0.094586 (0.101091)\n",
      "\n",
      "Epoch: [93][380/600]\t Time 0.673 (0.678)\t Loss 0.112603 (0.101286)\n",
      "\n",
      "Epoch: [93][390/600]\t Time 0.675 (0.678)\t Loss 0.128560 (0.101553)\n",
      "\n",
      "Epoch: [93][400/600]\t Time 0.679 (0.677)\t Loss 0.105620 (0.101445)\n",
      "\n",
      "Epoch: [93][410/600]\t Time 0.670 (0.677)\t Loss 0.103147 (0.101216)\n",
      "\n",
      "Epoch: [93][420/600]\t Time 0.686 (0.677)\t Loss 0.073188 (0.101249)\n",
      "\n",
      "Epoch: [93][430/600]\t Time 0.680 (0.677)\t Loss 0.103696 (0.101131)\n",
      "\n",
      "Epoch: [93][440/600]\t Time 0.675 (0.677)\t Loss 0.100570 (0.101165)\n",
      "\n",
      "Epoch: [93][450/600]\t Time 0.683 (0.677)\t Loss 0.097416 (0.101287)\n",
      "\n",
      "Epoch: [93][460/600]\t Time 0.677 (0.677)\t Loss 0.076789 (0.101327)\n",
      "\n",
      "Epoch: [93][470/600]\t Time 0.672 (0.677)\t Loss 0.108316 (0.101339)\n",
      "\n",
      "Epoch: [93][480/600]\t Time 0.677 (0.677)\t Loss 0.110976 (0.101241)\n",
      "\n",
      "Epoch: [93][490/600]\t Time 0.677 (0.677)\t Loss 0.159835 (0.101254)\n",
      "\n",
      "Epoch: [93][500/600]\t Time 0.667 (0.677)\t Loss 0.084436 (0.101329)\n",
      "\n",
      "Epoch: [93][510/600]\t Time 0.679 (0.677)\t Loss 0.128167 (0.101594)\n",
      "\n",
      "Epoch: [93][520/600]\t Time 0.674 (0.677)\t Loss 0.090694 (0.101709)\n",
      "\n",
      "Epoch: [93][530/600]\t Time 0.673 (0.677)\t Loss 0.108698 (0.101915)\n",
      "\n",
      "Epoch: [93][540/600]\t Time 0.677 (0.677)\t Loss 0.105957 (0.102030)\n",
      "\n",
      "Epoch: [93][550/600]\t Time 0.679 (0.677)\t Loss 0.117583 (0.101993)\n",
      "\n",
      "Epoch: [93][560/600]\t Time 0.679 (0.677)\t Loss 0.092199 (0.101995)\n",
      "\n",
      "Epoch: [93][570/600]\t Time 0.678 (0.677)\t Loss 0.103016 (0.102059)\n",
      "\n",
      "Epoch: [93][580/600]\t Time 0.673 (0.677)\t Loss 0.097461 (0.102284)\n",
      "\n",
      "Epoch: [93][590/600]\t Time 0.676 (0.677)\t Loss 0.117522 (0.102257)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.006 (1.006)\t Loss 0.113577 (0.113577)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.644 (0.684)\t Loss 0.128636 (0.105377)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.650 (0.666)\t Loss 0.080419 (0.101913)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.649 (0.659)\t Loss 0.104513 (0.103130)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.652 (0.656)\t Loss 0.100317 (0.104860)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.640 (0.654)\t Loss 0.128018 (0.104933)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.650 (0.652)\t Loss 0.120809 (0.106293)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.640 (0.652)\t Loss 0.089235 (0.103039)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.643 (0.651)\t Loss 0.103622 (0.104234)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.644 (0.651)\t Loss 0.103892 (0.104604)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.645 (0.650)\t Loss 0.119314 (0.104812)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.651 (0.650)\t Loss 0.091155 (0.104260)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.651 (0.650)\t Loss 0.099157 (0.104451)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.645 (0.650)\t Loss 0.132089 (0.104323)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.645 (0.649)\t Loss 0.077729 (0.103829)\n",
      "\n",
      "Epoch: [93]\t Train Loss: 0.102051  \t Val Loss: 0.103925\n",
      "\n",
      "False 0.10334676213562488\n",
      "Epoch: [94][0/600]\t Time 1.068 (1.068)\t Loss 0.088672 (0.088672)\n",
      "\n",
      "Epoch: [94][10/600]\t Time 0.672 (0.718)\t Loss 0.112824 (0.095414)\n",
      "\n",
      "Epoch: [94][20/600]\t Time 0.679 (0.698)\t Loss 0.107083 (0.099959)\n",
      "\n",
      "Epoch: [94][30/600]\t Time 0.678 (0.691)\t Loss 0.103170 (0.098510)\n",
      "\n",
      "Epoch: [94][40/600]\t Time 0.681 (0.688)\t Loss 0.106652 (0.100154)\n",
      "\n",
      "Epoch: [94][50/600]\t Time 0.685 (0.685)\t Loss 0.078272 (0.101829)\n",
      "\n",
      "Epoch: [94][60/600]\t Time 0.679 (0.684)\t Loss 0.093806 (0.101275)\n",
      "\n",
      "Epoch: [94][70/600]\t Time 0.673 (0.683)\t Loss 0.111470 (0.101918)\n",
      "\n",
      "Epoch: [94][80/600]\t Time 0.677 (0.682)\t Loss 0.120173 (0.103339)\n",
      "\n",
      "Epoch: [94][90/600]\t Time 0.675 (0.682)\t Loss 0.119777 (0.103407)\n",
      "\n",
      "Epoch: [94][100/600]\t Time 0.666 (0.681)\t Loss 0.082844 (0.103106)\n",
      "\n",
      "Epoch: [94][110/600]\t Time 0.679 (0.681)\t Loss 0.143458 (0.104390)\n",
      "\n",
      "Epoch: [94][120/600]\t Time 0.673 (0.681)\t Loss 0.141699 (0.104923)\n",
      "\n",
      "Epoch: [94][130/600]\t Time 0.679 (0.680)\t Loss 0.072715 (0.104563)\n",
      "\n",
      "Epoch: [94][140/600]\t Time 0.677 (0.680)\t Loss 0.093654 (0.104713)\n",
      "\n",
      "Epoch: [94][150/600]\t Time 0.679 (0.680)\t Loss 0.115802 (0.104250)\n",
      "\n",
      "Epoch: [94][160/600]\t Time 0.671 (0.680)\t Loss 0.090361 (0.103741)\n",
      "\n",
      "Epoch: [94][170/600]\t Time 0.675 (0.679)\t Loss 0.107290 (0.103387)\n",
      "\n",
      "Epoch: [94][180/600]\t Time 0.680 (0.679)\t Loss 0.120119 (0.103144)\n",
      "\n",
      "Epoch: [94][190/600]\t Time 0.676 (0.679)\t Loss 0.121484 (0.103295)\n",
      "\n",
      "Epoch: [94][200/600]\t Time 0.681 (0.679)\t Loss 0.094682 (0.103176)\n",
      "\n",
      "Epoch: [94][210/600]\t Time 0.676 (0.679)\t Loss 0.120216 (0.103249)\n",
      "\n",
      "Epoch: [94][220/600]\t Time 0.674 (0.679)\t Loss 0.101052 (0.102795)\n",
      "\n",
      "Epoch: [94][230/600]\t Time 0.688 (0.679)\t Loss 0.093257 (0.102706)\n",
      "\n",
      "Epoch: [94][240/600]\t Time 0.678 (0.679)\t Loss 0.136773 (0.102708)\n",
      "\n",
      "Epoch: [94][250/600]\t Time 0.679 (0.678)\t Loss 0.104542 (0.102914)\n",
      "\n",
      "Epoch: [94][260/600]\t Time 0.677 (0.678)\t Loss 0.142036 (0.103393)\n",
      "\n",
      "Epoch: [94][270/600]\t Time 0.678 (0.678)\t Loss 0.074847 (0.103507)\n",
      "\n",
      "Epoch: [94][280/600]\t Time 0.665 (0.678)\t Loss 0.086309 (0.103397)\n",
      "\n",
      "Epoch: [94][290/600]\t Time 0.677 (0.678)\t Loss 0.101903 (0.103059)\n",
      "\n",
      "Epoch: [94][300/600]\t Time 0.673 (0.678)\t Loss 0.087679 (0.103304)\n",
      "\n",
      "Epoch: [94][310/600]\t Time 0.669 (0.678)\t Loss 0.113578 (0.103285)\n",
      "\n",
      "Epoch: [94][320/600]\t Time 0.679 (0.678)\t Loss 0.076455 (0.103176)\n",
      "\n",
      "Epoch: [94][330/600]\t Time 0.675 (0.678)\t Loss 0.115718 (0.102947)\n",
      "\n",
      "Epoch: [94][340/600]\t Time 0.672 (0.678)\t Loss 0.072918 (0.102995)\n",
      "\n",
      "Epoch: [94][350/600]\t Time 0.664 (0.678)\t Loss 0.106990 (0.102970)\n",
      "\n",
      "Epoch: [94][360/600]\t Time 0.672 (0.678)\t Loss 0.108182 (0.102797)\n",
      "\n",
      "Epoch: [94][370/600]\t Time 0.674 (0.677)\t Loss 0.111366 (0.102721)\n",
      "\n",
      "Epoch: [94][380/600]\t Time 0.669 (0.677)\t Loss 0.124359 (0.102721)\n",
      "\n",
      "Epoch: [94][390/600]\t Time 0.679 (0.677)\t Loss 0.119343 (0.102812)\n",
      "\n",
      "Epoch: [94][400/600]\t Time 0.682 (0.677)\t Loss 0.089119 (0.102971)\n",
      "\n",
      "Epoch: [94][410/600]\t Time 0.672 (0.677)\t Loss 0.110021 (0.103094)\n",
      "\n",
      "Epoch: [94][420/600]\t Time 0.676 (0.677)\t Loss 0.102900 (0.103149)\n",
      "\n",
      "Epoch: [94][430/600]\t Time 0.684 (0.677)\t Loss 0.072554 (0.102995)\n",
      "\n",
      "Epoch: [94][440/600]\t Time 0.678 (0.677)\t Loss 0.096916 (0.103006)\n",
      "\n",
      "Epoch: [94][450/600]\t Time 0.675 (0.677)\t Loss 0.121101 (0.102797)\n",
      "\n",
      "Epoch: [94][460/600]\t Time 0.681 (0.677)\t Loss 0.105797 (0.102582)\n",
      "\n",
      "Epoch: [94][470/600]\t Time 0.674 (0.677)\t Loss 0.149371 (0.102665)\n",
      "\n",
      "Epoch: [94][480/600]\t Time 0.665 (0.677)\t Loss 0.081033 (0.102667)\n",
      "\n",
      "Epoch: [94][490/600]\t Time 0.678 (0.677)\t Loss 0.110299 (0.102690)\n",
      "\n",
      "Epoch: [94][500/600]\t Time 0.677 (0.677)\t Loss 0.135794 (0.102856)\n",
      "\n",
      "Epoch: [94][510/600]\t Time 0.672 (0.677)\t Loss 0.128247 (0.102896)\n",
      "\n",
      "Epoch: [94][520/600]\t Time 0.678 (0.677)\t Loss 0.090907 (0.102804)\n",
      "\n",
      "Epoch: [94][530/600]\t Time 0.677 (0.677)\t Loss 0.116254 (0.102814)\n",
      "\n",
      "Epoch: [94][540/600]\t Time 0.675 (0.677)\t Loss 0.143618 (0.102946)\n",
      "\n",
      "Epoch: [94][550/600]\t Time 0.679 (0.677)\t Loss 0.076619 (0.102913)\n",
      "\n",
      "Epoch: [94][560/600]\t Time 0.675 (0.677)\t Loss 0.091147 (0.102718)\n",
      "\n",
      "Epoch: [94][570/600]\t Time 0.676 (0.677)\t Loss 0.108120 (0.102549)\n",
      "\n",
      "Epoch: [94][580/600]\t Time 0.676 (0.677)\t Loss 0.101758 (0.102555)\n",
      "\n",
      "Epoch: [94][590/600]\t Time 0.680 (0.677)\t Loss 0.087028 (0.102618)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.022 (1.022)\t Loss 0.080368 (0.080368)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.652 (0.682)\t Loss 0.087345 (0.097587)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.652 (0.665)\t Loss 0.089804 (0.097971)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.650 (0.659)\t Loss 0.106671 (0.099954)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.646 (0.656)\t Loss 0.072880 (0.102276)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.646 (0.655)\t Loss 0.103817 (0.104709)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.641 (0.653)\t Loss 0.125539 (0.105801)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.642 (0.652)\t Loss 0.080730 (0.104250)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.651 (0.651)\t Loss 0.130638 (0.105048)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.655 (0.651)\t Loss 0.096855 (0.104376)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.644 (0.650)\t Loss 0.088294 (0.104185)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.646 (0.650)\t Loss 0.115171 (0.104419)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.644 (0.650)\t Loss 0.073085 (0.104701)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.645 (0.650)\t Loss 0.109350 (0.105111)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.641 (0.649)\t Loss 0.094504 (0.104781)\n",
      "\n",
      "Epoch: [94]\t Train Loss: 0.102532  \t Val Loss: 0.103282\n",
      "\n",
      "True 0.10328206526736418\n",
      "Epoch: [95][0/600]\t Time 1.033 (1.033)\t Loss 0.129833 (0.129833)\n",
      "\n",
      "Epoch: [95][10/600]\t Time 0.673 (0.708)\t Loss 0.115563 (0.100667)\n",
      "\n",
      "Epoch: [95][20/600]\t Time 0.689 (0.693)\t Loss 0.086588 (0.099603)\n",
      "\n",
      "Epoch: [95][30/600]\t Time 0.676 (0.688)\t Loss 0.106148 (0.100047)\n",
      "\n",
      "Epoch: [95][40/600]\t Time 0.668 (0.685)\t Loss 0.115258 (0.099913)\n",
      "\n",
      "Epoch: [95][50/600]\t Time 0.679 (0.683)\t Loss 0.097691 (0.099942)\n",
      "\n",
      "Epoch: [95][60/600]\t Time 0.674 (0.682)\t Loss 0.086318 (0.099002)\n",
      "\n",
      "Epoch: [95][70/600]\t Time 0.669 (0.681)\t Loss 0.089979 (0.099640)\n",
      "\n",
      "Epoch: [95][80/600]\t Time 0.673 (0.681)\t Loss 0.106337 (0.101363)\n",
      "\n",
      "Epoch: [95][90/600]\t Time 0.676 (0.680)\t Loss 0.112955 (0.101610)\n",
      "\n",
      "Epoch: [95][100/600]\t Time 0.674 (0.680)\t Loss 0.102346 (0.102428)\n",
      "\n",
      "Epoch: [95][110/600]\t Time 0.680 (0.680)\t Loss 0.093526 (0.102047)\n",
      "\n",
      "Epoch: [95][120/600]\t Time 0.676 (0.680)\t Loss 0.063298 (0.101618)\n",
      "\n",
      "Epoch: [95][130/600]\t Time 0.674 (0.680)\t Loss 0.084946 (0.101137)\n",
      "\n",
      "Epoch: [95][140/600]\t Time 0.678 (0.679)\t Loss 0.101407 (0.102401)\n",
      "\n",
      "Epoch: [95][150/600]\t Time 0.682 (0.679)\t Loss 0.093393 (0.102789)\n",
      "\n",
      "Epoch: [95][160/600]\t Time 0.674 (0.679)\t Loss 0.112832 (0.102732)\n",
      "\n",
      "Epoch: [95][170/600]\t Time 0.682 (0.679)\t Loss 0.122252 (0.102274)\n",
      "\n",
      "Epoch: [95][180/600]\t Time 0.675 (0.679)\t Loss 0.080382 (0.102302)\n",
      "\n",
      "Epoch: [95][190/600]\t Time 0.667 (0.679)\t Loss 0.108684 (0.102220)\n",
      "\n",
      "Epoch: [95][200/600]\t Time 0.678 (0.679)\t Loss 0.096028 (0.101887)\n",
      "\n",
      "Epoch: [95][210/600]\t Time 0.677 (0.679)\t Loss 0.116010 (0.101785)\n",
      "\n",
      "Epoch: [95][220/600]\t Time 0.672 (0.678)\t Loss 0.084698 (0.101328)\n",
      "\n",
      "Epoch: [95][230/600]\t Time 0.678 (0.678)\t Loss 0.123999 (0.101163)\n",
      "\n",
      "Epoch: [95][240/600]\t Time 0.678 (0.678)\t Loss 0.085068 (0.101033)\n",
      "\n",
      "Epoch: [95][250/600]\t Time 0.675 (0.678)\t Loss 0.112026 (0.101014)\n",
      "\n",
      "Epoch: [95][260/600]\t Time 0.677 (0.678)\t Loss 0.094117 (0.101301)\n",
      "\n",
      "Epoch: [95][270/600]\t Time 0.675 (0.678)\t Loss 0.134102 (0.101716)\n",
      "\n",
      "Epoch: [95][280/600]\t Time 0.678 (0.678)\t Loss 0.116418 (0.101922)\n",
      "\n",
      "Epoch: [95][290/600]\t Time 0.674 (0.678)\t Loss 0.086168 (0.101847)\n",
      "\n",
      "Epoch: [95][300/600]\t Time 0.674 (0.678)\t Loss 0.094538 (0.102185)\n",
      "\n",
      "Epoch: [95][310/600]\t Time 0.679 (0.678)\t Loss 0.121614 (0.102310)\n",
      "\n",
      "Epoch: [95][320/600]\t Time 0.676 (0.678)\t Loss 0.088263 (0.102359)\n",
      "\n",
      "Epoch: [95][330/600]\t Time 0.680 (0.678)\t Loss 0.121854 (0.102214)\n",
      "\n",
      "Epoch: [95][340/600]\t Time 0.673 (0.678)\t Loss 0.091313 (0.102052)\n",
      "\n",
      "Epoch: [95][350/600]\t Time 0.682 (0.678)\t Loss 0.071239 (0.102183)\n",
      "\n",
      "Epoch: [95][360/600]\t Time 0.679 (0.678)\t Loss 0.078462 (0.102269)\n",
      "\n",
      "Epoch: [95][370/600]\t Time 0.674 (0.678)\t Loss 0.089939 (0.102054)\n",
      "\n",
      "Epoch: [95][380/600]\t Time 0.671 (0.678)\t Loss 0.060290 (0.101882)\n",
      "\n",
      "Epoch: [95][390/600]\t Time 0.680 (0.678)\t Loss 0.092346 (0.101858)\n",
      "\n",
      "Epoch: [95][400/600]\t Time 0.679 (0.678)\t Loss 0.091348 (0.101887)\n",
      "\n",
      "Epoch: [95][410/600]\t Time 0.668 (0.678)\t Loss 0.092701 (0.101892)\n",
      "\n",
      "Epoch: [95][420/600]\t Time 0.677 (0.678)\t Loss 0.108117 (0.101796)\n",
      "\n",
      "Epoch: [95][430/600]\t Time 0.672 (0.677)\t Loss 0.132519 (0.102142)\n",
      "\n",
      "Epoch: [95][440/600]\t Time 0.670 (0.677)\t Loss 0.115333 (0.102024)\n",
      "\n",
      "Epoch: [95][450/600]\t Time 0.678 (0.677)\t Loss 0.106023 (0.102128)\n",
      "\n",
      "Epoch: [95][460/600]\t Time 0.674 (0.677)\t Loss 0.124166 (0.102209)\n",
      "\n",
      "Epoch: [95][470/600]\t Time 0.672 (0.677)\t Loss 0.072004 (0.102142)\n",
      "\n",
      "Epoch: [95][480/600]\t Time 0.679 (0.677)\t Loss 0.109597 (0.102120)\n",
      "\n",
      "Epoch: [95][490/600]\t Time 0.686 (0.677)\t Loss 0.118221 (0.102117)\n",
      "\n",
      "Epoch: [95][500/600]\t Time 0.685 (0.677)\t Loss 0.096901 (0.102195)\n",
      "\n",
      "Epoch: [95][510/600]\t Time 0.675 (0.677)\t Loss 0.094519 (0.101978)\n",
      "\n",
      "Epoch: [95][520/600]\t Time 0.680 (0.677)\t Loss 0.080638 (0.102023)\n",
      "\n",
      "Epoch: [95][530/600]\t Time 0.673 (0.677)\t Loss 0.118332 (0.102128)\n",
      "\n",
      "Epoch: [95][540/600]\t Time 0.678 (0.677)\t Loss 0.109096 (0.102046)\n",
      "\n",
      "Epoch: [95][550/600]\t Time 0.678 (0.677)\t Loss 0.134610 (0.102008)\n",
      "\n",
      "Epoch: [95][560/600]\t Time 0.679 (0.677)\t Loss 0.103868 (0.102018)\n",
      "\n",
      "Epoch: [95][570/600]\t Time 0.681 (0.677)\t Loss 0.117475 (0.102062)\n",
      "\n",
      "Epoch: [95][580/600]\t Time 0.684 (0.677)\t Loss 0.072288 (0.102087)\n",
      "\n",
      "Epoch: [95][590/600]\t Time 0.672 (0.677)\t Loss 0.068691 (0.102174)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.023 (1.023)\t Loss 0.119606 (0.119606)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.653 (0.688)\t Loss 0.163711 (0.109552)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.644 (0.667)\t Loss 0.102032 (0.110004)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.649 (0.660)\t Loss 0.120702 (0.108431)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.640 (0.657)\t Loss 0.094873 (0.108347)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.642 (0.655)\t Loss 0.102983 (0.107293)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.655 (0.654)\t Loss 0.084685 (0.106366)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.647 (0.653)\t Loss 0.109823 (0.106667)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.649 (0.652)\t Loss 0.096084 (0.105717)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.641 (0.651)\t Loss 0.134934 (0.106039)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.651 (0.651)\t Loss 0.159731 (0.106080)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.647 (0.650)\t Loss 0.118569 (0.105330)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.648 (0.650)\t Loss 0.121006 (0.105104)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.650 (0.650)\t Loss 0.148728 (0.104564)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.644 (0.649)\t Loss 0.086703 (0.103800)\n",
      "\n",
      "Epoch: [95]\t Train Loss: 0.102281  \t Val Loss: 0.104122\n",
      "\n",
      "False 0.10328206526736418\n",
      "Epoch: [96][0/600]\t Time 1.069 (1.069)\t Loss 0.092839 (0.092839)\n",
      "\n",
      "Epoch: [96][10/600]\t Time 0.672 (0.711)\t Loss 0.114463 (0.106356)\n",
      "\n",
      "Epoch: [96][20/600]\t Time 0.672 (0.694)\t Loss 0.092083 (0.106413)\n",
      "\n",
      "Epoch: [96][30/600]\t Time 0.674 (0.687)\t Loss 0.105488 (0.102110)\n",
      "\n",
      "Epoch: [96][40/600]\t Time 0.678 (0.685)\t Loss 0.087054 (0.101319)\n",
      "\n",
      "Epoch: [96][50/600]\t Time 0.675 (0.683)\t Loss 0.067235 (0.098883)\n",
      "\n",
      "Epoch: [96][60/600]\t Time 0.676 (0.682)\t Loss 0.108501 (0.100811)\n",
      "\n",
      "Epoch: [96][70/600]\t Time 0.677 (0.681)\t Loss 0.103193 (0.102233)\n",
      "\n",
      "Epoch: [96][80/600]\t Time 0.674 (0.680)\t Loss 0.093852 (0.102127)\n",
      "\n",
      "Epoch: [96][90/600]\t Time 0.680 (0.680)\t Loss 0.119924 (0.102002)\n",
      "\n",
      "Epoch: [96][100/600]\t Time 0.676 (0.680)\t Loss 0.071577 (0.101137)\n",
      "\n",
      "Epoch: [96][110/600]\t Time 0.688 (0.679)\t Loss 0.110134 (0.100777)\n",
      "\n",
      "Epoch: [96][120/600]\t Time 0.678 (0.679)\t Loss 0.124564 (0.100900)\n",
      "\n",
      "Epoch: [96][130/600]\t Time 0.673 (0.679)\t Loss 0.098864 (0.101396)\n",
      "\n",
      "Epoch: [96][140/600]\t Time 0.665 (0.679)\t Loss 0.078407 (0.101938)\n",
      "\n",
      "Epoch: [96][150/600]\t Time 0.677 (0.679)\t Loss 0.094165 (0.101438)\n",
      "\n",
      "Epoch: [96][160/600]\t Time 0.675 (0.679)\t Loss 0.098432 (0.102114)\n",
      "\n",
      "Epoch: [96][170/600]\t Time 0.673 (0.678)\t Loss 0.076504 (0.102332)\n",
      "\n",
      "Epoch: [96][180/600]\t Time 0.677 (0.678)\t Loss 0.075867 (0.103141)\n",
      "\n",
      "Epoch: [96][190/600]\t Time 0.673 (0.678)\t Loss 0.059962 (0.102192)\n",
      "\n",
      "Epoch: [96][200/600]\t Time 0.680 (0.678)\t Loss 0.118784 (0.102169)\n",
      "\n",
      "Epoch: [96][210/600]\t Time 0.673 (0.678)\t Loss 0.094928 (0.102261)\n",
      "\n",
      "Epoch: [96][220/600]\t Time 0.684 (0.678)\t Loss 0.104329 (0.102089)\n",
      "\n",
      "Epoch: [96][230/600]\t Time 0.675 (0.678)\t Loss 0.090131 (0.102227)\n",
      "\n",
      "Epoch: [96][240/600]\t Time 0.684 (0.678)\t Loss 0.122434 (0.102256)\n",
      "\n",
      "Epoch: [96][250/600]\t Time 0.675 (0.678)\t Loss 0.101325 (0.101965)\n",
      "\n",
      "Epoch: [96][260/600]\t Time 0.682 (0.678)\t Loss 0.105766 (0.102396)\n",
      "\n",
      "Epoch: [96][270/600]\t Time 0.683 (0.678)\t Loss 0.068337 (0.102520)\n",
      "\n",
      "Epoch: [96][280/600]\t Time 0.673 (0.678)\t Loss 0.124623 (0.102961)\n",
      "\n",
      "Epoch: [96][290/600]\t Time 0.677 (0.678)\t Loss 0.101492 (0.103368)\n",
      "\n",
      "Epoch: [96][300/600]\t Time 0.678 (0.678)\t Loss 0.096564 (0.103408)\n",
      "\n",
      "Epoch: [96][310/600]\t Time 0.679 (0.678)\t Loss 0.077345 (0.103509)\n",
      "\n",
      "Epoch: [96][320/600]\t Time 0.680 (0.678)\t Loss 0.122970 (0.103308)\n",
      "\n",
      "Epoch: [96][330/600]\t Time 0.674 (0.678)\t Loss 0.093823 (0.103199)\n",
      "\n",
      "Epoch: [96][340/600]\t Time 0.682 (0.678)\t Loss 0.086132 (0.103434)\n",
      "\n",
      "Epoch: [96][350/600]\t Time 0.675 (0.678)\t Loss 0.088359 (0.103207)\n",
      "\n",
      "Epoch: [96][360/600]\t Time 0.679 (0.678)\t Loss 0.091105 (0.102920)\n",
      "\n",
      "Epoch: [96][370/600]\t Time 0.676 (0.678)\t Loss 0.150017 (0.102815)\n",
      "\n",
      "Epoch: [96][380/600]\t Time 0.679 (0.678)\t Loss 0.069596 (0.102456)\n",
      "\n",
      "Epoch: [96][390/600]\t Time 0.678 (0.678)\t Loss 0.110570 (0.102325)\n",
      "\n",
      "Epoch: [96][400/600]\t Time 0.671 (0.678)\t Loss 0.075151 (0.102233)\n",
      "\n",
      "Epoch: [96][410/600]\t Time 0.680 (0.678)\t Loss 0.093600 (0.102181)\n",
      "\n",
      "Epoch: [96][420/600]\t Time 0.675 (0.678)\t Loss 0.085957 (0.101921)\n",
      "\n",
      "Epoch: [96][430/600]\t Time 0.677 (0.678)\t Loss 0.099205 (0.101897)\n",
      "\n",
      "Epoch: [96][440/600]\t Time 0.677 (0.678)\t Loss 0.095944 (0.101760)\n",
      "\n",
      "Epoch: [96][450/600]\t Time 0.679 (0.678)\t Loss 0.095987 (0.101758)\n",
      "\n",
      "Epoch: [96][460/600]\t Time 0.677 (0.678)\t Loss 0.085072 (0.101570)\n",
      "\n",
      "Epoch: [96][470/600]\t Time 0.671 (0.678)\t Loss 0.153470 (0.101852)\n",
      "\n",
      "Epoch: [96][480/600]\t Time 0.671 (0.678)\t Loss 0.074405 (0.101939)\n",
      "\n",
      "Epoch: [96][490/600]\t Time 0.675 (0.678)\t Loss 0.079703 (0.102086)\n",
      "\n",
      "Epoch: [96][500/600]\t Time 0.679 (0.678)\t Loss 0.102411 (0.101978)\n",
      "\n",
      "Epoch: [96][510/600]\t Time 0.675 (0.677)\t Loss 0.110773 (0.101919)\n",
      "\n",
      "Epoch: [96][520/600]\t Time 0.677 (0.678)\t Loss 0.086494 (0.101845)\n",
      "\n",
      "Epoch: [96][530/600]\t Time 0.675 (0.677)\t Loss 0.116635 (0.102006)\n",
      "\n",
      "Epoch: [96][540/600]\t Time 0.680 (0.677)\t Loss 0.112153 (0.102104)\n",
      "\n",
      "Epoch: [96][550/600]\t Time 0.671 (0.677)\t Loss 0.067368 (0.102187)\n",
      "\n",
      "Epoch: [96][560/600]\t Time 0.690 (0.677)\t Loss 0.125594 (0.102261)\n",
      "\n",
      "Epoch: [96][570/600]\t Time 0.676 (0.677)\t Loss 0.084441 (0.102126)\n",
      "\n",
      "Epoch: [96][580/600]\t Time 0.667 (0.677)\t Loss 0.106446 (0.102020)\n",
      "\n",
      "Epoch: [96][590/600]\t Time 0.679 (0.677)\t Loss 0.112469 (0.102076)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.035 (1.035)\t Loss 0.092251 (0.092251)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.657 (0.689)\t Loss 0.085777 (0.102992)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.643 (0.670)\t Loss 0.095204 (0.101215)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.644 (0.662)\t Loss 0.082220 (0.099343)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.652 (0.658)\t Loss 0.101698 (0.101875)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.643 (0.656)\t Loss 0.080570 (0.103103)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.642 (0.655)\t Loss 0.079787 (0.102681)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.644 (0.653)\t Loss 0.118127 (0.103789)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.652 (0.653)\t Loss 0.100262 (0.104626)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.647 (0.652)\t Loss 0.097529 (0.104569)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.646 (0.652)\t Loss 0.100571 (0.103559)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.650 (0.651)\t Loss 0.085779 (0.102999)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.647 (0.651)\t Loss 0.089856 (0.102584)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.644 (0.651)\t Loss 0.125337 (0.102682)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.638 (0.650)\t Loss 0.104848 (0.102915)\n",
      "\n",
      "Epoch: [96]\t Train Loss: 0.102161  \t Val Loss: 0.103128\n",
      "\n",
      "True 0.1031284017364184\n",
      "Epoch: [97][0/600]\t Time 1.056 (1.056)\t Loss 0.092307 (0.092307)\n",
      "\n",
      "Epoch: [97][10/600]\t Time 0.665 (0.710)\t Loss 0.070625 (0.098261)\n",
      "\n",
      "Epoch: [97][20/600]\t Time 0.677 (0.695)\t Loss 0.070626 (0.099675)\n",
      "\n",
      "Epoch: [97][30/600]\t Time 0.675 (0.689)\t Loss 0.095000 (0.097758)\n",
      "\n",
      "Epoch: [97][40/600]\t Time 0.669 (0.686)\t Loss 0.160391 (0.103175)\n",
      "\n",
      "Epoch: [97][50/600]\t Time 0.676 (0.684)\t Loss 0.117559 (0.103544)\n",
      "\n",
      "Epoch: [97][60/600]\t Time 0.673 (0.682)\t Loss 0.104305 (0.102849)\n",
      "\n",
      "Epoch: [97][70/600]\t Time 0.678 (0.682)\t Loss 0.134770 (0.103356)\n",
      "\n",
      "Epoch: [97][80/600]\t Time 0.675 (0.681)\t Loss 0.097909 (0.102590)\n",
      "\n",
      "Epoch: [97][90/600]\t Time 0.674 (0.680)\t Loss 0.166372 (0.102941)\n",
      "\n",
      "Epoch: [97][100/600]\t Time 0.675 (0.680)\t Loss 0.113241 (0.104215)\n",
      "\n",
      "Epoch: [97][110/600]\t Time 0.676 (0.680)\t Loss 0.095121 (0.104525)\n",
      "\n",
      "Epoch: [97][120/600]\t Time 0.680 (0.680)\t Loss 0.080890 (0.103977)\n",
      "\n",
      "Epoch: [97][130/600]\t Time 0.677 (0.679)\t Loss 0.086696 (0.104223)\n",
      "\n",
      "Epoch: [97][140/600]\t Time 0.682 (0.679)\t Loss 0.080845 (0.103953)\n",
      "\n",
      "Epoch: [97][150/600]\t Time 0.680 (0.679)\t Loss 0.124987 (0.104046)\n",
      "\n",
      "Epoch: [97][160/600]\t Time 0.672 (0.679)\t Loss 0.100775 (0.103881)\n",
      "\n",
      "Epoch: [97][170/600]\t Time 0.681 (0.679)\t Loss 0.101573 (0.104087)\n",
      "\n",
      "Epoch: [97][180/600]\t Time 0.676 (0.679)\t Loss 0.110477 (0.104176)\n",
      "\n",
      "Epoch: [97][190/600]\t Time 0.672 (0.679)\t Loss 0.105187 (0.104087)\n",
      "\n",
      "Epoch: [97][200/600]\t Time 0.677 (0.679)\t Loss 0.097770 (0.104114)\n",
      "\n",
      "Epoch: [97][210/600]\t Time 0.679 (0.678)\t Loss 0.118410 (0.104031)\n",
      "\n",
      "Epoch: [97][220/600]\t Time 0.676 (0.678)\t Loss 0.152767 (0.104217)\n",
      "\n",
      "Epoch: [97][230/600]\t Time 0.679 (0.678)\t Loss 0.085792 (0.104362)\n",
      "\n",
      "Epoch: [97][240/600]\t Time 0.671 (0.678)\t Loss 0.097899 (0.104216)\n",
      "\n",
      "Epoch: [97][250/600]\t Time 0.677 (0.678)\t Loss 0.074963 (0.104033)\n",
      "\n",
      "Epoch: [97][260/600]\t Time 0.680 (0.678)\t Loss 0.108678 (0.104528)\n",
      "\n",
      "Epoch: [97][270/600]\t Time 0.676 (0.678)\t Loss 0.102282 (0.104529)\n",
      "\n",
      "Epoch: [97][280/600]\t Time 0.673 (0.678)\t Loss 0.114962 (0.104755)\n",
      "\n",
      "Epoch: [97][290/600]\t Time 0.676 (0.678)\t Loss 0.084482 (0.104159)\n",
      "\n",
      "Epoch: [97][300/600]\t Time 0.681 (0.678)\t Loss 0.092215 (0.104137)\n",
      "\n",
      "Epoch: [97][310/600]\t Time 0.675 (0.678)\t Loss 0.111965 (0.104102)\n",
      "\n",
      "Epoch: [97][320/600]\t Time 0.685 (0.678)\t Loss 0.119766 (0.104230)\n",
      "\n",
      "Epoch: [97][330/600]\t Time 0.682 (0.678)\t Loss 0.137304 (0.104272)\n",
      "\n",
      "Epoch: [97][340/600]\t Time 0.675 (0.678)\t Loss 0.147038 (0.104294)\n",
      "\n",
      "Epoch: [97][350/600]\t Time 0.681 (0.678)\t Loss 0.102777 (0.104110)\n",
      "\n",
      "Epoch: [97][360/600]\t Time 0.678 (0.678)\t Loss 0.087603 (0.103851)\n",
      "\n",
      "Epoch: [97][370/600]\t Time 0.677 (0.678)\t Loss 0.120738 (0.103611)\n",
      "\n",
      "Epoch: [97][380/600]\t Time 0.672 (0.678)\t Loss 0.135631 (0.103609)\n",
      "\n",
      "Epoch: [97][390/600]\t Time 0.677 (0.678)\t Loss 0.108325 (0.103485)\n",
      "\n",
      "Epoch: [97][400/600]\t Time 0.680 (0.678)\t Loss 0.100296 (0.103371)\n",
      "\n",
      "Epoch: [97][410/600]\t Time 0.677 (0.678)\t Loss 0.110635 (0.103297)\n",
      "\n",
      "Epoch: [97][420/600]\t Time 0.675 (0.678)\t Loss 0.089954 (0.103371)\n",
      "\n",
      "Epoch: [97][430/600]\t Time 0.678 (0.678)\t Loss 0.113622 (0.103313)\n",
      "\n",
      "Epoch: [97][440/600]\t Time 0.674 (0.678)\t Loss 0.109346 (0.103701)\n",
      "\n",
      "Epoch: [97][450/600]\t Time 0.675 (0.678)\t Loss 0.104801 (0.103580)\n",
      "\n",
      "Epoch: [97][460/600]\t Time 0.677 (0.678)\t Loss 0.084414 (0.103526)\n",
      "\n",
      "Epoch: [97][470/600]\t Time 0.675 (0.677)\t Loss 0.083410 (0.103387)\n",
      "\n",
      "Epoch: [97][480/600]\t Time 0.685 (0.677)\t Loss 0.136630 (0.103418)\n",
      "\n",
      "Epoch: [97][490/600]\t Time 0.674 (0.677)\t Loss 0.090200 (0.103135)\n",
      "\n",
      "Epoch: [97][500/600]\t Time 0.682 (0.677)\t Loss 0.117879 (0.103166)\n",
      "\n",
      "Epoch: [97][510/600]\t Time 0.684 (0.677)\t Loss 0.076559 (0.103018)\n",
      "\n",
      "Epoch: [97][520/600]\t Time 0.674 (0.677)\t Loss 0.081662 (0.102936)\n",
      "\n",
      "Epoch: [97][530/600]\t Time 0.666 (0.677)\t Loss 0.107828 (0.102948)\n",
      "\n",
      "Epoch: [97][540/600]\t Time 0.677 (0.677)\t Loss 0.134845 (0.102797)\n",
      "\n",
      "Epoch: [97][550/600]\t Time 0.677 (0.677)\t Loss 0.080865 (0.102770)\n",
      "\n",
      "Epoch: [97][560/600]\t Time 0.672 (0.677)\t Loss 0.123847 (0.102779)\n",
      "\n",
      "Epoch: [97][570/600]\t Time 0.672 (0.677)\t Loss 0.142199 (0.102801)\n",
      "\n",
      "Epoch: [97][580/600]\t Time 0.671 (0.677)\t Loss 0.100797 (0.102791)\n",
      "\n",
      "Epoch: [97][590/600]\t Time 0.663 (0.677)\t Loss 0.071057 (0.102694)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.025 (1.025)\t Loss 0.151366 (0.151366)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.646 (0.680)\t Loss 0.082763 (0.099463)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.648 (0.662)\t Loss 0.090995 (0.100807)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.643 (0.657)\t Loss 0.122885 (0.103766)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.647 (0.654)\t Loss 0.102078 (0.102721)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.648 (0.653)\t Loss 0.076533 (0.102788)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.645 (0.652)\t Loss 0.101381 (0.102747)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.646 (0.651)\t Loss 0.121189 (0.103813)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.640 (0.651)\t Loss 0.122792 (0.103693)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.649 (0.651)\t Loss 0.113476 (0.103765)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.652 (0.650)\t Loss 0.092266 (0.103651)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.643 (0.650)\t Loss 0.083712 (0.103313)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.644 (0.650)\t Loss 0.069208 (0.102705)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.652 (0.649)\t Loss 0.064514 (0.102384)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.645 (0.649)\t Loss 0.104094 (0.103102)\n",
      "\n",
      "Epoch: [97]\t Train Loss: 0.102527  \t Val Loss: 0.103709\n",
      "\n",
      "False 0.1031284017364184\n",
      "Epoch: [98][0/600]\t Time 1.054 (1.054)\t Loss 0.123579 (0.123579)\n",
      "\n",
      "Epoch: [98][10/600]\t Time 0.674 (0.708)\t Loss 0.124051 (0.116408)\n",
      "\n",
      "Epoch: [98][20/600]\t Time 0.680 (0.692)\t Loss 0.110945 (0.107228)\n",
      "\n",
      "Epoch: [98][30/600]\t Time 0.682 (0.687)\t Loss 0.090193 (0.107180)\n",
      "\n",
      "Epoch: [98][40/600]\t Time 0.672 (0.684)\t Loss 0.086307 (0.104770)\n",
      "\n",
      "Epoch: [98][50/600]\t Time 0.676 (0.682)\t Loss 0.115167 (0.104520)\n",
      "\n",
      "Epoch: [98][60/600]\t Time 0.680 (0.681)\t Loss 0.120700 (0.105683)\n",
      "\n",
      "Epoch: [98][70/600]\t Time 0.675 (0.680)\t Loss 0.089646 (0.105472)\n",
      "\n",
      "Epoch: [98][80/600]\t Time 0.676 (0.680)\t Loss 0.089340 (0.104854)\n",
      "\n",
      "Epoch: [98][90/600]\t Time 0.683 (0.679)\t Loss 0.096063 (0.105722)\n",
      "\n",
      "Epoch: [98][100/600]\t Time 0.677 (0.679)\t Loss 0.076183 (0.105702)\n",
      "\n",
      "Epoch: [98][110/600]\t Time 0.684 (0.679)\t Loss 0.101383 (0.104773)\n",
      "\n",
      "Epoch: [98][120/600]\t Time 0.678 (0.679)\t Loss 0.095756 (0.104494)\n",
      "\n",
      "Epoch: [98][130/600]\t Time 0.677 (0.679)\t Loss 0.083850 (0.104303)\n",
      "\n",
      "Epoch: [98][140/600]\t Time 0.671 (0.678)\t Loss 0.068177 (0.103539)\n",
      "\n",
      "Epoch: [98][150/600]\t Time 0.684 (0.678)\t Loss 0.125841 (0.103168)\n",
      "\n",
      "Epoch: [98][160/600]\t Time 0.679 (0.678)\t Loss 0.088651 (0.103079)\n",
      "\n",
      "Epoch: [98][170/600]\t Time 0.673 (0.678)\t Loss 0.112408 (0.102665)\n",
      "\n",
      "Epoch: [98][180/600]\t Time 0.682 (0.678)\t Loss 0.133858 (0.102378)\n",
      "\n",
      "Epoch: [98][190/600]\t Time 0.676 (0.678)\t Loss 0.106613 (0.102507)\n",
      "\n",
      "Epoch: [98][200/600]\t Time 0.666 (0.678)\t Loss 0.124520 (0.102603)\n",
      "\n",
      "Epoch: [98][210/600]\t Time 0.680 (0.678)\t Loss 0.092511 (0.102619)\n",
      "\n",
      "Epoch: [98][220/600]\t Time 0.675 (0.678)\t Loss 0.093910 (0.102635)\n",
      "\n",
      "Epoch: [98][230/600]\t Time 0.667 (0.677)\t Loss 0.084520 (0.102589)\n",
      "\n",
      "Epoch: [98][240/600]\t Time 0.675 (0.677)\t Loss 0.079140 (0.102361)\n",
      "\n",
      "Epoch: [98][250/600]\t Time 0.676 (0.677)\t Loss 0.108880 (0.102660)\n",
      "\n",
      "Epoch: [98][260/600]\t Time 0.682 (0.677)\t Loss 0.103423 (0.102570)\n",
      "\n",
      "Epoch: [98][270/600]\t Time 0.675 (0.677)\t Loss 0.099093 (0.102748)\n",
      "\n",
      "Epoch: [98][280/600]\t Time 0.680 (0.677)\t Loss 0.141497 (0.103328)\n",
      "\n",
      "Epoch: [98][290/600]\t Time 0.677 (0.677)\t Loss 0.074339 (0.103032)\n",
      "\n",
      "Epoch: [98][300/600]\t Time 0.683 (0.677)\t Loss 0.075832 (0.102886)\n",
      "\n",
      "Epoch: [98][310/600]\t Time 0.683 (0.677)\t Loss 0.126110 (0.102812)\n",
      "\n",
      "Epoch: [98][320/600]\t Time 0.674 (0.677)\t Loss 0.099702 (0.102715)\n",
      "\n",
      "Epoch: [98][330/600]\t Time 0.683 (0.677)\t Loss 0.130610 (0.102798)\n",
      "\n",
      "Epoch: [98][340/600]\t Time 0.676 (0.677)\t Loss 0.139127 (0.103286)\n",
      "\n",
      "Epoch: [98][350/600]\t Time 0.667 (0.677)\t Loss 0.115757 (0.103240)\n",
      "\n",
      "Epoch: [98][360/600]\t Time 0.680 (0.677)\t Loss 0.101936 (0.102745)\n",
      "\n",
      "Epoch: [98][370/600]\t Time 0.679 (0.677)\t Loss 0.116259 (0.102392)\n",
      "\n",
      "Epoch: [98][380/600]\t Time 0.677 (0.677)\t Loss 0.092337 (0.102613)\n",
      "\n",
      "Epoch: [98][390/600]\t Time 0.676 (0.677)\t Loss 0.076205 (0.102705)\n",
      "\n",
      "Epoch: [98][400/600]\t Time 0.680 (0.677)\t Loss 0.102719 (0.102691)\n",
      "\n",
      "Epoch: [98][410/600]\t Time 0.677 (0.677)\t Loss 0.124505 (0.102618)\n",
      "\n",
      "Epoch: [98][420/600]\t Time 0.683 (0.677)\t Loss 0.088815 (0.102494)\n",
      "\n",
      "Epoch: [98][430/600]\t Time 0.675 (0.677)\t Loss 0.094810 (0.102501)\n",
      "\n",
      "Epoch: [98][440/600]\t Time 0.673 (0.677)\t Loss 0.102830 (0.102390)\n",
      "\n",
      "Epoch: [98][450/600]\t Time 0.677 (0.677)\t Loss 0.082909 (0.102317)\n",
      "\n",
      "Epoch: [98][460/600]\t Time 0.677 (0.677)\t Loss 0.095445 (0.102476)\n",
      "\n",
      "Epoch: [98][470/600]\t Time 0.676 (0.677)\t Loss 0.089436 (0.102622)\n",
      "\n",
      "Epoch: [98][480/600]\t Time 0.679 (0.677)\t Loss 0.079920 (0.102582)\n",
      "\n",
      "Epoch: [98][490/600]\t Time 0.686 (0.677)\t Loss 0.091148 (0.102566)\n",
      "\n",
      "Epoch: [98][500/600]\t Time 0.677 (0.677)\t Loss 0.108370 (0.102643)\n",
      "\n",
      "Epoch: [98][510/600]\t Time 0.681 (0.677)\t Loss 0.076332 (0.102591)\n",
      "\n",
      "Epoch: [98][520/600]\t Time 0.672 (0.677)\t Loss 0.100968 (0.102481)\n",
      "\n",
      "Epoch: [98][530/600]\t Time 0.679 (0.677)\t Loss 0.093785 (0.102498)\n",
      "\n",
      "Epoch: [98][540/600]\t Time 0.680 (0.677)\t Loss 0.079863 (0.102473)\n",
      "\n",
      "Epoch: [98][550/600]\t Time 0.675 (0.677)\t Loss 0.081061 (0.102361)\n",
      "\n",
      "Epoch: [98][560/600]\t Time 0.682 (0.677)\t Loss 0.096080 (0.102146)\n",
      "\n",
      "Epoch: [98][570/600]\t Time 0.676 (0.677)\t Loss 0.077383 (0.102043)\n",
      "\n",
      "Epoch: [98][580/600]\t Time 0.668 (0.677)\t Loss 0.076218 (0.102054)\n",
      "\n",
      "Epoch: [98][590/600]\t Time 0.678 (0.677)\t Loss 0.093020 (0.102162)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.020 (1.020)\t Loss 0.102200 (0.102200)\n",
      "\n",
      "Val/Test: [10/150]\t Time 0.648 (0.679)\t Loss 0.114288 (0.098124)\n",
      "\n",
      "Val/Test: [20/150]\t Time 0.643 (0.662)\t Loss 0.108823 (0.099282)\n",
      "\n",
      "Val/Test: [30/150]\t Time 0.641 (0.656)\t Loss 0.089016 (0.099672)\n",
      "\n",
      "Val/Test: [40/150]\t Time 0.646 (0.653)\t Loss 0.113207 (0.100499)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.643 (0.652)\t Loss 0.088132 (0.102538)\n",
      "\n",
      "Val/Test: [60/150]\t Time 0.644 (0.651)\t Loss 0.069633 (0.103578)\n",
      "\n",
      "Val/Test: [70/150]\t Time 0.646 (0.650)\t Loss 0.096836 (0.104308)\n",
      "\n",
      "Val/Test: [80/150]\t Time 0.642 (0.650)\t Loss 0.107604 (0.103430)\n",
      "\n",
      "Val/Test: [90/150]\t Time 0.650 (0.649)\t Loss 0.102710 (0.104273)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.652 (0.649)\t Loss 0.089969 (0.103939)\n",
      "\n",
      "Val/Test: [110/150]\t Time 0.642 (0.649)\t Loss 0.125792 (0.103514)\n",
      "\n",
      "Val/Test: [120/150]\t Time 0.652 (0.649)\t Loss 0.065275 (0.103365)\n",
      "\n",
      "Val/Test: [130/150]\t Time 0.643 (0.649)\t Loss 0.090518 (0.103636)\n",
      "\n",
      "Val/Test: [140/150]\t Time 0.651 (0.648)\t Loss 0.100562 (0.102860)\n",
      "\n",
      "Epoch: [98]\t Train Loss: 0.102167  \t Val Loss: 0.103348\n",
      "\n",
      "False 0.1031284017364184\n",
      "Epoch: [99][0/600]\t Time 1.040 (1.040)\t Loss 0.109423 (0.109423)\n",
      "\n",
      "Epoch: [99][10/600]\t Time 0.672 (0.711)\t Loss 0.094023 (0.100892)\n",
      "\n",
      "Epoch: [99][20/600]\t Time 0.676 (0.694)\t Loss 0.099183 (0.101176)\n",
      "\n",
      "Epoch: [99][30/600]\t Time 0.666 (0.688)\t Loss 0.095363 (0.100018)\n",
      "\n",
      "Epoch: [99][40/600]\t Time 0.678 (0.685)\t Loss 0.082058 (0.098605)\n",
      "\n",
      "Epoch: [99][50/600]\t Time 0.677 (0.684)\t Loss 0.133865 (0.100049)\n",
      "\n",
      "Epoch: [99][60/600]\t Time 0.669 (0.682)\t Loss 0.112874 (0.099770)\n",
      "\n",
      "Epoch: [99][70/600]\t Time 0.680 (0.682)\t Loss 0.115593 (0.101356)\n",
      "\n",
      "Epoch: [99][80/600]\t Time 0.674 (0.681)\t Loss 0.110705 (0.101271)\n",
      "\n",
      "Epoch: [99][90/600]\t Time 0.677 (0.681)\t Loss 0.126612 (0.101444)\n",
      "\n",
      "Epoch: [99][100/600]\t Time 0.678 (0.680)\t Loss 0.101776 (0.101309)\n",
      "\n",
      "Epoch: [99][110/600]\t Time 0.677 (0.680)\t Loss 0.118385 (0.100934)\n",
      "\n",
      "Epoch: [99][120/600]\t Time 0.674 (0.680)\t Loss 0.089764 (0.100173)\n",
      "\n",
      "Epoch: [99][130/600]\t Time 0.680 (0.680)\t Loss 0.123149 (0.101391)\n",
      "\n",
      "Epoch: [99][140/600]\t Time 0.677 (0.679)\t Loss 0.103479 (0.102437)\n",
      "\n",
      "Epoch: [99][150/600]\t Time 0.674 (0.679)\t Loss 0.094126 (0.102579)\n",
      "\n",
      "Epoch: [99][160/600]\t Time 0.679 (0.679)\t Loss 0.117046 (0.103273)\n",
      "\n",
      "Epoch: [99][170/600]\t Time 0.680 (0.679)\t Loss 0.093654 (0.103471)\n",
      "\n",
      "Epoch: [99][180/600]\t Time 0.665 (0.679)\t Loss 0.085068 (0.102674)\n",
      "\n",
      "Epoch: [99][190/600]\t Time 0.680 (0.679)\t Loss 0.124910 (0.102828)\n",
      "\n",
      "Epoch: [99][200/600]\t Time 0.678 (0.679)\t Loss 0.097066 (0.102936)\n",
      "\n",
      "Epoch: [99][210/600]\t Time 0.674 (0.679)\t Loss 0.112176 (0.103179)\n",
      "\n",
      "Epoch: [99][220/600]\t Time 0.678 (0.679)\t Loss 0.084061 (0.103066)\n",
      "\n",
      "Epoch: [99][230/600]\t Time 0.681 (0.679)\t Loss 0.103922 (0.102966)\n",
      "\n",
      "Epoch: [99][240/600]\t Time 0.677 (0.679)\t Loss 0.099898 (0.103014)\n",
      "\n",
      "Epoch: [99][250/600]\t Time 0.685 (0.679)\t Loss 0.104636 (0.103252)\n",
      "\n",
      "Epoch: [99][260/600]\t Time 0.673 (0.679)\t Loss 0.082848 (0.103527)\n",
      "\n",
      "Epoch: [99][270/600]\t Time 0.680 (0.679)\t Loss 0.071999 (0.103174)\n",
      "\n",
      "Epoch: [99][280/600]\t Time 0.677 (0.679)\t Loss 0.123133 (0.103105)\n",
      "\n",
      "Epoch: [99][290/600]\t Time 0.671 (0.678)\t Loss 0.098487 (0.103188)\n",
      "\n",
      "Epoch: [99][300/600]\t Time 0.676 (0.678)\t Loss 0.125008 (0.102931)\n",
      "\n",
      "Epoch: [99][310/600]\t Time 0.679 (0.678)\t Loss 0.117692 (0.102890)\n",
      "\n",
      "Epoch: [99][320/600]\t Time 0.678 (0.678)\t Loss 0.103682 (0.103089)\n",
      "\n",
      "Epoch: [99][330/600]\t Time 0.682 (0.678)\t Loss 0.112272 (0.103077)\n",
      "\n",
      "Epoch: [99][340/600]\t Time 0.679 (0.678)\t Loss 0.101593 (0.102995)\n",
      "\n",
      "Epoch: [99][350/600]\t Time 0.675 (0.678)\t Loss 0.096770 (0.103229)\n",
      "\n",
      "Epoch: [99][360/600]\t Time 0.677 (0.678)\t Loss 0.089976 (0.103110)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-41:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 38, in _worker_loop\n",
      "    data_queue.put((idx, samples))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 349, in put\n",
      "    obj = ForkingPickler.dumps(obj)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/reduction.py\", line 50, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/torch/multiprocessing/reductions.py\", line 113, in reduce_storage\n",
      "    fd, size = storage._share_fd_()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-366a73fd45d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstartModel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./resnet_for_rnn/avepool_dropout_ResNet34_LSTM_experiment/checkpoint.pth.tar'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstartModel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstartModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstartEpoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mnumEpochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-70cec894203d>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(startModel, startEpoch, numEpochs)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstartEpoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstartEpoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnumEpochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# train for one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdset_loaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# evaluate on validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-a078c37f59e2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0minput_image_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_audio_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpu_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m             \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_audio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpu_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpu_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# compute output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_image_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_audio_var\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# measure accuracy and record loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-24edc8d297ac>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mvideoProcessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_dropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideoProcessed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0maudioProcessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maudioBranch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudioData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_PARTITIONS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_AUDIO_FEATURES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# will output a (n x partitions)x 128 tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideoProcessed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maudioProcessed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpu_dtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#(N,10,160)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mh0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_LSTM_HIDDEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpu_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mc0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_LSTM_HIDDEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpu_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mcat\u001b[1;34m(iterable, dim)\u001b[0m\n\u001b[0;32m    834\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 836\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mConcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "startModel='./resnet_for_rnn/avepool_dropout_ResNet34_LSTM_experiment/checkpoint.pth.tar'\n",
    "train_model(startModel=startModel, startEpoch=100 , numEpochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# visualize the pred and groundtruth and images of the latest val batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "val_pred = (val_preds.data).cpu().numpy()\n",
    "val_gt = (Variable(val_targets).data).cpu().numpy()\n",
    "# select a video in batch to see: idx can be 0-7\n",
    "idx = 7\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(np.arange(5),val_gt[idx])\n",
    "plt.xticks(np.arange(5),['e','n','a','c','o'])\n",
    "plt.gca().set_ylim([0.0,1.0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(np.arange(5),val_pred[idx])\n",
    "plt.xticks(np.arange(5),['e','n','a','c','o'])\n",
    "plt.gca().set_ylim([0.0,1.0])\n",
    "plt.show()\n",
    "val_unflattened_sample = val_imgs.view(-1,3,256,256)\n",
    "# Make a grid from batch\n",
    "plt.figure( figsize=(30, 3))\n",
    "out = torchvision.utils.make_grid(val_unflattened_sample[idx*10:idx*10+10],nrow=10)\n",
    "imshow(out, title='trainning sample')\n",
    "#plt.savefig('train_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

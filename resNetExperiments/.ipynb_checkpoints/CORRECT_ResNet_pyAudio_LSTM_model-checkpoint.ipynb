{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_dir = '/home/noa_glaser/proj/data/train-frames/train'\n",
    "val_dataset_dir = '/home/noa_glaser/proj/data/train-frames/val'\n",
    "audio_dataset_dir = '/home/noa_glaser/proj/data/train-audio'\n",
    "label_dataset_dir = '/home/noa_glaser/proj/data/'\n",
    "exp_name = 'ResNet_LSTM_L1' # roughly 3K videos\n",
    "num_classes = 5 \n",
    "num_partition = 10\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import time\n",
    "import copy\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first, get the Images (path) and their labels (five personality traits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_img_audio_label(dataset_dir,audio_dataset_dir,label_dataset_dir):\n",
    "    \"\"\"Returns a list of np.array(img_paths), np.array(audio_paths),\n",
    "        np.array(labels), np.array(raw_movienames)\n",
    "    Args:\n",
    "    dataset for video, for audio_feats from pyAudioAnalysis, labels\n",
    "    \"\"\"\n",
    " \n",
    "    print(\"processing dataset: \"+ dataset_dir)\n",
    "    img_paths = [] \n",
    "    audio_paths=[]\n",
    "    raw_movienames = []\n",
    "    labels = []\n",
    "\n",
    "    annotaion_filename = label_dataset_dir + \"/annotation_training.pkl\"\n",
    "    \n",
    "    with open(annotaion_filename, 'rb') as f:\n",
    "        label_dicts = pickle.load(f, encoding='latin1') \n",
    "\n",
    "    for movie in os.listdir(dataset_dir):\n",
    "        fileEnding ='_50uniform' #TODO: figure out how to make more general\n",
    "        if fileEnding not in movie: continue #skip non-movie files\n",
    "        raw_moviename = movie.replace(fileEnding,'.mp4')      \n",
    "        big_five = [label_dicts['extraversion'][raw_moviename], \n",
    "                    label_dicts['neuroticism'][raw_moviename],\n",
    "                    label_dicts['agreeableness'][raw_moviename],\n",
    "                    label_dicts['conscientiousness'][raw_moviename],\n",
    "                    label_dicts['openness'][raw_moviename] ]\n",
    "                    #label_dicts['interview'][raw_moviename]]      \n",
    "        movie_path = os.path.join(dataset_dir, movie)\n",
    "        mv_partitions = []\n",
    "        p = 0\n",
    "        all_imgs = os.listdir(movie_path)\n",
    "        assert(len(all_imgs) >= num_partition)\n",
    "        opened = True\n",
    "        for i in range(num_partition):\n",
    "            path = os.path.join(movie_path, all_imgs[i])\n",
    "            try:\n",
    "                open(path)\n",
    "            except:\n",
    "                print('image failed to open',path)\n",
    "                opened = False\n",
    "                \n",
    "            mv_partitions.append(path)\n",
    "        assert(len(mv_partitions)==num_partition)\n",
    "        \n",
    "        \n",
    "        audiofeat_path = os.path.join(audio_dataset_dir,raw_moviename+'.wav.csv')\n",
    "        try:\n",
    "            open(audiofeat_path)\n",
    "        except:\n",
    "            print('audio failed to open',path)\n",
    "            opened = False\n",
    "        if opened :\n",
    "            img_paths.append(mv_partitions)\n",
    "            audio_paths.append(audiofeat_path)\n",
    "            raw_movienames.append(raw_moviename)\n",
    "            labels.append(big_five)\n",
    "            \n",
    "    \n",
    "    return np.array(img_paths),np.array(audio_paths),\\\n",
    "                np.array(labels), np.array(raw_movienames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## use this if we have seperated train/val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dataset: /home/noa_glaser/proj/data/train-frames/train\n",
      "processing dataset: /home/noa_glaser/proj/data/train-frames/val\n"
     ]
    }
   ],
   "source": [
    "train_img_paths,train_audio_paths, train_labels, train_movienames \\\n",
    "        = get_img_audio_label(train_dataset_dir,audio_dataset_dir,label_dataset_dir) \n",
    "val_img_paths,val_audio_paths, val_labels, val_movienames \\\n",
    "= get_img_audio_label(val_dataset_dir,audio_dataset_dir,label_dataset_dir)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Loader. Data is normalized before feeding into model (as required by the pretrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def default_img_loader(img_paths,transform):\n",
    "    ten_img_tensor = []\n",
    "    for path in img_paths:\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if transform is not None:\n",
    "            img = transform(img)\n",
    "        ten_img_tensor.append(img)\n",
    "        \n",
    "    return torch.cat(ten_img_tensor)\n",
    "        \n",
    "\n",
    "def default_audio_loader(path):\n",
    "\treturn np.loadtxt(path,delimiter=',')\n",
    "\n",
    "class VisualAudio(data.Dataset):\n",
    "    def __init__(self,split,img_paths,audio_paths, movie_names,labels,transform=None,\n",
    "                 img_loader=default_img_loader,audio_loader=default_audio_loader):\n",
    "        self.split = split \n",
    "        self.img_paths = img_paths\n",
    "        self.audio_paths = audio_paths\n",
    "        self.movie_names = movie_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.img_loader=img_loader\n",
    "        self.audio_loader= audio_loader\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_paths, audio_paths,target = self.img_paths[index], \\\n",
    "                                        self.audio_paths[index], self.labels[index]\n",
    "        ten_img_tensor = self.img_loader(img_paths,self.transform)\n",
    "        ten_audio = self.audio_loader(audio_paths)\n",
    "        #return 30x224x224 , 10x68, 10 x 5\n",
    "        \n",
    "        assert(ten_img_tensor.size() == (30,256,256))\n",
    "        \n",
    "        return ten_img_tensor, ten_audio[:10,:], target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 4800, 'val': 1200}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import  transforms\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.RandomCrop(256),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "        \n",
    "dsets = {}\n",
    "dsets['train'] = VisualAudio('train',train_img_paths,train_audio_paths,\\\n",
    "                    train_movienames ,train_labels,transform=data_transforms['train'] )\n",
    "dsets['val'] = VisualAudio('val',val_img_paths,val_audio_paths,\\\n",
    "                         val_movienames,val_labels,transform=data_transforms['val'] )\n",
    "\n",
    "dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=1) for x in ['train', 'val']}\n",
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "dset_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Some dataset examples (each batch is 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    \n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "#train_imgsamples,train_audiosamle,train_labelsample = next(iter(dset_loaders['train']))\n",
    "\n",
    "#train_unflattened_sample = train_imgsamples.view(-1,3,256,256)[0:10,:,:,:]\n",
    "## Make a grid from batch\n",
    "#plt.figure( figsize=(100, 20))\n",
    "#out = torchvision.utils.make_grid(train_unflattened_sample,nrow=10)\n",
    "#imshow(out, title='trainning sample')\n",
    "#plt.savefig('train_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#val_imgsamples,val_audiosamle,val_labelsample = next(iter(dset_loaders['val']))\n",
    "#val_unflattened_sample = val_imgsamples.view(-1,3,256,256)\n",
    "### Make a grid from val batch\n",
    "#plt.figure( figsize=(10, 16))\n",
    "#out2 = torchvision.utils.make_grid(val_unflattened_sample,nrow=10)\n",
    "#imshow(out2, title='validation sample')\n",
    "#plt.savefig('val_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AudioVisualLSTM(nn.Module):\n",
    "    NUM_AUDIO_INPUT = 68\n",
    "    NUM_VID_FEATURES = 128\n",
    "    NUM_AUDIO_FEATURES = 32\n",
    "    NUM_LSTM_HIDDEN = 128\n",
    "    NUM_PARTITIONS = 10\n",
    "    NUM_CLASS = 5\n",
    "    NUM_IMG_SIZE = 256\n",
    "    NUM_CHANNEL = 3\n",
    "    \n",
    "    def __init__(self):        \n",
    "        super(AudioVisualLSTM, self).__init__()\n",
    "        self.audioBranch =  nn.Sequential(nn.Linear(self.NUM_AUDIO_INPUT,self.NUM_AUDIO_FEATURES))\n",
    "        self.videoBranch = self._createVideoBranch()\n",
    "        self.video_dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=(self.NUM_VID_FEATURES+self.NUM_AUDIO_FEATURES),\n",
    "            hidden_size=self.NUM_LSTM_HIDDEN,\n",
    "            num_layers=1,\n",
    "            bias=True,\n",
    "            batch_first=True # input and output tensors provided as (batch, seq, feature)\n",
    "            # can add dropout later\n",
    "            )\n",
    "        self.fc = nn.Linear(self.NUM_LSTM_HIDDEN,self.NUM_CLASS)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.avg = nn.AvgPool1d(self.NUM_PARTITIONS,self.NUM_PARTITIONS)\n",
    "\n",
    "    def _createVideoBranch(self):\n",
    "        model_pretrained = torchvision.models.resnet34(pretrained=True)\n",
    "        # All of the parameters are freezed, not to change (newly constructed layers' params won't be influenced)\n",
    "        for param in model_pretrained.parameters():\n",
    "            param.requires_grad = False   \n",
    "        model_pretrained.fc = nn.Linear(model_pretrained.fc.in_features, self.NUM_VID_FEATURES)\n",
    "        return model_pretrained\n",
    "    \n",
    "    def forward(self, x):\n",
    "        videoData = x[0].view(-1,self.NUM_CHANNEL,self.NUM_IMG_SIZE,self.NUM_IMG_SIZE)\n",
    "        audioData = x[1].view(-1,self.NUM_AUDIO_INPUT)\n",
    "        videoProcessed = self.videoBranch(videoData).view(-1,self.NUM_PARTITIONS,self.NUM_VID_FEATURES) # will output a (n x partitions)x 32 tensor\n",
    "        videoProcessed = self.video_dropout(videoProcessed)\n",
    "        audioProcessed = self.audioBranch(audioData).view(-1,self.NUM_PARTITIONS,self.NUM_AUDIO_FEATURES)# will output a (n x partitions)x 128 tensor\n",
    "        x = torch.cat((videoProcessed, audioProcessed), 2).type(gpu_dtype) #(N,10,160)\n",
    "        h0 = Variable(torch.zeros(1, x.size()[0], self.NUM_LSTM_HIDDEN)).type(gpu_dtype)\n",
    "        c0 = Variable(torch.zeros(1, x.size()[0], self.NUM_LSTM_HIDDEN)).type(gpu_dtype)\n",
    "        x,cn = self.lstm(x, (h0, c0))\n",
    "        x = x.contiguous().view(-1,self.NUM_LSTM_HIDDEN)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x) #(N*P,5)\n",
    "        x = self.avg(x.view(-1,self.NUM_PARTITIONS,self.NUM_CLASS).transpose(1,2)).squeeze() #(N,5)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some gpu configs\n",
    "use_gpu = True\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "\n",
    "def get_learnable_params(m,verbose = 0):\n",
    "    ret = []\n",
    "    for l in m.parameters():\n",
    "        if l.requires_grad == True:\n",
    "            ret.append(l)\n",
    "            if verbose == 1:\n",
    "                print (l.size())\n",
    "            if verbose == 2:\n",
    "                print (l)\n",
    "    return ret\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"Saves checkpoint to disk\"\"\"\n",
    "    directory = \"resnet_for_rnn/%s/\"%(exp_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filename = directory + filename\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'resnet_for_rnn/%s/'%(exp_name) + 'model_best.pth.tar')\n",
    "\n",
    "def log_value(to_log, log_path = './log_'+ exp_name + '.txt'):\n",
    "    log_file = open(log_path, 'a+')\n",
    "    log_file.write(to_log)\n",
    "    log_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_freq = 50\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch) :\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        input_image,input_audio, target = data\n",
    "        input_image_var, input_audio_var,target_var = Variable(input_image.type(gpu_dtype)), \\\n",
    "            Variable(input_audio.type(gpu_dtype)),Variable(target.type(gpu_dtype))\n",
    "        # compute output\n",
    "        output = model([input_image_var,input_audio_var])\n",
    "        loss = criterion(output, target_var)\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data[0], input_image.size(0))\n",
    "        # compute gradient and do Adam step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i % log_freq == 0:\n",
    "            to_log = 'Epoch: [{0}][{1}/{2}]\\t Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t Loss {loss.val:f} ({loss.avg:f})\\n'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time, loss=losses)\n",
    "            log_value(to_log)\n",
    "            print(to_log)\n",
    "            \n",
    "       \n",
    "    return losses.avg\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    \"\"\"Perform validation on the validation set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_E = AverageMeter()\n",
    "    losses_N = AverageMeter()\n",
    "    losses_A= AverageMeter()\n",
    "    losses_C= AverageMeter()\n",
    "    losses_O= AverageMeter()\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, data in enumerate(val_loader):\n",
    "        input_image,input_audio, target = data\n",
    "        input_image_var, input_audio_var,target_var = Variable(input_image.type(gpu_dtype)), \\\n",
    "        Variable(input_audio.type(gpu_dtype)),Variable(target.type(gpu_dtype))\n",
    "        # compute output\n",
    "        output = model([input_image_var,input_audio_var])\n",
    "           \n",
    "        loss = criterion(output, target_var)\n",
    "        loss_E= criterion(output[:,0], target_var[:,0])\n",
    "        loss_N= criterion(output[:,1], target_var[:,1])\n",
    "        loss_A= criterion(output[:,2], target_var[:,2])\n",
    "        loss_C= criterion(output[:,3], target_var[:,3])\n",
    "        loss_O= criterion(output[:,4], target_var[:,4])\n",
    "        \n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data[0], input_image.size(0))\n",
    "        \n",
    "        losses_E.update(loss_E.data[0], input_image.size(0))\n",
    "        losses_N.update(loss_N.data[0], input_image.size(0))\n",
    "        losses_A.update(loss_A.data[0], input_image.size(0))\n",
    "        losses_C.update(loss_C.data[0], input_image.size(0))\n",
    "        losses_O.update(loss_O.data[0], input_image.size(0))\n",
    "        ocean_losses = [losses_O.avg,losses_C.avg,losses_E.avg,losses_A.avg,losses_N.avg]\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i %  log_freq == 0:\n",
    "            \n",
    "            to_log = 'Val/Test: [{0}/{1}]\\t Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t Loss {loss.val:f} ({loss.avg:f})\\t Loss_O {losses_O.val:f} ({losses_O.avg:f}) \\t Loss_C {losses_C.val:f} ({losses_C.avg:f})\\t Loss_E {losses_E.val:f} ({losses_E.avg:f})\\t Loss_A {losses_A.val:f} ({losses_A.avg:f})\\t Loss_N {losses_N.val:f} ({losses_N.avg:f})\\n'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses,losses_O=losses_O,losses_C=losses_C,losses_E=losses_E,losses_A=losses_A,losses_N=losses_N)\n",
    "            log_value(to_log)\n",
    "            print(to_log)\n",
    "                     \n",
    "    return losses.avg, output,input_image,target,ocean_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: for larger dataset, consider a step function or exponentialdecay\n",
    "def lr_scheduler(optimizer, epoch):\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# can retriecve these latest val results for plotting\n",
    "val_preds,val_imgs,val_targets = None,None,None\n",
    "\n",
    "def train_model(startModel=None, startEpoch=0, numEpochs=10):    \n",
    "    model = AudioVisualLSTM().type(gpu_dtype)\n",
    "    #del model_base\n",
    "\n",
    "    #  changed to l1 loss to reflect competition \n",
    "    criterion = nn.L1Loss().type(gpu_dtype)\n",
    "\n",
    "    #only optimizing the new_fc layer parameters, other pretrained weights are freezedÂ¶\n",
    "    optimizer  = optim.SGD(get_learnable_params(model),lr=1e-7, momentum=0.9,weight_decay=5e-3)\n",
    "\n",
    "    best_loss = 1000 # will get overwritten\n",
    "    \n",
    "    if(startModel != None):\n",
    "        print(\"=> loading checkpoint '{}'\".format(startModel))\n",
    "        checkpoint = torch.load(startModel)\n",
    "        startEpoch = checkpoint['epoch']\n",
    "        best_loss = checkpoint['best_loss'] # for now because old loss is stale\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        # todo - figure out why not working\n",
    "        #optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        # print(optimizer.param_groups)\n",
    "        \n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(startModel, checkpoint['epoch']))\n",
    "    #else:\n",
    "        # benchmark the model \n",
    "        #best_loss = validate(dset_loaders['val'], model, criterion, startEpoch)\n",
    "        \n",
    "    bestModel = model\n",
    "\n",
    "    for epoch in range(startEpoch,startEpoch+numEpochs):\n",
    "        # train for one epoch\n",
    "        train_loss = train(dset_loaders['train'], model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        val_loss,val_preds,val_imgs,val_targets,val_ocean_loss = validate(dset_loaders['val'], model, criterion, epoch)\n",
    "\n",
    "        # log \n",
    "        log_value('Epoch: [{0}]\\t Train Loss: {train_loss:f}  \\t Val Loss: {val_loss:f} Loss_O: {Loss_O:f} Loss_C: {Loss_C:f} Loss_E: {Loss_E:f} Loss_A: {Loss_A:f} Loss_N: {Loss_N:f}\\n'.format(epoch,\\\n",
    "                    train_loss=train_loss,val_loss=val_loss,Loss_O=val_ocean_loss[0],Loss_C=val_ocean_loss[1],Loss_E=val_ocean_loss[2],Loss_A=val_ocean_loss[3],Loss_N=val_ocean_loss[4]),'./%s_epoch_log.txt'%exp_name)\n",
    "        print('Epoch: [{0}]\\t Train Loss: {train_loss:f}  \\t Val Loss: {val_loss:f} Loss_O: {Loss_O:f} Loss_C: {Loss_C:f} Loss_E: {Loss_E:f} Loss_A: {Loss_A:f} Loss_N: {Loss_N:f}\\n'.format(epoch,\\\n",
    "                    train_loss=train_loss,val_loss=val_loss,Loss_O=val_ocean_loss[0],Loss_C=val_ocean_loss[1],Loss_E=val_ocean_loss[2],Loss_A=val_ocean_loss[3],Loss_N=val_ocean_loss[4]))\n",
    "\n",
    "        # remember best loss and save checkpoint\n",
    "        is_best = val_loss <= best_loss\n",
    "        best_loss = min(val_loss, best_loss)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': 'resnet_LSTM_l1',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print (is_best, best_loss)\n",
    "\n",
    "    print ('Best Loss: ', best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Start to train a new model run this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train from the latest model checpoint run this cell, specify startepoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint './resnet_for_rnn/ResNet_LSTM_L1/checkpoint.pth.tar'\n",
      "=> loaded checkpoint './resnet_for_rnn/ResNet_LSTM_L1/checkpoint.pth.tar' (epoch 206)\n",
      "Epoch: [206][0/600]\t Time 9.288 (9.288)\t Loss 0.113466 (0.113466)\n",
      "\n",
      "Epoch: [206][50/600]\t Time 0.682 (0.983)\t Loss 0.095939 (0.103283)\n",
      "\n",
      "Epoch: [206][100/600]\t Time 0.824 (0.870)\t Loss 0.103013 (0.104075)\n",
      "\n",
      "Epoch: [206][150/600]\t Time 0.691 (0.833)\t Loss 0.105353 (0.103096)\n",
      "\n",
      "Epoch: [206][200/600]\t Time 0.698 (0.813)\t Loss 0.079003 (0.103513)\n",
      "\n",
      "Epoch: [206][250/600]\t Time 0.809 (0.800)\t Loss 0.133078 (0.103087)\n",
      "\n",
      "Epoch: [206][300/600]\t Time 0.690 (0.794)\t Loss 0.100575 (0.102669)\n",
      "\n",
      "Epoch: [206][350/600]\t Time 0.691 (0.785)\t Loss 0.068790 (0.102722)\n",
      "\n",
      "Epoch: [206][400/600]\t Time 0.693 (0.784)\t Loss 0.091774 (0.102956)\n",
      "\n",
      "Epoch: [206][450/600]\t Time 0.700 (0.780)\t Loss 0.082015 (0.102337)\n",
      "\n",
      "Epoch: [206][500/600]\t Time 0.729 (0.775)\t Loss 0.106015 (0.102194)\n",
      "\n",
      "Epoch: [206][550/600]\t Time 0.877 (0.772)\t Loss 0.087619 (0.101945)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.662 (1.662)\t Loss 0.090858 (0.090858)\t Loss_O 0.091817 (0.091817) \t Loss_C 0.084264 (0.084264)\t Loss_E 0.093751 (0.093751)\t Loss_A 0.097439 (0.097439)\t Loss_N 0.087017 (0.087017)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.663 (0.712)\t Loss 0.061876 (0.104030)\t Loss_O 0.063032 (0.101083) \t Loss_C 0.071871 (0.104810)\t Loss_E 0.066395 (0.107227)\t Loss_A 0.043884 (0.098572)\t Loss_N 0.064199 (0.108459)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.659 (0.718)\t Loss 0.112884 (0.103022)\t Loss_O 0.105809 (0.102845) \t Loss_C 0.096009 (0.103705)\t Loss_E 0.121271 (0.104237)\t Loss_A 0.100462 (0.093937)\t Loss_N 0.140872 (0.110388)\n",
      "\n",
      "Epoch: [206]\t Train Loss: 0.102257  \t Val Loss: 0.103474 Loss_O: 0.102102 Loss_C: 0.102366 Loss_E: 0.105554 Loss_A: 0.096644 Loss_N: 0.110703\n",
      "\n",
      "False 0.10299886782964071\n",
      "Epoch: [207][0/600]\t Time 1.062 (1.062)\t Loss 0.110019 (0.110019)\n",
      "\n",
      "Epoch: [207][50/600]\t Time 0.697 (0.702)\t Loss 0.101073 (0.097613)\n",
      "\n",
      "Epoch: [207][100/600]\t Time 0.694 (0.697)\t Loss 0.131153 (0.102983)\n",
      "\n",
      "Epoch: [207][150/600]\t Time 0.682 (0.695)\t Loss 0.124107 (0.102768)\n",
      "\n",
      "Epoch: [207][200/600]\t Time 0.694 (0.694)\t Loss 0.083138 (0.102136)\n",
      "\n",
      "Epoch: [207][250/600]\t Time 0.697 (0.695)\t Loss 0.070300 (0.101195)\n",
      "\n",
      "Epoch: [207][300/600]\t Time 0.696 (0.695)\t Loss 0.114112 (0.101998)\n",
      "\n",
      "Epoch: [207][350/600]\t Time 0.697 (0.695)\t Loss 0.087721 (0.101893)\n",
      "\n",
      "Epoch: [207][400/600]\t Time 0.693 (0.695)\t Loss 0.104798 (0.102188)\n",
      "\n",
      "Epoch: [207][450/600]\t Time 0.686 (0.695)\t Loss 0.093095 (0.101672)\n",
      "\n",
      "Epoch: [207][500/600]\t Time 0.693 (0.695)\t Loss 0.102034 (0.101572)\n",
      "\n",
      "Epoch: [207][550/600]\t Time 0.689 (0.695)\t Loss 0.089034 (0.101680)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.039 (1.039)\t Loss 0.113615 (0.113615)\t Loss_O 0.089644 (0.089644) \t Loss_C 0.132696 (0.132696)\t Loss_E 0.111784 (0.111784)\t Loss_A 0.121466 (0.121466)\t Loss_N 0.112486 (0.112486)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.662 (0.674)\t Loss 0.120597 (0.105430)\t Loss_O 0.150188 (0.104511) \t Loss_C 0.080550 (0.104323)\t Loss_E 0.090216 (0.106635)\t Loss_A 0.151791 (0.097874)\t Loss_N 0.130241 (0.113809)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.665 (0.670)\t Loss 0.135514 (0.103385)\t Loss_O 0.087078 (0.100831) \t Loss_C 0.154220 (0.102052)\t Loss_E 0.125719 (0.105225)\t Loss_A 0.160005 (0.097271)\t Loss_N 0.150548 (0.111545)\n",
      "\n",
      "Epoch: [207]\t Train Loss: 0.101610  \t Val Loss: 0.103087 Loss_O: 0.102085 Loss_C: 0.101359 Loss_E: 0.105315 Loss_A: 0.096196 Loss_N: 0.110478\n",
      "\n",
      "False 0.10299886782964071\n",
      "Epoch: [208][0/600]\t Time 1.083 (1.083)\t Loss 0.099712 (0.099712)\n",
      "\n",
      "Epoch: [208][50/600]\t Time 0.686 (0.702)\t Loss 0.085161 (0.104140)\n",
      "\n",
      "Epoch: [208][100/600]\t Time 0.694 (0.699)\t Loss 0.070927 (0.104199)\n",
      "\n",
      "Epoch: [208][150/600]\t Time 0.693 (0.697)\t Loss 0.086659 (0.104560)\n",
      "\n",
      "Epoch: [208][200/600]\t Time 0.701 (0.697)\t Loss 0.106911 (0.104360)\n",
      "\n",
      "Epoch: [208][250/600]\t Time 0.691 (0.697)\t Loss 0.066259 (0.104008)\n",
      "\n",
      "Epoch: [208][300/600]\t Time 0.704 (0.696)\t Loss 0.097283 (0.104083)\n",
      "\n",
      "Epoch: [208][350/600]\t Time 0.688 (0.696)\t Loss 0.131463 (0.103298)\n",
      "\n",
      "Epoch: [208][400/600]\t Time 0.696 (0.696)\t Loss 0.084896 (0.103679)\n",
      "\n",
      "Epoch: [208][450/600]\t Time 0.698 (0.696)\t Loss 0.091797 (0.103039)\n",
      "\n",
      "Epoch: [208][500/600]\t Time 0.698 (0.696)\t Loss 0.076015 (0.103141)\n",
      "\n",
      "Epoch: [208][550/600]\t Time 0.688 (0.696)\t Loss 0.102004 (0.102832)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.039 (1.039)\t Loss 0.104432 (0.104432)\t Loss_O 0.114004 (0.114004) \t Loss_C 0.079494 (0.079494)\t Loss_E 0.104266 (0.104266)\t Loss_A 0.113918 (0.113918)\t Loss_N 0.110478 (0.110478)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.661 (0.673)\t Loss 0.101454 (0.108323)\t Loss_O 0.116418 (0.105440) \t Loss_C 0.082900 (0.101120)\t Loss_E 0.096489 (0.115172)\t Loss_A 0.135153 (0.105353)\t Loss_N 0.076312 (0.114531)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.663 (0.667)\t Loss 0.093556 (0.103819)\t Loss_O 0.062119 (0.102535) \t Loss_C 0.102223 (0.101471)\t Loss_E 0.115363 (0.106172)\t Loss_A 0.086464 (0.098083)\t Loss_N 0.101612 (0.110834)\n",
      "\n",
      "Epoch: [208]\t Train Loss: 0.102633  \t Val Loss: 0.103804 Loss_O: 0.102227 Loss_C: 0.102870 Loss_E: 0.106041 Loss_A: 0.096626 Loss_N: 0.111259\n",
      "\n",
      "False 0.10299886782964071\n",
      "Epoch: [209][0/600]\t Time 1.095 (1.095)\t Loss 0.065672 (0.065672)\n",
      "\n",
      "Epoch: [209][50/600]\t Time 0.685 (0.699)\t Loss 0.101985 (0.100633)\n",
      "\n",
      "Epoch: [209][100/600]\t Time 0.684 (0.695)\t Loss 0.137445 (0.102017)\n",
      "\n",
      "Epoch: [209][150/600]\t Time 0.683 (0.693)\t Loss 0.088997 (0.102018)\n",
      "\n",
      "Epoch: [209][200/600]\t Time 0.685 (0.693)\t Loss 0.116821 (0.101510)\n",
      "\n",
      "Epoch: [209][250/600]\t Time 0.687 (0.692)\t Loss 0.133044 (0.101479)\n",
      "\n",
      "Epoch: [209][300/600]\t Time 0.682 (0.692)\t Loss 0.122306 (0.101398)\n",
      "\n",
      "Epoch: [209][350/600]\t Time 0.689 (0.692)\t Loss 0.108257 (0.101435)\n",
      "\n",
      "Epoch: [209][400/600]\t Time 0.688 (0.691)\t Loss 0.080574 (0.101758)\n",
      "\n",
      "Epoch: [209][450/600]\t Time 0.684 (0.691)\t Loss 0.079002 (0.101972)\n",
      "\n",
      "Epoch: [209][500/600]\t Time 0.686 (0.691)\t Loss 0.090226 (0.102346)\n",
      "\n",
      "Epoch: [209][550/600]\t Time 0.705 (0.692)\t Loss 0.115844 (0.101955)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.015 (1.015)\t Loss 0.150486 (0.150486)\t Loss_O 0.137849 (0.137849) \t Loss_C 0.147191 (0.147191)\t Loss_E 0.150153 (0.150153)\t Loss_A 0.167543 (0.167543)\t Loss_N 0.149694 (0.149694)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.664 (0.668)\t Loss 0.111223 (0.106766)\t Loss_O 0.127919 (0.104865) \t Loss_C 0.056482 (0.104713)\t Loss_E 0.117620 (0.108221)\t Loss_A 0.113648 (0.100172)\t Loss_N 0.140446 (0.115857)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.658 (0.665)\t Loss 0.077429 (0.103950)\t Loss_O 0.081543 (0.102642) \t Loss_C 0.081832 (0.103730)\t Loss_E 0.062242 (0.105551)\t Loss_A 0.076742 (0.096798)\t Loss_N 0.084787 (0.111031)\n",
      "\n",
      "Epoch: [209]\t Train Loss: 0.102028  \t Val Loss: 0.103695 Loss_O: 0.102131 Loss_C: 0.102999 Loss_E: 0.105719 Loss_A: 0.096668 Loss_N: 0.110957\n",
      "\n",
      "False 0.10299886782964071\n",
      "Epoch: [210][0/600]\t Time 1.054 (1.054)\t Loss 0.096180 (0.096180)\n",
      "\n",
      "Epoch: [210][50/600]\t Time 0.689 (0.701)\t Loss 0.126674 (0.102146)\n",
      "\n",
      "Epoch: [210][100/600]\t Time 0.700 (0.698)\t Loss 0.099394 (0.099866)\n",
      "\n",
      "Epoch: [210][150/600]\t Time 0.689 (0.697)\t Loss 0.088620 (0.100370)\n",
      "\n",
      "Epoch: [210][200/600]\t Time 0.690 (0.695)\t Loss 0.084702 (0.099603)\n",
      "\n",
      "Epoch: [210][250/600]\t Time 0.693 (0.694)\t Loss 0.104104 (0.100199)\n",
      "\n",
      "Epoch: [210][300/600]\t Time 0.695 (0.694)\t Loss 0.084633 (0.100430)\n",
      "\n",
      "Epoch: [210][350/600]\t Time 0.700 (0.694)\t Loss 0.104860 (0.100637)\n",
      "\n",
      "Epoch: [210][400/600]\t Time 0.694 (0.693)\t Loss 0.113001 (0.100907)\n",
      "\n",
      "Epoch: [210][450/600]\t Time 0.694 (0.693)\t Loss 0.112538 (0.101079)\n",
      "\n",
      "Epoch: [210][500/600]\t Time 0.697 (0.694)\t Loss 0.066945 (0.101423)\n",
      "\n",
      "Epoch: [210][550/600]\t Time 0.695 (0.694)\t Loss 0.125652 (0.101173)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.057 (1.057)\t Loss 0.141461 (0.141461)\t Loss_O 0.118051 (0.118051) \t Loss_C 0.134727 (0.134727)\t Loss_E 0.153425 (0.153425)\t Loss_A 0.119320 (0.119320)\t Loss_N 0.181783 (0.181783)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.662 (0.669)\t Loss 0.113818 (0.102231)\t Loss_O 0.123192 (0.099866) \t Loss_C 0.126177 (0.100759)\t Loss_E 0.111253 (0.107196)\t Loss_A 0.058311 (0.093557)\t Loss_N 0.150155 (0.109776)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.657 (0.666)\t Loss 0.103994 (0.101698)\t Loss_O 0.130353 (0.100374) \t Loss_C 0.107223 (0.101281)\t Loss_E 0.116311 (0.103831)\t Loss_A 0.062044 (0.093654)\t Loss_N 0.104040 (0.109352)\n",
      "\n",
      "Epoch: [210]\t Train Loss: 0.101338  \t Val Loss: 0.103443 Loss_O: 0.102013 Loss_C: 0.102404 Loss_E: 0.105528 Loss_A: 0.096575 Loss_N: 0.110694\n",
      "\n",
      "False 0.10299886782964071\n",
      "Epoch: [211][0/600]\t Time 1.098 (1.098)\t Loss 0.072448 (0.072448)\n",
      "\n",
      "Epoch: [211][50/600]\t Time 0.697 (0.702)\t Loss 0.104558 (0.101286)\n",
      "\n",
      "Epoch: [211][100/600]\t Time 0.694 (0.698)\t Loss 0.079890 (0.102564)\n",
      "\n",
      "Epoch: [211][150/600]\t Time 0.698 (0.697)\t Loss 0.132260 (0.103876)\n",
      "\n",
      "Epoch: [211][200/600]\t Time 0.685 (0.696)\t Loss 0.118960 (0.104545)\n",
      "\n",
      "Epoch: [211][250/600]\t Time 0.691 (0.695)\t Loss 0.109398 (0.102890)\n",
      "\n",
      "Epoch: [211][300/600]\t Time 0.692 (0.695)\t Loss 0.109230 (0.103017)\n",
      "\n",
      "Epoch: [211][350/600]\t Time 0.687 (0.695)\t Loss 0.110223 (0.102960)\n",
      "\n",
      "Epoch: [211][400/600]\t Time 0.693 (0.694)\t Loss 0.103409 (0.102185)\n",
      "\n",
      "Epoch: [211][450/600]\t Time 0.693 (0.694)\t Loss 0.086165 (0.102582)\n",
      "\n",
      "Epoch: [211][500/600]\t Time 0.689 (0.694)\t Loss 0.079741 (0.102575)\n",
      "\n",
      "Epoch: [211][550/600]\t Time 0.691 (0.693)\t Loss 0.113445 (0.102299)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.068 (1.068)\t Loss 0.089442 (0.089442)\t Loss_O 0.085161 (0.085161) \t Loss_C 0.103863 (0.103863)\t Loss_E 0.093516 (0.093516)\t Loss_A 0.087985 (0.087985)\t Loss_N 0.076685 (0.076685)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.660 (0.671)\t Loss 0.080521 (0.102549)\t Loss_O 0.092939 (0.104597) \t Loss_C 0.050364 (0.100748)\t Loss_E 0.063232 (0.103679)\t Loss_A 0.102668 (0.093832)\t Loss_N 0.093401 (0.109888)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.660 (0.666)\t Loss 0.119233 (0.102025)\t Loss_O 0.111171 (0.102200) \t Loss_C 0.098267 (0.099660)\t Loss_E 0.122413 (0.104036)\t Loss_A 0.151281 (0.096301)\t Loss_N 0.113032 (0.107928)\n",
      "\n",
      "Epoch: [211]\t Train Loss: 0.102261  \t Val Loss: 0.103245 Loss_O: 0.101934 Loss_C: 0.102343 Loss_E: 0.104922 Loss_A: 0.096687 Loss_N: 0.110337\n",
      "\n",
      "False 0.10299886782964071\n",
      "Epoch: [212][0/600]\t Time 1.102 (1.102)\t Loss 0.073352 (0.073352)\n",
      "\n",
      "Epoch: [212][50/600]\t Time 0.692 (0.699)\t Loss 0.083743 (0.101982)\n",
      "\n",
      "Epoch: [212][100/600]\t Time 0.690 (0.695)\t Loss 0.061566 (0.100045)\n",
      "\n",
      "Epoch: [212][150/600]\t Time 0.691 (0.693)\t Loss 0.107777 (0.099745)\n",
      "\n",
      "Epoch: [212][200/600]\t Time 0.686 (0.692)\t Loss 0.080968 (0.100220)\n",
      "\n",
      "Epoch: [212][250/600]\t Time 0.691 (0.692)\t Loss 0.125835 (0.101133)\n",
      "\n",
      "Epoch: [212][300/600]\t Time 0.691 (0.692)\t Loss 0.082597 (0.101722)\n",
      "\n",
      "Epoch: [212][350/600]\t Time 0.689 (0.692)\t Loss 0.108894 (0.101972)\n",
      "\n",
      "Epoch: [212][400/600]\t Time 0.690 (0.692)\t Loss 0.114589 (0.102803)\n",
      "\n",
      "Epoch: [212][450/600]\t Time 0.695 (0.692)\t Loss 0.093517 (0.102770)\n",
      "\n",
      "Epoch: [212][500/600]\t Time 0.694 (0.692)\t Loss 0.096733 (0.102751)\n",
      "\n",
      "Epoch: [212][550/600]\t Time 0.701 (0.693)\t Loss 0.106813 (0.102750)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.045 (1.045)\t Loss 0.096654 (0.096654)\t Loss_O 0.071944 (0.071944) \t Loss_C 0.146589 (0.146589)\t Loss_E 0.086184 (0.086184)\t Loss_A 0.069273 (0.069273)\t Loss_N 0.109282 (0.109282)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.672 (0.673)\t Loss 0.119050 (0.103308)\t Loss_O 0.138270 (0.098873) \t Loss_C 0.099184 (0.105692)\t Loss_E 0.123876 (0.103972)\t Loss_A 0.112152 (0.098227)\t Loss_N 0.121769 (0.109774)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.664 (0.667)\t Loss 0.111969 (0.104994)\t Loss_O 0.096314 (0.102660) \t Loss_C 0.148243 (0.103758)\t Loss_E 0.090002 (0.106425)\t Loss_A 0.108554 (0.099235)\t Loss_N 0.116732 (0.112892)\n",
      "\n",
      "Epoch: [212]\t Train Loss: 0.102787  \t Val Loss: 0.103281 Loss_O: 0.102074 Loss_C: 0.102279 Loss_E: 0.105168 Loss_A: 0.096450 Loss_N: 0.110432\n",
      "\n",
      "False 0.10299886782964071\n",
      "Epoch: [213][0/600]\t Time 1.087 (1.087)\t Loss 0.099396 (0.099396)\n",
      "\n",
      "Epoch: [213][50/600]\t Time 0.690 (0.698)\t Loss 0.099029 (0.101906)\n",
      "\n",
      "Epoch: [213][100/600]\t Time 0.691 (0.694)\t Loss 0.110393 (0.101938)\n",
      "\n",
      "Epoch: [213][150/600]\t Time 0.692 (0.693)\t Loss 0.117635 (0.099821)\n",
      "\n",
      "Epoch: [213][200/600]\t Time 0.702 (0.693)\t Loss 0.078497 (0.099250)\n",
      "\n",
      "Epoch: [213][250/600]\t Time 0.689 (0.693)\t Loss 0.145233 (0.100853)\n",
      "\n",
      "Epoch: [213][300/600]\t Time 0.701 (0.694)\t Loss 0.177337 (0.100800)\n",
      "\n",
      "Epoch: [213][350/600]\t Time 0.693 (0.694)\t Loss 0.127468 (0.101237)\n",
      "\n",
      "Epoch: [213][400/600]\t Time 0.704 (0.694)\t Loss 0.100912 (0.100942)\n",
      "\n",
      "Epoch: [213][450/600]\t Time 0.693 (0.694)\t Loss 0.081398 (0.101353)\n",
      "\n",
      "Epoch: [213][500/600]\t Time 0.694 (0.694)\t Loss 0.126453 (0.101125)\n",
      "\n",
      "Epoch: [213][550/600]\t Time 0.695 (0.694)\t Loss 0.066284 (0.101649)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.058 (1.058)\t Loss 0.084035 (0.084035)\t Loss_O 0.081028 (0.081028) \t Loss_C 0.106767 (0.106767)\t Loss_E 0.090717 (0.090717)\t Loss_A 0.063084 (0.063084)\t Loss_N 0.078579 (0.078579)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.665 (0.672)\t Loss 0.130568 (0.097261)\t Loss_O 0.138911 (0.096132) \t Loss_C 0.131031 (0.096780)\t Loss_E 0.144473 (0.097851)\t Loss_A 0.123235 (0.090471)\t Loss_N 0.115191 (0.105072)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.657 (0.667)\t Loss 0.068689 (0.101435)\t Loss_O 0.065788 (0.099575) \t Loss_C 0.085338 (0.100541)\t Loss_E 0.077900 (0.102670)\t Loss_A 0.044020 (0.094171)\t Loss_N 0.070398 (0.110219)\n",
      "\n",
      "Epoch: [213]\t Train Loss: 0.101499  \t Val Loss: 0.103675 Loss_O: 0.102275 Loss_C: 0.102576 Loss_E: 0.105800 Loss_A: 0.096650 Loss_N: 0.111076\n",
      "\n",
      "False 0.10299886782964071\n",
      "Epoch: [214][0/600]\t Time 1.075 (1.075)\t Loss 0.116339 (0.116339)\n",
      "\n",
      "Epoch: [214][50/600]\t Time 0.699 (0.698)\t Loss 0.111363 (0.102739)\n",
      "\n",
      "Epoch: [214][100/600]\t Time 0.694 (0.694)\t Loss 0.064282 (0.103073)\n",
      "\n",
      "Epoch: [214][150/600]\t Time 0.697 (0.693)\t Loss 0.099146 (0.101884)\n",
      "\n",
      "Epoch: [214][200/600]\t Time 0.695 (0.692)\t Loss 0.103401 (0.101210)\n",
      "\n",
      "Epoch: [214][250/600]\t Time 0.694 (0.692)\t Loss 0.086485 (0.101529)\n",
      "\n",
      "Epoch: [214][300/600]\t Time 0.698 (0.692)\t Loss 0.079883 (0.101865)\n",
      "\n",
      "Epoch: [214][350/600]\t Time 0.698 (0.692)\t Loss 0.064918 (0.101967)\n",
      "\n",
      "Epoch: [214][400/600]\t Time 0.694 (0.691)\t Loss 0.092029 (0.101791)\n",
      "\n",
      "Epoch: [214][450/600]\t Time 0.699 (0.691)\t Loss 0.103758 (0.101649)\n",
      "\n",
      "Epoch: [214][500/600]\t Time 0.698 (0.691)\t Loss 0.118826 (0.101563)\n",
      "\n",
      "Epoch: [214][550/600]\t Time 0.701 (0.691)\t Loss 0.096535 (0.101822)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.061 (1.061)\t Loss 0.082150 (0.082150)\t Loss_O 0.074945 (0.074945) \t Loss_C 0.067449 (0.067449)\t Loss_E 0.079841 (0.079841)\t Loss_A 0.088177 (0.088177)\t Loss_N 0.100339 (0.100339)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.664 (0.671)\t Loss 0.087226 (0.101589)\t Loss_O 0.055224 (0.101973) \t Loss_C 0.082677 (0.098296)\t Loss_E 0.101448 (0.102627)\t Loss_A 0.097203 (0.096227)\t Loss_N 0.099576 (0.108820)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.663 (0.666)\t Loss 0.093535 (0.101450)\t Loss_O 0.075840 (0.100239) \t Loss_C 0.098974 (0.101770)\t Loss_E 0.144140 (0.103064)\t Loss_A 0.079683 (0.094112)\t Loss_N 0.069039 (0.108065)\n",
      "\n",
      "Epoch: [214]\t Train Loss: 0.101917  \t Val Loss: 0.103837 Loss_O: 0.102341 Loss_C: 0.103157 Loss_E: 0.105956 Loss_A: 0.096684 Loss_N: 0.111047\n",
      "\n",
      "False 0.10299886782964071\n",
      "Epoch: [215][0/600]\t Time 1.081 (1.081)\t Loss 0.103053 (0.103053)\n",
      "\n",
      "Epoch: [215][50/600]\t Time 0.693 (0.703)\t Loss 0.103902 (0.106373)\n",
      "\n",
      "Epoch: [215][100/600]\t Time 0.694 (0.699)\t Loss 0.119953 (0.103831)\n",
      "\n",
      "Epoch: [215][150/600]\t Time 0.692 (0.698)\t Loss 0.086483 (0.102617)\n",
      "\n",
      "Epoch: [215][200/600]\t Time 0.690 (0.696)\t Loss 0.104245 (0.101304)\n",
      "\n",
      "Epoch: [215][250/600]\t Time 0.695 (0.696)\t Loss 0.130706 (0.101432)\n",
      "\n",
      "Epoch: [215][300/600]\t Time 0.691 (0.695)\t Loss 0.118234 (0.102173)\n",
      "\n",
      "Epoch: [215][350/600]\t Time 0.690 (0.695)\t Loss 0.119583 (0.102237)\n",
      "\n",
      "Epoch: [215][400/600]\t Time 0.691 (0.694)\t Loss 0.095980 (0.101653)\n",
      "\n",
      "Epoch: [215][450/600]\t Time 0.692 (0.694)\t Loss 0.113806 (0.101349)\n",
      "\n",
      "Epoch: [215][500/600]\t Time 0.690 (0.694)\t Loss 0.145160 (0.101531)\n",
      "\n",
      "Epoch: [215][550/600]\t Time 0.689 (0.693)\t Loss 0.125727 (0.101616)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.008 (1.008)\t Loss 0.091389 (0.091389)\t Loss_O 0.081104 (0.081104) \t Loss_C 0.083787 (0.083787)\t Loss_E 0.092132 (0.092132)\t Loss_A 0.077617 (0.077617)\t Loss_N 0.122307 (0.122307)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.666 (0.674)\t Loss 0.115591 (0.106916)\t Loss_O 0.126646 (0.098359) \t Loss_C 0.061612 (0.107507)\t Loss_E 0.094398 (0.113387)\t Loss_A 0.118114 (0.099603)\t Loss_N 0.177186 (0.115722)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.662 (0.668)\t Loss 0.092441 (0.105499)\t Loss_O 0.087110 (0.101903) \t Loss_C 0.093941 (0.106038)\t Loss_E 0.118681 (0.109146)\t Loss_A 0.078480 (0.098326)\t Loss_N 0.083995 (0.112083)\n",
      "\n",
      "Epoch: [215]\t Train Loss: 0.101833  \t Val Loss: 0.103530 Loss_O: 0.102029 Loss_C: 0.103053 Loss_E: 0.105238 Loss_A: 0.096558 Loss_N: 0.110770\n",
      "\n",
      "False 0.10299886782964071\n",
      "Epoch: [216][0/600]\t Time 1.059 (1.059)\t Loss 0.084236 (0.084236)\n",
      "\n",
      "Epoch: [216][50/600]\t Time 0.694 (0.698)\t Loss 0.090608 (0.100849)\n",
      "\n",
      "Epoch: [216][100/600]\t Time 0.694 (0.694)\t Loss 0.139694 (0.102351)\n",
      "\n",
      "Epoch: [216][150/600]\t Time 0.690 (0.693)\t Loss 0.081657 (0.103554)\n",
      "\n",
      "Epoch: [216][200/600]\t Time 0.690 (0.693)\t Loss 0.102979 (0.101949)\n",
      "\n",
      "Epoch: [216][250/600]\t Time 0.693 (0.693)\t Loss 0.117069 (0.100977)\n",
      "\n",
      "Epoch: [216][300/600]\t Time 0.688 (0.693)\t Loss 0.085830 (0.101485)\n",
      "\n",
      "Epoch: [216][350/600]\t Time 0.686 (0.693)\t Loss 0.075770 (0.101900)\n",
      "\n",
      "Epoch: [216][400/600]\t Time 0.685 (0.692)\t Loss 0.089799 (0.101788)\n",
      "\n",
      "Epoch: [216][450/600]\t Time 0.689 (0.692)\t Loss 0.095433 (0.101786)\n",
      "\n",
      "Epoch: [216][500/600]\t Time 0.686 (0.692)\t Loss 0.072703 (0.101921)\n",
      "\n",
      "Epoch: [216][550/600]\t Time 0.688 (0.692)\t Loss 0.101607 (0.101614)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.062 (1.062)\t Loss 0.085299 (0.085299)\t Loss_O 0.091160 (0.091160) \t Loss_C 0.080798 (0.080798)\t Loss_E 0.072605 (0.072605)\t Loss_A 0.076232 (0.076232)\t Loss_N 0.105702 (0.105702)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.667 (0.676)\t Loss 0.090892 (0.104378)\t Loss_O 0.079081 (0.100103) \t Loss_C 0.072843 (0.104521)\t Loss_E 0.092297 (0.107135)\t Loss_A 0.085173 (0.101914)\t Loss_N 0.125067 (0.108218)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.664 (0.671)\t Loss 0.099479 (0.104332)\t Loss_O 0.100622 (0.103346) \t Loss_C 0.133911 (0.102474)\t Loss_E 0.079034 (0.105955)\t Loss_A 0.097972 (0.100032)\t Loss_N 0.085858 (0.109852)\n",
      "\n",
      "Epoch: [216]\t Train Loss: 0.101673  \t Val Loss: 0.103448 Loss_O: 0.102019 Loss_C: 0.102962 Loss_E: 0.104993 Loss_A: 0.096745 Loss_N: 0.110521\n",
      "\n",
      "False 0.10299886782964071\n",
      "Epoch: [217][0/600]\t Time 1.072 (1.072)\t Loss 0.145163 (0.145163)\n",
      "\n",
      "Epoch: [217][50/600]\t Time 0.691 (0.703)\t Loss 0.126823 (0.101503)\n",
      "\n",
      "Epoch: [217][100/600]\t Time 0.696 (0.699)\t Loss 0.109661 (0.101969)\n",
      "\n",
      "Epoch: [217][150/600]\t Time 0.691 (0.698)\t Loss 0.114365 (0.101672)\n",
      "\n",
      "Epoch: [217][200/600]\t Time 0.698 (0.697)\t Loss 0.116238 (0.101811)\n",
      "\n",
      "Epoch: [217][250/600]\t Time 0.699 (0.696)\t Loss 0.090265 (0.101948)\n",
      "\n",
      "Epoch: [217][300/600]\t Time 0.697 (0.695)\t Loss 0.114518 (0.102232)\n",
      "\n",
      "Epoch: [217][350/600]\t Time 0.692 (0.695)\t Loss 0.091051 (0.102240)\n",
      "\n",
      "Epoch: [217][400/600]\t Time 0.693 (0.695)\t Loss 0.109715 (0.102068)\n",
      "\n",
      "Epoch: [217][450/600]\t Time 0.693 (0.695)\t Loss 0.093314 (0.102157)\n",
      "\n",
      "Epoch: [217][500/600]\t Time 0.695 (0.695)\t Loss 0.117660 (0.101834)\n",
      "\n",
      "Epoch: [217][550/600]\t Time 0.700 (0.695)\t Loss 0.080046 (0.101884)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.013 (1.013)\t Loss 0.113859 (0.113859)\t Loss_O 0.111476 (0.111476) \t Loss_C 0.153974 (0.153974)\t Loss_E 0.103357 (0.103357)\t Loss_A 0.096179 (0.096179)\t Loss_N 0.104312 (0.104312)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.659 (0.668)\t Loss 0.090011 (0.103521)\t Loss_O 0.080866 (0.105576) \t Loss_C 0.070854 (0.103229)\t Loss_E 0.100630 (0.104334)\t Loss_A 0.103240 (0.095431)\t Loss_N 0.094463 (0.109036)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.663 (0.665)\t Loss 0.120446 (0.102823)\t Loss_O 0.124945 (0.101884) \t Loss_C 0.122534 (0.102789)\t Loss_E 0.116481 (0.103709)\t Loss_A 0.077682 (0.094982)\t Loss_N 0.160588 (0.110750)\n",
      "\n",
      "Epoch: [217]\t Train Loss: 0.101891  \t Val Loss: 0.103333 Loss_O: 0.102041 Loss_C: 0.102266 Loss_E: 0.105153 Loss_A: 0.096560 Loss_N: 0.110643\n",
      "\n",
      "False 0.10299886782964071\n",
      "Epoch: [218][0/600]\t Time 1.101 (1.101)\t Loss 0.093153 (0.093153)\n",
      "\n",
      "Epoch: [218][50/600]\t Time 0.691 (0.703)\t Loss 0.110060 (0.101649)\n",
      "\n",
      "Epoch: [218][100/600]\t Time 0.702 (0.699)\t Loss 0.095001 (0.102194)\n",
      "\n",
      "Epoch: [218][150/600]\t Time 0.693 (0.698)\t Loss 0.083039 (0.103847)\n",
      "\n",
      "Epoch: [218][200/600]\t Time 0.699 (0.697)\t Loss 0.067396 (0.102079)\n",
      "\n",
      "Epoch: [218][250/600]\t Time 0.690 (0.696)\t Loss 0.109393 (0.103111)\n",
      "\n",
      "Epoch: [218][300/600]\t Time 0.694 (0.696)\t Loss 0.092049 (0.102991)\n",
      "\n",
      "Epoch: [218][350/600]\t Time 0.706 (0.696)\t Loss 0.102868 (0.103057)\n",
      "\n",
      "Epoch: [218][400/600]\t Time 0.701 (0.696)\t Loss 0.132911 (0.103228)\n",
      "\n",
      "Epoch: [218][450/600]\t Time 0.691 (0.695)\t Loss 0.092499 (0.102783)\n",
      "\n",
      "Epoch: [218][500/600]\t Time 0.691 (0.695)\t Loss 0.112784 (0.102572)\n",
      "\n",
      "Epoch: [218][550/600]\t Time 0.691 (0.694)\t Loss 0.084584 (0.102449)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.050 (1.050)\t Loss 0.110133 (0.110133)\t Loss_O 0.098923 (0.098923) \t Loss_C 0.097689 (0.097689)\t Loss_E 0.111214 (0.111214)\t Loss_A 0.120826 (0.120826)\t Loss_N 0.122014 (0.122014)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.657 (0.670)\t Loss 0.073545 (0.102391)\t Loss_O 0.098596 (0.099967) \t Loss_C 0.052060 (0.103491)\t Loss_E 0.091484 (0.101062)\t Loss_A 0.054902 (0.096270)\t Loss_N 0.070681 (0.111165)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.661 (0.666)\t Loss 0.105660 (0.104395)\t Loss_O 0.110645 (0.103795) \t Loss_C 0.073283 (0.104245)\t Loss_E 0.113743 (0.105182)\t Loss_A 0.070128 (0.097289)\t Loss_N 0.160503 (0.111461)\n",
      "\n",
      "Epoch: [218]\t Train Loss: 0.102124  \t Val Loss: 0.103472 Loss_O: 0.102123 Loss_C: 0.102757 Loss_E: 0.105383 Loss_A: 0.096550 Loss_N: 0.110547\n",
      "\n",
      "False 0.10299886782964071\n",
      "Epoch: [219][0/600]\t Time 1.086 (1.086)\t Loss 0.126207 (0.126207)\n",
      "\n",
      "Epoch: [219][50/600]\t Time 0.688 (0.701)\t Loss 0.102307 (0.102136)\n",
      "\n",
      "Epoch: [219][100/600]\t Time 0.689 (0.698)\t Loss 0.115661 (0.103467)\n",
      "\n",
      "Epoch: [219][150/600]\t Time 0.699 (0.697)\t Loss 0.087503 (0.102076)\n",
      "\n",
      "Epoch: [219][200/600]\t Time 0.694 (0.697)\t Loss 0.094550 (0.102865)\n",
      "\n",
      "Epoch: [219][250/600]\t Time 0.696 (0.696)\t Loss 0.116245 (0.103736)\n",
      "\n",
      "Epoch: [219][300/600]\t Time 0.696 (0.696)\t Loss 0.118780 (0.103813)\n",
      "\n",
      "Epoch: [219][350/600]\t Time 0.700 (0.696)\t Loss 0.098303 (0.103358)\n",
      "\n",
      "Epoch: [219][400/600]\t Time 0.698 (0.696)\t Loss 0.124120 (0.103213)\n",
      "\n",
      "Epoch: [219][450/600]\t Time 0.696 (0.696)\t Loss 0.103302 (0.103039)\n",
      "\n",
      "Epoch: [219][500/600]\t Time 0.695 (0.696)\t Loss 0.085143 (0.102742)\n",
      "\n",
      "Epoch: [219][550/600]\t Time 0.694 (0.696)\t Loss 0.078310 (0.102586)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.035 (1.035)\t Loss 0.102861 (0.102861)\t Loss_O 0.085942 (0.085942) \t Loss_C 0.092958 (0.092958)\t Loss_E 0.132172 (0.132172)\t Loss_A 0.095762 (0.095762)\t Loss_N 0.107474 (0.107474)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.662 (0.673)\t Loss 0.084405 (0.099914)\t Loss_O 0.071114 (0.099370) \t Loss_C 0.061768 (0.095611)\t Loss_E 0.070232 (0.106453)\t Loss_A 0.103345 (0.089970)\t Loss_N 0.115565 (0.108166)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.668 (0.669)\t Loss 0.114130 (0.104028)\t Loss_O 0.103166 (0.102105) \t Loss_C 0.129674 (0.104067)\t Loss_E 0.110064 (0.105915)\t Loss_A 0.100229 (0.095786)\t Loss_N 0.127517 (0.112268)\n",
      "\n",
      "Epoch: [219]\t Train Loss: 0.102139  \t Val Loss: 0.103398 Loss_O: 0.102119 Loss_C: 0.102402 Loss_E: 0.105137 Loss_A: 0.096746 Loss_N: 0.110588\n",
      "\n",
      "False 0.10299886782964071\n",
      "Epoch: [220][0/600]\t Time 1.075 (1.075)\t Loss 0.125601 (0.125601)\n",
      "\n",
      "Epoch: [220][50/600]\t Time 0.682 (0.700)\t Loss 0.085045 (0.100749)\n",
      "\n",
      "Epoch: [220][100/600]\t Time 0.694 (0.697)\t Loss 0.084389 (0.099788)\n",
      "\n",
      "Epoch: [220][150/600]\t Time 0.701 (0.696)\t Loss 0.101735 (0.098909)\n",
      "\n",
      "Epoch: [220][200/600]\t Time 0.697 (0.696)\t Loss 0.104195 (0.100291)\n",
      "\n",
      "Epoch: [220][250/600]\t Time 0.699 (0.696)\t Loss 0.134965 (0.101631)\n",
      "\n",
      "Epoch: [220][300/600]\t Time 0.694 (0.696)\t Loss 0.132596 (0.101403)\n",
      "\n",
      "Epoch: [220][350/600]\t Time 0.695 (0.696)\t Loss 0.089132 (0.101324)\n",
      "\n",
      "Epoch: [220][400/600]\t Time 0.689 (0.696)\t Loss 0.109161 (0.101088)\n",
      "\n",
      "Epoch: [220][450/600]\t Time 0.685 (0.695)\t Loss 0.089942 (0.100964)\n",
      "\n",
      "Epoch: [220][500/600]\t Time 0.689 (0.695)\t Loss 0.101567 (0.101161)\n",
      "\n",
      "Epoch: [220][550/600]\t Time 0.698 (0.694)\t Loss 0.097152 (0.101297)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.021 (1.021)\t Loss 0.116198 (0.116198)\t Loss_O 0.081708 (0.081708) \t Loss_C 0.080344 (0.080344)\t Loss_E 0.135356 (0.135356)\t Loss_A 0.150193 (0.150193)\t Loss_N 0.133389 (0.133389)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.660 (0.669)\t Loss 0.095996 (0.103980)\t Loss_O 0.060086 (0.100141) \t Loss_C 0.124560 (0.103944)\t Loss_E 0.100508 (0.106330)\t Loss_A 0.077620 (0.095866)\t Loss_N 0.117206 (0.113620)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.661 (0.665)\t Loss 0.087960 (0.105648)\t Loss_O 0.088274 (0.103193) \t Loss_C 0.108981 (0.105351)\t Loss_E 0.059689 (0.108250)\t Loss_A 0.072970 (0.098548)\t Loss_N 0.109885 (0.112899)\n",
      "\n",
      "Epoch: [220]\t Train Loss: 0.101391  \t Val Loss: 0.103862 Loss_O: 0.102294 Loss_C: 0.103372 Loss_E: 0.105623 Loss_A: 0.096962 Loss_N: 0.111060\n",
      "\n",
      "False 0.10299886782964071\n",
      "Epoch: [221][0/600]\t Time 1.081 (1.081)\t Loss 0.080362 (0.080362)\n",
      "\n",
      "Epoch: [221][50/600]\t Time 0.689 (0.699)\t Loss 0.113865 (0.106316)\n",
      "\n",
      "Epoch: [221][100/600]\t Time 0.688 (0.695)\t Loss 0.115979 (0.105428)\n",
      "\n",
      "Epoch: [221][150/600]\t Time 0.690 (0.693)\t Loss 0.105007 (0.104870)\n",
      "\n",
      "Epoch: [221][200/600]\t Time 0.686 (0.693)\t Loss 0.090640 (0.104090)\n",
      "\n",
      "Epoch: [221][250/600]\t Time 0.688 (0.692)\t Loss 0.071946 (0.103244)\n",
      "\n",
      "Epoch: [221][300/600]\t Time 0.696 (0.692)\t Loss 0.103994 (0.102538)\n",
      "\n",
      "Epoch: [221][350/600]\t Time 0.691 (0.693)\t Loss 0.079358 (0.102308)\n",
      "\n",
      "Epoch: [221][400/600]\t Time 0.692 (0.693)\t Loss 0.102341 (0.101885)\n",
      "\n",
      "Epoch: [221][450/600]\t Time 0.691 (0.692)\t Loss 0.100212 (0.102002)\n",
      "\n",
      "Epoch: [221][500/600]\t Time 0.693 (0.692)\t Loss 0.133211 (0.102551)\n",
      "\n",
      "Epoch: [221][550/600]\t Time 0.692 (0.692)\t Loss 0.107216 (0.102268)\n",
      "\n",
      "Val/Test: [0/150]\t Time 1.049 (1.049)\t Loss 0.116979 (0.116979)\t Loss_O 0.113953 (0.113953) \t Loss_C 0.111252 (0.111252)\t Loss_E 0.151901 (0.151901)\t Loss_A 0.103383 (0.103383)\t Loss_N 0.104408 (0.104408)\n",
      "\n",
      "Val/Test: [50/150]\t Time 0.663 (0.669)\t Loss 0.149734 (0.104186)\t Loss_O 0.138201 (0.104343) \t Loss_C 0.146332 (0.102112)\t Loss_E 0.172671 (0.104685)\t Loss_A 0.125426 (0.098826)\t Loss_N 0.166039 (0.110962)\n",
      "\n",
      "Val/Test: [100/150]\t Time 0.662 (0.666)\t Loss 0.086495 (0.102989)\t Loss_O 0.075366 (0.103377) \t Loss_C 0.098455 (0.099577)\t Loss_E 0.090999 (0.103954)\t Loss_A 0.064596 (0.095978)\t Loss_N 0.103058 (0.112060)\n",
      "\n",
      "Epoch: [221]\t Train Loss: 0.101946  \t Val Loss: 0.103115 Loss_O: 0.102221 Loss_C: 0.101288 Loss_E: 0.105227 Loss_A: 0.096359 Loss_N: 0.110480\n",
      "\n",
      "False 0.10299886782964071\n",
      "Epoch: [222][0/600]\t Time 1.080 (1.080)\t Loss 0.110258 (0.110258)\n",
      "\n",
      "Epoch: [222][50/600]\t Time 0.698 (0.700)\t Loss 0.064149 (0.100986)\n",
      "\n",
      "Epoch: [222][100/600]\t Time 0.698 (0.695)\t Loss 0.115925 (0.100432)\n",
      "\n",
      "Epoch: [222][150/600]\t Time 0.694 (0.694)\t Loss 0.154462 (0.101265)\n",
      "\n",
      "Epoch: [222][200/600]\t Time 0.694 (0.694)\t Loss 0.122012 (0.101878)\n",
      "\n",
      "Epoch: [222][250/600]\t Time 0.697 (0.695)\t Loss 0.097328 (0.102409)\n",
      "\n",
      "Epoch: [222][300/600]\t Time 0.699 (0.695)\t Loss 0.120592 (0.102064)\n",
      "\n",
      "Epoch: [222][350/600]\t Time 0.690 (0.695)\t Loss 0.108835 (0.102683)\n"
     ]
    }
   ],
   "source": [
    "startModel='./resnet_for_rnn/ResNet_LSTM_L1/checkpoint.pth.tar'\n",
    "train_model(startModel=startModel, startEpoch=1 , numEpochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# visualize the pred and groundtruth and images of the latest val batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "val_pred = (val_preds.data).cpu().numpy()\n",
    "val_gt = (Variable(val_targets).data).cpu().numpy()\n",
    "# select a video in batch to see: idx can be 0-7\n",
    "idx = 7\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(np.arange(5),val_gt[idx])\n",
    "plt.xticks(np.arange(5),['e','n','a','c','o'])\n",
    "plt.gca().set_ylim([0.0,1.0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(np.arange(5),val_pred[idx])\n",
    "plt.xticks(np.arange(5),['e','n','a','c','o'])\n",
    "plt.gca().set_ylim([0.0,1.0])\n",
    "plt.show()\n",
    "val_unflattened_sample = val_imgs.view(-1,3,256,256)\n",
    "# Make a grid from batch\n",
    "plt.figure( figsize=(30, 3))\n",
    "out = torchvision.utils.make_grid(val_unflattened_sample[idx*10:idx*10+10],nrow=10)\n",
    "imshow(out, title='trainning sample')\n",
    "#plt.savefig('train_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

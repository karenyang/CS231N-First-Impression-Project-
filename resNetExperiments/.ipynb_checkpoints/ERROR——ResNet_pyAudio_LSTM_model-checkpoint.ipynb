{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_dir = '/home/noa_glaser/dataBig/train-1-2-4/train'\n",
    "val_dataset_dir = '/home/noa_glaser/dataBig/train-1-2-4/val'\n",
    "audio_dataset_dir = '/home/noa_glaser/dataBig/train-1-2-4-audio'\n",
    "label_dataset_dir = train_dataset_dir\n",
    "exp_name = 'ResNet34_LSTM_experiment' # roughly 3K videos\n",
    "num_classes = 5 \n",
    "num_partition = 10\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import time\n",
    "import copy\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first, get the Images (path) and their labels (five personality traits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_img_audio_label(dataset_dir,audio_dataset_dir,label_dataset_dir):\n",
    "    \"\"\"Returns a list of np.array(img_paths), np.array(audio_paths),np.array(labels), np.array(raw_movienames)\n",
    "    Args:\n",
    "    dataset for video, for audio_feats from pyAudioAnalysis, labels\n",
    "    \"\"\"\n",
    " \n",
    "    print(\"processing dataset: \"+ dataset_dir)\n",
    "    img_paths = [] \n",
    "    audio_paths=[]\n",
    "    raw_movienames = []\n",
    "    labels = []\n",
    "\n",
    "    annotaion_filename = label_dataset_dir + \"/annotation_training.pkl\"\n",
    "    \n",
    "    with open(annotaion_filename, 'rb') as f:\n",
    "        label_dicts = pickle.load(f, encoding='latin1') \n",
    "\n",
    "    for movie in os.listdir(dataset_dir):\n",
    "        fileEnding ='_50uniform' #TODO: figure out how to make more general\n",
    "        if fileEnding not in movie: continue #skip non-movie files\n",
    "        raw_moviename = movie.replace(fileEnding,'.mp4')\n",
    "        raw_movienames.append(raw_moviename)\n",
    "        \n",
    "        big_five = [label_dicts['extraversion'][raw_moviename], \n",
    "                    label_dicts['neuroticism'][raw_moviename],\n",
    "                    label_dicts['agreeableness'][raw_moviename],\n",
    "                    label_dicts['conscientiousness'][raw_moviename],\n",
    "                    label_dicts['openness'][raw_moviename] ]\n",
    "                    #label_dicts['interview'][raw_moviename]]\n",
    "        labels.append(big_five)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        movie_path = os.path.join(dataset_dir, movie)\n",
    "        mv_partitions = []\n",
    "        p = 0\n",
    "        all_imgs = os.listdir(movie_path)\n",
    "        assert(len(all_imgs) >= num_partition)\n",
    "        for i in range(num_partition):\n",
    "            path = os.path.join(movie_path, all_imgs[i])\n",
    "            try:\n",
    "                open(path)\n",
    "            except:\n",
    "                print('image failed to open',path)\n",
    "            mv_partitions.append(path)\n",
    "        assert(len(mv_partitions)==num_partition)\n",
    "        img_paths.append(mv_partitions)\n",
    "        \n",
    "        audiofeat_path = os.path.join(audio_dataset_dir,raw_moviename+'.wav.csv')\n",
    "        try:\n",
    "            open(audiofeat_path)\n",
    "        except:\n",
    "            print('image failed to open',path)\n",
    "        audio_paths.append(audiofeat_path)\n",
    "            \n",
    "    \n",
    "    return np.array(img_paths), np.array(audio_paths),np.array(labels), np.array(raw_movienames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use this if we need to split our dataset into train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all_img_names, all_labels, all_movies = _get_imgname_and_moviename_and_labels( dataset_dir)  \n",
    "#validation_size = 0.3\n",
    "#num_validation = int(validation_size * len(all_img_names))\n",
    "\n",
    "#all_img_names = np.array(all_img_names)\n",
    "#all_labels = np.array(all_labels)\n",
    "#all_movies = np.array(all_movies)\n",
    "#training_filenames = all_img_names[num_validation:]\n",
    "#validation_filenames = all_img_names[:num_validation]\n",
    "#train_labels = all_labels[num_validation:]\n",
    "#val_labels = all_labels[:num_validation]\n",
    "#train_movies = all_movies[num_validation:]\n",
    "#val_movies = all_movies[:num_validation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## use this if we have seperated train/val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dataset: /home/noa_glaser/dataBig/train-1-2-4/train\n",
      "processing dataset: /home/noa_glaser/dataBig/train-1-2-4/val\n"
     ]
    }
   ],
   "source": [
    "train_img_paths,train_audio_paths, train_labels, train_movienames = get_img_audio_label(train_dataset_dir,audio_dataset_dir,label_dataset_dir)  \n",
    "val_img_paths,val_audio_paths, val_labels, val_movienames = get_img_audio_label(val_dataset_dir,audio_dataset_dir,label_dataset_dir)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Loader. Data is normalized before feeding into model (as required by the pretrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def default_img_loader(img_paths,transform):\n",
    "    ten_img_tensor = []\n",
    "    for path in img_paths:\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if transform is not None:\n",
    "            img = transform(img)\n",
    "        ten_img_tensor.append(img)\n",
    "        \n",
    "    return torch.cat(ten_img_tensor)\n",
    "        \n",
    "\n",
    "def default_audio_loader(path):\n",
    "\treturn np.loadtxt(path,delimiter=',')\n",
    "\n",
    "class VisualAudio(data.Dataset):\n",
    "    def __init__(self,split,img_paths,audio_paths, movie_names,labels,transform=None,\n",
    "                 img_loader=default_img_loader,audio_loader=default_audio_loader):\n",
    "        if len(img_paths) != len(audio_paths) or len(audio_paths) == 0 or len(labels)!=len(audio_paths) :\n",
    "            raise(RuntimeError(\"something is wrong with the data directory\"))\n",
    "                  \n",
    "        self.split = split \n",
    "        self.img_paths = img_paths\n",
    "        self.audio_paths = audio_paths\n",
    "        self.movie_names = movie_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.img_loader=img_loader\n",
    "        self.audio_loader= audio_loader\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_paths, audio_paths,target = self.img_paths[index], self.audio_paths[index], self.labels[index]\n",
    "        ten_img_tensor = self.img_loader(img_paths,self.transform)\n",
    "        ten_audio = self.audio_loader(audio_paths)\n",
    "        #return 30x224x224 , 10x68, 10 x 5\n",
    "        \n",
    "        assert(ten_img_tensor.size() == (30,256,256))\n",
    "        \n",
    "        return ten_img_tensor, ten_audio[:10,:], target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 2304, 'val': 576}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import  transforms\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.RandomCrop(256),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "        \n",
    "dsets = {}\n",
    "dsets['train'] = VisualAudio('train',train_img_paths,train_audio_paths,  train_movienames ,train_labels,transform=data_transforms['train'] )\n",
    "dsets['val'] = VisualAudio('val',val_img_paths,val_audio_paths,val_movienames,val_labels,transform=data_transforms['val'] )\n",
    "\n",
    "dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=batch_size,\n",
    "                                               shuffle=True, num_workers=1) for x in ['train', 'val']}\n",
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "dset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Some dataset examples (each batch is 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    \n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "#train_imgsamples,train_audiosamle,train_labelsample = next(iter(dset_loaders['train']))\n",
    "\n",
    "\n",
    "#train_unflattened_sample = train_imgsamples.view(-1,3,256,256)\n",
    "# Make a grid from batch\n",
    "#plt.figure( figsize=(10, 16))\n",
    "#out = torchvision.utils.make_grid(train_unflattened_sample,nrow=10)\n",
    "#imshow(out, title='trainning sample')\n",
    "#plt.savefig('train_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#val_imgsamples,val_audiosamle,val_labelsample = next(iter(dset_loaders['val']))\n",
    "#val_unflattened_sample = val_imgsamples.view(-1,3,256,256)\n",
    "### Make a grid from val batch\n",
    "#plt.figure( figsize=(10, 16))\n",
    "#out2 = torchvision.utils.make_grid(val_unflattened_sample,nrow=10)\n",
    "#imshow(out2, title='validation sample')\n",
    "#plt.savefig('val_exp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some gpu configs\n",
    "use_gpu = True\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"Saves checkpoint to disk\"\"\"\n",
    "    directory = \"resnet_for_rnn/%s/\"%(exp_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filename = directory + filename\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'resnet_for_rnn/%s/'%(exp_name) + 'model_best.pth.tar')\n",
    "\n",
    "def log_value(to_log, log_path = './log_'+ exp_name + '.txt'):\n",
    "    log_file = open(log_path, 'a+')\n",
    "    log_file.write(to_log)\n",
    "    log_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_model(startModel=None, startEpoch=0, numEpochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_learnable_params(m,verbose = 0):\n",
    "    ret = []\n",
    "    for l in m.parameters():\n",
    "        if l.requires_grad == True:\n",
    "            ret.append(l)\n",
    "            if verbose == 1:\n",
    "                print (l.size())\n",
    "            if verbose == 2:\n",
    "                print (l)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#p = get_learnable_params(model_base,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify only the last fc layers params are being optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_freq = 10\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch) :\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        input_image,input_audio, target = data\n",
    "        input_image_var, input_audio_var,target_var = Variable(input_image.type(gpu_dtype)), \\\n",
    "            Variable(input_audio.type(gpu_dtype)),Variable(target.type(gpu_dtype))\n",
    "        # compute output\n",
    "        output = model([input_image_var,input_audio_var])\n",
    "        loss = criterion(output, target_var)\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data[0], input_image.size(0))\n",
    "        # compute gradient and do Adam step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i % log_freq == 0:\n",
    "            to_log = 'Epoch: [{0}][{1}/{2}]\\t Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t Loss {loss.val:f} ({loss.avg:f})\\n'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time, loss=losses)\n",
    "            log_value(to_log)\n",
    "            print(to_log)\n",
    "            \n",
    "       \n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    \"\"\"Perform validation on the validation set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, data in enumerate(val_loader):\n",
    "        input_image,input_audio, target = data\n",
    "        input_image_var, input_audio_var,target_var = Variable(input_image.type(gpu_dtype)), \\\n",
    "        Variable(input_audio.type(gpu_dtype)),Variable(target.type(gpu_dtype))\n",
    "        # compute output\n",
    "        output = model([input_image_var,input_audio_var])\n",
    "        loss = criterion(output, target_var)\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data[0], input_image.size(0))\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i %  log_freq == 0:\n",
    "            to_log = 'Val/Test: [{0}/{1}]\\t Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t Loss {loss.val:f} ({loss.avg:f})\\n'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses)\n",
    "            log_value(to_log)\n",
    "            #print(to_log)\n",
    "                        \n",
    "    return losses.avg\n",
    "\n",
    "def train_model(startModel=None, startEpoch=0, numEpochs=10):    \n",
    "    model = AudioVisualLSTM().type(gpu_dtype)\n",
    "    #del model_base\n",
    "\n",
    "    #  changed to l1 loss to reflect competition \n",
    "    criterion = nn.L1Loss().type(gpu_dtype)\n",
    "\n",
    "    #only optimizing the new_fc layer parameters, other pretrained weights are freezed¶\n",
    "    optimizer  = optim.SGD(get_learnable_params(model),lr=5e-2, momentum=0.9,weight_decay=5e-4)\n",
    "\n",
    "    best_loss = 1000 # will get overwritten\n",
    "    \n",
    "    if(startModel != None):\n",
    "        print(\"=> loading checkpoint '{}'\".format(startModel))\n",
    "        checkpoint = torch.load(startModel)\n",
    "        startEpoch = checkpoint['epoch']\n",
    "        best_loss = checkpoint['best_loss'] # for now because old loss is stale\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        # todo - figure out why not working\n",
    "        #optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        # print(optimizer.param_groups)\n",
    "        \n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(startModel, checkpoint['epoch']))\n",
    "    #else:\n",
    "        # benchmark the model \n",
    "        #best_loss = validate(dset_loaders['val'], model, criterion, startEpoch)\n",
    "        \n",
    "    bestModel = model\n",
    "\n",
    "    for epoch in range(startEpoch,startEpoch+numEpochs):\n",
    "        # train for one epoch\n",
    "        train_loss = train(dset_loaders['train'], model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        val_loss = validate(dset_loaders['val'], model, criterion, epoch)\n",
    "\n",
    "        # log \n",
    "        log_value('Epoch: [{0}]\\t Train Loss: {train_loss:f}  \\t Val Loss: {val_loss:f}\\n'.format(epoch,train_loss=train_loss,val_loss=val_loss) ,'./epoch_log.txt')\n",
    "        print('Epoch: [{0}]\\t Train Loss: {train_loss:f}  \\t Val Loss: {val_loss:f}\\n'.format(epoch,train_loss=train_loss,val_loss=val_loss) ,'./epoch_log.txt')\n",
    "\n",
    "        # remember best loss and save checkpoint\n",
    "        is_best = val_loss <= best_loss\n",
    "        best_loss = min(val_loss, best_loss)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': 'resnet34_lstm',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print (is_best, best_loss)\n",
    "\n",
    "    print ('Best Loss: ', best_loss)\n",
    "\n",
    "# TODO: for larger dataset, consider a step function or exponentialdecay\n",
    "def lr_scheduler(optimizer, epoch):\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AudioVisualLSTM(nn.Module):\n",
    "    NUM_VID_FEATURES = 128\n",
    "    NUM_AUDIO_FEATURES = 32\n",
    "    NUM_LSTM_HIDDEN = 128\n",
    "    NUM_PARTITIONS = 10\n",
    "    NUM_CLASS = 5\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self):        \n",
    "        super(AudioVisualLSTM, self).__init__()\n",
    "        self.audioBranch =  nn.Sequential(nn.Linear(68,32))\n",
    "        self.videoBranch = self._createVideoBranch()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=(self.NUM_VID_FEATURES+self.NUM_AUDIO_FEATURES),\n",
    "            hidden_size=self.NUM_LSTM_HIDDEN,\n",
    "            num_layers=1,\n",
    "            bias=True,\n",
    "            batch_first=True # input and output tensors provided as (batch, seq, feature)\n",
    "            # can add dropout later\n",
    "            )\n",
    "        self.fc = nn.Linear(self.NUM_LSTM_HIDDEN,self.NUM_CLASS)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def _createVideoBranch(self):\n",
    "        #return nn.Linear(256,1024)\n",
    "        model_pretrained = torchvision.models.resnet34(pretrained=True)\n",
    "        # All of the parameters are freezed, not to change (newly constructed layers' params won't be influenced)\n",
    "        for param in model_pretrained.parameters():\n",
    "            param.requires_grad = False   \n",
    "        model_pretrained.fc = nn.Linear(model_pretrained.fc.in_features, self.NUM_VID_FEATURES)\n",
    "        return model_pretrained\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N=8\n",
    "        P=10\n",
    "        videoData = x[0].view(N*P,3,256,256)\n",
    "        audioData = x[1].view(N*P,68)\n",
    "        videoProcessed = self.videoBranch(videoData).view(N,P,self.NUM_VID_FEATURES) # will output a (n x partitions)x 32 tensor\n",
    "        audioProcessed = self.audioBranch(audioData).view(N,P,self.NUM_AUDIO_FEATURES)# will output a (n x partitions)x 128 tensor\n",
    "        x = torch.cat((videoProcessed, audioProcessed), 2).type(gpu_dtype) #(N,10,160)\n",
    "        h0 = Variable(torch.zeros(1, N, self.NUM_LSTM_HIDDEN)).type(gpu_dtype)\n",
    "        c0 = Variable(torch.zeros(1, N, self.NUM_LSTM_HIDDEN)).type(gpu_dtype)\n",
    "        x,cn = self.lstm(x, (h0, c0))\n",
    "        x = x.contiguous().view(-1,128)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x) #(N*P,5)\n",
    "        x = torch.mean(x.view(N,P,5),1).squeeze() #(N,5)    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train output Criterian (L1 loss) and Optimizer (Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/288]\t Time 2.138 (2.138)\t Loss 0.123904 (0.123904)\n",
      "\n",
      "Epoch: [0][10/288]\t Time 0.648 (0.788)\t Loss 0.159804 (0.121154)\n",
      "\n",
      "Epoch: [0][20/288]\t Time 0.651 (0.721)\t Loss 0.111943 (0.123177)\n",
      "\n",
      "Epoch: [0][30/288]\t Time 0.642 (0.696)\t Loss 0.141495 (0.122214)\n",
      "\n",
      "Epoch: [0][40/288]\t Time 0.640 (0.684)\t Loss 0.160162 (0.122436)\n",
      "\n",
      "Epoch: [0][50/288]\t Time 0.650 (0.677)\t Loss 0.126707 (0.123618)\n",
      "\n",
      "Epoch: [0][60/288]\t Time 0.649 (0.672)\t Loss 0.108017 (0.122246)\n",
      "\n",
      "Epoch: [0][70/288]\t Time 0.651 (0.670)\t Loss 0.136600 (0.121374)\n",
      "\n",
      "Epoch: [0][80/288]\t Time 0.655 (0.668)\t Loss 0.106340 (0.121882)\n",
      "\n",
      "Epoch: [0][90/288]\t Time 0.655 (0.667)\t Loss 0.150106 (0.122254)\n",
      "\n",
      "Epoch: [0][100/288]\t Time 0.651 (0.666)\t Loss 0.117595 (0.121901)\n",
      "\n",
      "Epoch: [0][110/288]\t Time 0.653 (0.665)\t Loss 0.114371 (0.120829)\n",
      "\n",
      "Epoch: [0][120/288]\t Time 0.658 (0.664)\t Loss 0.102844 (0.121347)\n",
      "\n",
      "Epoch: [0][130/288]\t Time 0.657 (0.664)\t Loss 0.089621 (0.121285)\n",
      "\n",
      "Epoch: [0][140/288]\t Time 0.660 (0.664)\t Loss 0.091638 (0.120564)\n",
      "\n",
      "Epoch: [0][150/288]\t Time 0.654 (0.664)\t Loss 0.092596 (0.120527)\n",
      "\n",
      "Epoch: [0][160/288]\t Time 0.655 (0.663)\t Loss 0.095703 (0.119889)\n",
      "\n",
      "Epoch: [0][170/288]\t Time 0.679 (0.663)\t Loss 0.118640 (0.119816)\n",
      "\n",
      "Epoch: [0][180/288]\t Time 0.662 (0.664)\t Loss 0.159963 (0.119787)\n",
      "\n",
      "Epoch: [0][190/288]\t Time 0.660 (0.663)\t Loss 0.142542 (0.119959)\n",
      "\n",
      "Epoch: [0][200/288]\t Time 0.660 (0.663)\t Loss 0.128272 (0.119981)\n",
      "\n",
      "Epoch: [0][210/288]\t Time 0.665 (0.663)\t Loss 0.118438 (0.119564)\n",
      "\n",
      "Epoch: [0][220/288]\t Time 0.687 (0.664)\t Loss 0.103635 (0.119397)\n",
      "\n",
      "Epoch: [0][230/288]\t Time 0.667 (0.663)\t Loss 0.095271 (0.119224)\n",
      "\n",
      "Epoch: [0][240/288]\t Time 0.660 (0.663)\t Loss 0.105437 (0.119486)\n",
      "\n",
      "Epoch: [0][250/288]\t Time 0.664 (0.663)\t Loss 0.094868 (0.119856)\n",
      "\n",
      "Epoch: [0][260/288]\t Time 0.660 (0.663)\t Loss 0.126365 (0.119540)\n",
      "\n",
      "Epoch: [0][270/288]\t Time 0.665 (0.663)\t Loss 0.107998 (0.119346)\n",
      "\n",
      "Epoch: [0][280/288]\t Time 0.659 (0.663)\t Loss 0.139945 (0.119407)\n",
      "\n",
      "Epoch: [0]\t Train Loss: 0.119310  \t Val Loss: 0.111710\n",
      " ./epoch_log.txt\n",
      "True 0.1117104615809189\n",
      "Epoch: [1][0/288]\t Time 1.018 (1.018)\t Loss 0.146357 (0.146357)\n",
      "\n",
      "Epoch: [1][10/288]\t Time 0.662 (0.695)\t Loss 0.137446 (0.118186)\n",
      "\n",
      "Epoch: [1][20/288]\t Time 0.655 (0.680)\t Loss 0.087585 (0.113135)\n",
      "\n",
      "Epoch: [1][30/288]\t Time 0.662 (0.673)\t Loss 0.116601 (0.115517)\n",
      "\n",
      "Epoch: [1][40/288]\t Time 0.661 (0.670)\t Loss 0.091593 (0.113318)\n",
      "\n",
      "Epoch: [1][50/288]\t Time 0.663 (0.668)\t Loss 0.099319 (0.110515)\n",
      "\n",
      "Epoch: [1][60/288]\t Time 0.658 (0.667)\t Loss 0.141473 (0.113490)\n",
      "\n",
      "Epoch: [1][70/288]\t Time 0.660 (0.666)\t Loss 0.182772 (0.112970)\n",
      "\n",
      "Epoch: [1][80/288]\t Time 0.659 (0.665)\t Loss 0.161215 (0.113100)\n",
      "\n",
      "Epoch: [1][90/288]\t Time 0.659 (0.665)\t Loss 0.152126 (0.113548)\n",
      "\n",
      "Epoch: [1][100/288]\t Time 0.667 (0.665)\t Loss 0.139619 (0.113303)\n",
      "\n",
      "Epoch: [1][110/288]\t Time 0.657 (0.664)\t Loss 0.121264 (0.112945)\n",
      "\n",
      "Epoch: [1][120/288]\t Time 0.658 (0.664)\t Loss 0.093986 (0.112452)\n",
      "\n",
      "Epoch: [1][130/288]\t Time 0.659 (0.663)\t Loss 0.135588 (0.113156)\n",
      "\n",
      "Epoch: [1][140/288]\t Time 0.653 (0.663)\t Loss 0.100962 (0.113757)\n",
      "\n",
      "Epoch: [1][150/288]\t Time 0.660 (0.663)\t Loss 0.109473 (0.113335)\n",
      "\n",
      "Epoch: [1][160/288]\t Time 0.659 (0.663)\t Loss 0.118856 (0.113263)\n",
      "\n",
      "Epoch: [1][170/288]\t Time 0.660 (0.663)\t Loss 0.089195 (0.113106)\n",
      "\n",
      "Epoch: [1][180/288]\t Time 0.661 (0.663)\t Loss 0.109583 (0.112719)\n",
      "\n",
      "Epoch: [1][190/288]\t Time 0.653 (0.663)\t Loss 0.125109 (0.112735)\n",
      "\n",
      "Epoch: [1][200/288]\t Time 0.662 (0.662)\t Loss 0.086710 (0.112952)\n",
      "\n",
      "Epoch: [1][210/288]\t Time 0.654 (0.662)\t Loss 0.125969 (0.112703)\n",
      "\n",
      "Epoch: [1][220/288]\t Time 0.660 (0.662)\t Loss 0.092624 (0.113435)\n",
      "\n",
      "Epoch: [1][230/288]\t Time 0.658 (0.662)\t Loss 0.116347 (0.113517)\n",
      "\n",
      "Epoch: [1][240/288]\t Time 0.655 (0.662)\t Loss 0.114568 (0.113028)\n",
      "\n",
      "Epoch: [1][250/288]\t Time 0.660 (0.662)\t Loss 0.095524 (0.112836)\n",
      "\n",
      "Epoch: [1][260/288]\t Time 0.658 (0.662)\t Loss 0.101746 (0.112879)\n",
      "\n",
      "Epoch: [1][270/288]\t Time 0.650 (0.662)\t Loss 0.104493 (0.112743)\n",
      "\n",
      "Epoch: [1][280/288]\t Time 0.658 (0.662)\t Loss 0.133023 (0.112603)\n",
      "\n",
      "Epoch: [1]\t Train Loss: 0.112532  \t Val Loss: 0.108264\n",
      " ./epoch_log.txt\n",
      "True 0.10826405613786644\n",
      "Epoch: [2][0/288]\t Time 1.049 (1.049)\t Loss 0.127163 (0.127163)\n",
      "\n",
      "Epoch: [2][10/288]\t Time 0.662 (0.697)\t Loss 0.102843 (0.109588)\n",
      "\n",
      "Epoch: [2][20/288]\t Time 0.665 (0.681)\t Loss 0.109757 (0.110353)\n",
      "\n",
      "Epoch: [2][30/288]\t Time 0.657 (0.674)\t Loss 0.084299 (0.111116)\n",
      "\n",
      "Epoch: [2][40/288]\t Time 0.661 (0.671)\t Loss 0.132983 (0.110311)\n",
      "\n",
      "Epoch: [2][50/288]\t Time 0.660 (0.669)\t Loss 0.104419 (0.109530)\n",
      "\n",
      "Epoch: [2][60/288]\t Time 0.658 (0.667)\t Loss 0.083321 (0.110084)\n",
      "\n",
      "Epoch: [2][70/288]\t Time 0.664 (0.666)\t Loss 0.097806 (0.109222)\n",
      "\n",
      "Epoch: [2][80/288]\t Time 0.660 (0.665)\t Loss 0.106971 (0.108271)\n",
      "\n",
      "Epoch: [2][90/288]\t Time 0.662 (0.665)\t Loss 0.142601 (0.109255)\n",
      "\n",
      "Epoch: [2][100/288]\t Time 0.657 (0.664)\t Loss 0.109520 (0.108866)\n",
      "\n",
      "Epoch: [2][110/288]\t Time 0.658 (0.664)\t Loss 0.091075 (0.109510)\n",
      "\n",
      "Epoch: [2][120/288]\t Time 0.664 (0.664)\t Loss 0.092715 (0.109145)\n",
      "\n",
      "Epoch: [2][130/288]\t Time 0.660 (0.663)\t Loss 0.102578 (0.109288)\n",
      "\n",
      "Epoch: [2][140/288]\t Time 0.662 (0.663)\t Loss 0.118476 (0.108797)\n",
      "\n",
      "Epoch: [2][150/288]\t Time 0.660 (0.663)\t Loss 0.110377 (0.108411)\n",
      "\n",
      "Epoch: [2][160/288]\t Time 0.659 (0.663)\t Loss 0.133936 (0.108141)\n",
      "\n",
      "Epoch: [2][170/288]\t Time 0.664 (0.663)\t Loss 0.077201 (0.108087)\n",
      "\n",
      "Epoch: [2][180/288]\t Time 0.662 (0.663)\t Loss 0.119282 (0.107672)\n",
      "\n",
      "Epoch: [2][190/288]\t Time 0.662 (0.663)\t Loss 0.103727 (0.107662)\n",
      "\n",
      "Epoch: [2][200/288]\t Time 0.659 (0.662)\t Loss 0.096993 (0.107819)\n",
      "\n",
      "Epoch: [2][210/288]\t Time 0.660 (0.662)\t Loss 0.100133 (0.107888)\n",
      "\n",
      "Epoch: [2][220/288]\t Time 0.660 (0.662)\t Loss 0.120238 (0.108405)\n",
      "\n",
      "Epoch: [2][230/288]\t Time 0.659 (0.662)\t Loss 0.114695 (0.109055)\n",
      "\n",
      "Epoch: [2][240/288]\t Time 0.662 (0.662)\t Loss 0.105416 (0.109595)\n",
      "\n",
      "Epoch: [2][250/288]\t Time 0.660 (0.662)\t Loss 0.126574 (0.109553)\n",
      "\n",
      "Epoch: [2][260/288]\t Time 0.661 (0.662)\t Loss 0.114511 (0.109884)\n",
      "\n",
      "Epoch: [2][270/288]\t Time 0.654 (0.662)\t Loss 0.122829 (0.110037)\n",
      "\n",
      "Epoch: [2][280/288]\t Time 0.663 (0.662)\t Loss 0.144802 (0.110076)\n",
      "\n",
      "Epoch: [2]\t Train Loss: 0.109788  \t Val Loss: 0.110372\n",
      " ./epoch_log.txt\n",
      "False 0.10826405613786644\n",
      "Epoch: [3][0/288]\t Time 1.049 (1.049)\t Loss 0.128514 (0.128514)\n",
      "\n",
      "Epoch: [3][10/288]\t Time 0.666 (0.704)\t Loss 0.111644 (0.115917)\n",
      "\n",
      "Epoch: [3][20/288]\t Time 0.669 (0.687)\t Loss 0.088613 (0.108490)\n",
      "\n",
      "Epoch: [3][30/288]\t Time 0.664 (0.679)\t Loss 0.135866 (0.108944)\n",
      "\n",
      "Epoch: [3][40/288]\t Time 0.665 (0.675)\t Loss 0.137204 (0.109179)\n",
      "\n",
      "Epoch: [3][50/288]\t Time 0.664 (0.674)\t Loss 0.095183 (0.108454)\n",
      "\n",
      "Epoch: [3][60/288]\t Time 0.658 (0.672)\t Loss 0.090587 (0.109575)\n",
      "\n",
      "Epoch: [3][70/288]\t Time 0.668 (0.671)\t Loss 0.119665 (0.110271)\n",
      "\n",
      "Epoch: [3][80/288]\t Time 0.659 (0.670)\t Loss 0.112929 (0.110061)\n",
      "\n",
      "Epoch: [3][90/288]\t Time 0.665 (0.669)\t Loss 0.121284 (0.109940)\n",
      "\n",
      "Epoch: [3][100/288]\t Time 0.666 (0.669)\t Loss 0.140266 (0.110369)\n",
      "\n",
      "Epoch: [3][110/288]\t Time 0.672 (0.669)\t Loss 0.099257 (0.110554)\n",
      "\n",
      "Epoch: [3][120/288]\t Time 0.662 (0.668)\t Loss 0.142471 (0.109901)\n",
      "\n",
      "Epoch: [3][130/288]\t Time 0.670 (0.668)\t Loss 0.106334 (0.109329)\n",
      "\n",
      "Epoch: [3][140/288]\t Time 0.663 (0.667)\t Loss 0.145182 (0.109411)\n",
      "\n",
      "Epoch: [3][150/288]\t Time 0.666 (0.667)\t Loss 0.138699 (0.108975)\n",
      "\n",
      "Epoch: [3][160/288]\t Time 0.664 (0.667)\t Loss 0.068846 (0.108291)\n",
      "\n",
      "Epoch: [3][170/288]\t Time 0.662 (0.667)\t Loss 0.133778 (0.109076)\n",
      "\n",
      "Epoch: [3][180/288]\t Time 0.664 (0.667)\t Loss 0.085907 (0.108692)\n",
      "\n",
      "Epoch: [3][190/288]\t Time 0.656 (0.666)\t Loss 0.117820 (0.109285)\n",
      "\n",
      "Epoch: [3][200/288]\t Time 0.668 (0.666)\t Loss 0.146511 (0.109557)\n",
      "\n",
      "Epoch: [3][210/288]\t Time 0.656 (0.666)\t Loss 0.108763 (0.109871)\n",
      "\n",
      "Epoch: [3][220/288]\t Time 0.664 (0.666)\t Loss 0.110215 (0.110113)\n",
      "\n",
      "Epoch: [3][230/288]\t Time 0.666 (0.666)\t Loss 0.099427 (0.110024)\n",
      "\n",
      "Epoch: [3][240/288]\t Time 0.670 (0.666)\t Loss 0.112350 (0.110243)\n",
      "\n",
      "Epoch: [3][250/288]\t Time 0.663 (0.666)\t Loss 0.119850 (0.110002)\n",
      "\n",
      "Epoch: [3][260/288]\t Time 0.670 (0.666)\t Loss 0.134822 (0.109671)\n",
      "\n",
      "Epoch: [3][270/288]\t Time 0.663 (0.666)\t Loss 0.094340 (0.109694)\n",
      "\n",
      "Epoch: [3][280/288]\t Time 0.670 (0.666)\t Loss 0.071383 (0.109296)\n",
      "\n",
      "Epoch: [3]\t Train Loss: 0.109055  \t Val Loss: 0.107497\n",
      " ./epoch_log.txt\n",
      "True 0.1074967086315155\n",
      "Epoch: [4][0/288]\t Time 1.039 (1.039)\t Loss 0.153641 (0.153641)\n",
      "\n",
      "Epoch: [4][10/288]\t Time 0.660 (0.696)\t Loss 0.090393 (0.108028)\n",
      "\n",
      "Epoch: [4][20/288]\t Time 0.666 (0.679)\t Loss 0.126779 (0.109462)\n",
      "\n",
      "Epoch: [4][30/288]\t Time 0.657 (0.673)\t Loss 0.091744 (0.107696)\n",
      "\n",
      "Epoch: [4][40/288]\t Time 0.661 (0.670)\t Loss 0.103468 (0.108179)\n",
      "\n",
      "Epoch: [4][50/288]\t Time 0.660 (0.668)\t Loss 0.096182 (0.109905)\n",
      "\n",
      "Epoch: [4][60/288]\t Time 0.658 (0.666)\t Loss 0.103122 (0.108180)\n",
      "\n",
      "Epoch: [4][70/288]\t Time 0.662 (0.665)\t Loss 0.076433 (0.108488)\n",
      "\n",
      "Epoch: [4][80/288]\t Time 0.659 (0.665)\t Loss 0.097857 (0.107976)\n",
      "\n",
      "Epoch: [4][90/288]\t Time 0.665 (0.665)\t Loss 0.110383 (0.107332)\n",
      "\n",
      "Epoch: [4][100/288]\t Time 0.662 (0.664)\t Loss 0.083906 (0.107929)\n",
      "\n",
      "Epoch: [4][110/288]\t Time 0.659 (0.664)\t Loss 0.103135 (0.107642)\n",
      "\n",
      "Epoch: [4][120/288]\t Time 0.658 (0.664)\t Loss 0.078240 (0.106943)\n",
      "\n",
      "Epoch: [4][130/288]\t Time 0.657 (0.663)\t Loss 0.118502 (0.107165)\n",
      "\n",
      "Epoch: [4][140/288]\t Time 0.657 (0.663)\t Loss 0.111806 (0.107644)\n",
      "\n",
      "Epoch: [4][150/288]\t Time 0.659 (0.663)\t Loss 0.126515 (0.107061)\n",
      "\n",
      "Epoch: [4][160/288]\t Time 0.663 (0.663)\t Loss 0.118316 (0.106735)\n",
      "\n",
      "Epoch: [4][170/288]\t Time 0.656 (0.663)\t Loss 0.103388 (0.107616)\n",
      "\n",
      "Epoch: [4][180/288]\t Time 0.669 (0.663)\t Loss 0.088795 (0.107987)\n",
      "\n",
      "Epoch: [4][190/288]\t Time 0.660 (0.663)\t Loss 0.119056 (0.108127)\n",
      "\n",
      "Epoch: [4][200/288]\t Time 0.669 (0.663)\t Loss 0.109049 (0.108397)\n",
      "\n",
      "Epoch: [4][210/288]\t Time 0.654 (0.663)\t Loss 0.116275 (0.108759)\n",
      "\n",
      "Epoch: [4][220/288]\t Time 0.658 (0.663)\t Loss 0.105057 (0.108418)\n",
      "\n",
      "Epoch: [4][230/288]\t Time 0.666 (0.663)\t Loss 0.093700 (0.108258)\n",
      "\n",
      "Epoch: [4][240/288]\t Time 0.668 (0.663)\t Loss 0.078362 (0.107967)\n",
      "\n",
      "Epoch: [4][250/288]\t Time 0.663 (0.663)\t Loss 0.106963 (0.107916)\n",
      "\n",
      "Epoch: [4][260/288]\t Time 0.669 (0.663)\t Loss 0.082072 (0.107844)\n",
      "\n",
      "Epoch: [4][270/288]\t Time 0.659 (0.663)\t Loss 0.105554 (0.108062)\n",
      "\n",
      "Epoch: [4][280/288]\t Time 0.664 (0.663)\t Loss 0.072845 (0.107969)\n",
      "\n",
      "Epoch: [4]\t Train Loss: 0.107791  \t Val Loss: 0.107635\n",
      " ./epoch_log.txt\n",
      "False 0.1074967086315155\n",
      "Best Loss:  0.1074967086315155\n"
     ]
    }
   ],
   "source": [
    "train_model(startModel=None, startEpoch=0, numEpochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint './resnet_for_rnn/ResNet34_LSTM_experiment/checkpoint.pth.tar'\n",
      "=> loaded checkpoint './resnet_for_rnn/ResNet34_LSTM_experiment/checkpoint.pth.tar' (epoch 5)\n",
      "Epoch: [5][0/288]\t Time 1.098 (1.098)\t Loss 0.093593 (0.093593)\n",
      "\n",
      "Epoch: [5][10/288]\t Time 0.655 (0.696)\t Loss 0.122839 (0.115089)\n",
      "\n",
      "Epoch: [5][20/288]\t Time 0.658 (0.677)\t Loss 0.123852 (0.109020)\n",
      "\n",
      "Epoch: [5][30/288]\t Time 0.661 (0.671)\t Loss 0.075191 (0.108563)\n",
      "\n",
      "Epoch: [5][40/288]\t Time 0.662 (0.668)\t Loss 0.148312 (0.108486)\n",
      "\n",
      "Epoch: [5][50/288]\t Time 0.663 (0.667)\t Loss 0.107247 (0.107414)\n",
      "\n",
      "Epoch: [5][60/288]\t Time 0.663 (0.666)\t Loss 0.106809 (0.107767)\n",
      "\n",
      "Epoch: [5][70/288]\t Time 0.667 (0.665)\t Loss 0.079493 (0.106799)\n",
      "\n",
      "Epoch: [5][80/288]\t Time 0.665 (0.665)\t Loss 0.109875 (0.106121)\n",
      "\n",
      "Epoch: [5][90/288]\t Time 0.666 (0.665)\t Loss 0.075577 (0.107354)\n",
      "\n",
      "Epoch: [5][100/288]\t Time 0.659 (0.665)\t Loss 0.108715 (0.106641)\n",
      "\n",
      "Epoch: [5][110/288]\t Time 0.660 (0.665)\t Loss 0.092800 (0.106635)\n",
      "\n",
      "Epoch: [5][120/288]\t Time 0.665 (0.665)\t Loss 0.113850 (0.106634)\n",
      "\n",
      "Epoch: [5][130/288]\t Time 0.667 (0.665)\t Loss 0.086628 (0.106467)\n",
      "\n",
      "Epoch: [5][140/288]\t Time 0.670 (0.665)\t Loss 0.084648 (0.106373)\n",
      "\n",
      "Epoch: [5][150/288]\t Time 0.664 (0.665)\t Loss 0.080203 (0.106050)\n",
      "\n",
      "Epoch: [5][160/288]\t Time 0.668 (0.665)\t Loss 0.096147 (0.106627)\n",
      "\n",
      "Epoch: [5][170/288]\t Time 0.663 (0.665)\t Loss 0.096629 (0.107130)\n",
      "\n",
      "Epoch: [5][180/288]\t Time 0.665 (0.665)\t Loss 0.104150 (0.106892)\n",
      "\n",
      "Epoch: [5][190/288]\t Time 0.664 (0.665)\t Loss 0.112315 (0.106581)\n",
      "\n",
      "Epoch: [5][200/288]\t Time 0.657 (0.665)\t Loss 0.080713 (0.106529)\n",
      "\n",
      "Epoch: [5][210/288]\t Time 0.663 (0.665)\t Loss 0.087801 (0.106576)\n",
      "\n",
      "Epoch: [5][220/288]\t Time 0.667 (0.665)\t Loss 0.109487 (0.106745)\n",
      "\n",
      "Epoch: [5][230/288]\t Time 0.671 (0.665)\t Loss 0.110419 (0.106964)\n",
      "\n",
      "Epoch: [5][240/288]\t Time 0.664 (0.665)\t Loss 0.125928 (0.106863)\n",
      "\n",
      "Epoch: [5][250/288]\t Time 0.668 (0.665)\t Loss 0.076342 (0.106986)\n",
      "\n",
      "Epoch: [5][260/288]\t Time 0.670 (0.665)\t Loss 0.095825 (0.107172)\n",
      "\n",
      "Epoch: [5][270/288]\t Time 0.666 (0.665)\t Loss 0.103716 (0.107567)\n",
      "\n",
      "Epoch: [5][280/288]\t Time 0.666 (0.665)\t Loss 0.088850 (0.107689)\n",
      "\n",
      "Epoch: [5]\t Train Loss: 0.107529  \t Val Loss: 0.104150\n",
      " ./epoch_log.txt\n",
      "True 0.10415008891787794\n",
      "Epoch: [6][0/288]\t Time 1.059 (1.059)\t Loss 0.068229 (0.068229)\n",
      "\n",
      "Epoch: [6][10/288]\t Time 0.663 (0.699)\t Loss 0.114581 (0.104702)\n",
      "\n",
      "Epoch: [6][20/288]\t Time 0.664 (0.682)\t Loss 0.136964 (0.107141)\n",
      "\n",
      "Epoch: [6][30/288]\t Time 0.660 (0.677)\t Loss 0.095062 (0.105828)\n",
      "\n",
      "Epoch: [6][40/288]\t Time 0.661 (0.673)\t Loss 0.133145 (0.105898)\n",
      "\n",
      "Epoch: [6][50/288]\t Time 0.661 (0.671)\t Loss 0.082456 (0.105216)\n",
      "\n",
      "Epoch: [6][60/288]\t Time 0.659 (0.670)\t Loss 0.097132 (0.104840)\n",
      "\n",
      "Epoch: [6][70/288]\t Time 0.665 (0.669)\t Loss 0.064555 (0.104730)\n",
      "\n",
      "Epoch: [6][80/288]\t Time 0.670 (0.669)\t Loss 0.141392 (0.106375)\n",
      "\n",
      "Epoch: [6][90/288]\t Time 0.666 (0.669)\t Loss 0.099058 (0.106198)\n",
      "\n",
      "Epoch: [6][100/288]\t Time 0.663 (0.668)\t Loss 0.064590 (0.105558)\n",
      "\n",
      "Epoch: [6][110/288]\t Time 0.671 (0.668)\t Loss 0.100985 (0.105581)\n",
      "\n",
      "Epoch: [6][120/288]\t Time 0.665 (0.667)\t Loss 0.130275 (0.105842)\n",
      "\n",
      "Epoch: [6][130/288]\t Time 0.668 (0.667)\t Loss 0.121330 (0.106339)\n",
      "\n",
      "Epoch: [6][140/288]\t Time 0.662 (0.667)\t Loss 0.109780 (0.106300)\n",
      "\n",
      "Epoch: [6][150/288]\t Time 0.662 (0.667)\t Loss 0.080309 (0.105599)\n",
      "\n",
      "Epoch: [6][160/288]\t Time 0.665 (0.667)\t Loss 0.096265 (0.105677)\n",
      "\n",
      "Epoch: [6][170/288]\t Time 0.659 (0.666)\t Loss 0.128604 (0.105901)\n",
      "\n",
      "Epoch: [6][180/288]\t Time 0.663 (0.666)\t Loss 0.083235 (0.105751)\n",
      "\n",
      "Epoch: [6][190/288]\t Time 0.655 (0.666)\t Loss 0.095196 (0.106165)\n",
      "\n",
      "Epoch: [6][200/288]\t Time 0.661 (0.666)\t Loss 0.078110 (0.105996)\n",
      "\n",
      "Epoch: [6][210/288]\t Time 0.663 (0.666)\t Loss 0.146659 (0.106537)\n",
      "\n",
      "Epoch: [6][220/288]\t Time 0.666 (0.666)\t Loss 0.090176 (0.105960)\n",
      "\n",
      "Epoch: [6][230/288]\t Time 0.668 (0.666)\t Loss 0.118277 (0.105537)\n",
      "\n",
      "Epoch: [6][240/288]\t Time 0.668 (0.666)\t Loss 0.093527 (0.105494)\n",
      "\n",
      "Epoch: [6][250/288]\t Time 0.666 (0.666)\t Loss 0.098263 (0.105165)\n",
      "\n",
      "Epoch: [6][260/288]\t Time 0.670 (0.665)\t Loss 0.061950 (0.105606)\n",
      "\n",
      "Epoch: [6][270/288]\t Time 0.661 (0.665)\t Loss 0.076332 (0.105973)\n",
      "\n",
      "Epoch: [6][280/288]\t Time 0.662 (0.665)\t Loss 0.097620 (0.106075)\n",
      "\n",
      "Epoch: [6]\t Train Loss: 0.106047  \t Val Loss: 0.107036\n",
      " ./epoch_log.txt\n",
      "False 0.10415008891787794\n",
      "Epoch: [7][0/288]\t Time 1.100 (1.100)\t Loss 0.089467 (0.089467)\n",
      "\n",
      "Epoch: [7][10/288]\t Time 0.663 (0.710)\t Loss 0.072959 (0.107422)\n",
      "\n",
      "Epoch: [7][20/288]\t Time 0.660 (0.687)\t Loss 0.168187 (0.109961)\n",
      "\n",
      "Epoch: [7][30/288]\t Time 0.666 (0.680)\t Loss 0.118616 (0.110954)\n",
      "\n",
      "Epoch: [7][40/288]\t Time 0.659 (0.676)\t Loss 0.115417 (0.111407)\n",
      "\n",
      "Epoch: [7][50/288]\t Time 0.667 (0.673)\t Loss 0.122365 (0.110263)\n",
      "\n",
      "Epoch: [7][60/288]\t Time 0.656 (0.672)\t Loss 0.112840 (0.110038)\n",
      "\n",
      "Epoch: [7][70/288]\t Time 0.662 (0.671)\t Loss 0.082172 (0.109206)\n",
      "\n",
      "Epoch: [7][80/288]\t Time 0.665 (0.670)\t Loss 0.106575 (0.109065)\n",
      "\n",
      "Epoch: [7][90/288]\t Time 0.671 (0.669)\t Loss 0.125755 (0.109924)\n",
      "\n",
      "Epoch: [7][100/288]\t Time 0.666 (0.669)\t Loss 0.084082 (0.108919)\n",
      "\n",
      "Epoch: [7][110/288]\t Time 0.668 (0.668)\t Loss 0.127541 (0.108137)\n",
      "\n",
      "Epoch: [7][120/288]\t Time 0.664 (0.668)\t Loss 0.089933 (0.107322)\n",
      "\n",
      "Epoch: [7][130/288]\t Time 0.669 (0.667)\t Loss 0.107178 (0.107110)\n",
      "\n",
      "Epoch: [7][140/288]\t Time 0.663 (0.667)\t Loss 0.114777 (0.107413)\n",
      "\n",
      "Epoch: [7][150/288]\t Time 0.661 (0.667)\t Loss 0.085890 (0.106506)\n",
      "\n",
      "Epoch: [7][160/288]\t Time 0.663 (0.667)\t Loss 0.112210 (0.106250)\n",
      "\n",
      "Epoch: [7][170/288]\t Time 0.657 (0.667)\t Loss 0.121263 (0.106410)\n",
      "\n",
      "Epoch: [7][180/288]\t Time 0.662 (0.667)\t Loss 0.075521 (0.105762)\n",
      "\n",
      "Epoch: [7][190/288]\t Time 0.661 (0.667)\t Loss 0.120348 (0.106486)\n",
      "\n",
      "Epoch: [7][200/288]\t Time 0.671 (0.666)\t Loss 0.104594 (0.106616)\n",
      "\n",
      "Epoch: [7][210/288]\t Time 0.666 (0.666)\t Loss 0.134403 (0.106152)\n",
      "\n",
      "Epoch: [7][220/288]\t Time 0.668 (0.666)\t Loss 0.130216 (0.105980)\n",
      "\n",
      "Epoch: [7][230/288]\t Time 0.666 (0.666)\t Loss 0.108819 (0.106464)\n",
      "\n",
      "Epoch: [7][240/288]\t Time 0.668 (0.666)\t Loss 0.113666 (0.106858)\n",
      "\n",
      "Epoch: [7][250/288]\t Time 0.664 (0.666)\t Loss 0.099093 (0.106841)\n",
      "\n",
      "Epoch: [7][260/288]\t Time 0.668 (0.666)\t Loss 0.094377 (0.106963)\n",
      "\n",
      "Epoch: [7][270/288]\t Time 0.667 (0.666)\t Loss 0.112730 (0.106862)\n",
      "\n",
      "Epoch: [7][280/288]\t Time 0.658 (0.666)\t Loss 0.099589 (0.106797)\n",
      "\n",
      "Epoch: [7]\t Train Loss: 0.106628  \t Val Loss: 0.104705\n",
      " ./epoch_log.txt\n",
      "False 0.10415008891787794\n",
      "Epoch: [8][0/288]\t Time 1.098 (1.098)\t Loss 0.084544 (0.084544)\n",
      "\n",
      "Epoch: [8][10/288]\t Time 0.667 (0.705)\t Loss 0.086288 (0.100177)\n",
      "\n",
      "Epoch: [8][20/288]\t Time 0.659 (0.685)\t Loss 0.147000 (0.106087)\n",
      "\n",
      "Epoch: [8][30/288]\t Time 0.665 (0.678)\t Loss 0.110246 (0.109086)\n",
      "\n",
      "Epoch: [8][40/288]\t Time 0.671 (0.675)\t Loss 0.093103 (0.108958)\n",
      "\n",
      "Epoch: [8][50/288]\t Time 0.667 (0.672)\t Loss 0.071205 (0.110477)\n",
      "\n",
      "Epoch: [8][60/288]\t Time 0.673 (0.671)\t Loss 0.093090 (0.109944)\n",
      "\n",
      "Epoch: [8][70/288]\t Time 0.664 (0.670)\t Loss 0.138820 (0.109422)\n",
      "\n",
      "Epoch: [8][80/288]\t Time 0.664 (0.669)\t Loss 0.096648 (0.110180)\n",
      "\n",
      "Epoch: [8][90/288]\t Time 0.660 (0.669)\t Loss 0.127929 (0.110096)\n",
      "\n",
      "Epoch: [8][100/288]\t Time 0.663 (0.668)\t Loss 0.116597 (0.110390)\n",
      "\n",
      "Epoch: [8][110/288]\t Time 0.662 (0.668)\t Loss 0.102132 (0.109296)\n",
      "\n",
      "Epoch: [8][120/288]\t Time 0.659 (0.668)\t Loss 0.090970 (0.109589)\n",
      "\n",
      "Epoch: [8][130/288]\t Time 0.665 (0.667)\t Loss 0.087077 (0.109131)\n",
      "\n",
      "Epoch: [8][140/288]\t Time 0.654 (0.667)\t Loss 0.093950 (0.109260)\n",
      "\n",
      "Epoch: [8][150/288]\t Time 0.666 (0.667)\t Loss 0.101440 (0.109447)\n",
      "\n",
      "Epoch: [8][160/288]\t Time 0.666 (0.667)\t Loss 0.098444 (0.109356)\n",
      "\n",
      "Epoch: [8][170/288]\t Time 0.658 (0.666)\t Loss 0.087215 (0.109078)\n",
      "\n",
      "Epoch: [8][180/288]\t Time 0.666 (0.666)\t Loss 0.098790 (0.108479)\n",
      "\n",
      "Epoch: [8][190/288]\t Time 0.663 (0.666)\t Loss 0.122061 (0.108031)\n",
      "\n",
      "Epoch: [8][200/288]\t Time 0.671 (0.666)\t Loss 0.125971 (0.108154)\n",
      "\n",
      "Epoch: [8][210/288]\t Time 0.660 (0.665)\t Loss 0.138459 (0.108492)\n",
      "\n",
      "Epoch: [8][220/288]\t Time 0.662 (0.665)\t Loss 0.123354 (0.107735)\n",
      "\n",
      "Epoch: [8][230/288]\t Time 0.660 (0.665)\t Loss 0.109428 (0.107697)\n",
      "\n",
      "Epoch: [8][240/288]\t Time 0.659 (0.665)\t Loss 0.093100 (0.107695)\n",
      "\n",
      "Epoch: [8][250/288]\t Time 0.664 (0.665)\t Loss 0.086573 (0.107345)\n",
      "\n",
      "Epoch: [8][260/288]\t Time 0.659 (0.664)\t Loss 0.106647 (0.107051)\n",
      "\n",
      "Epoch: [8][270/288]\t Time 0.667 (0.664)\t Loss 0.088229 (0.106532)\n",
      "\n",
      "Epoch: [8][280/288]\t Time 0.662 (0.664)\t Loss 0.147287 (0.106453)\n",
      "\n",
      "Epoch: [8]\t Train Loss: 0.106134  \t Val Loss: 0.105405\n",
      " ./epoch_log.txt\n",
      "False 0.10415008891787794\n",
      "Epoch: [9][0/288]\t Time 1.061 (1.061)\t Loss 0.108507 (0.108507)\n",
      "\n",
      "Epoch: [9][10/288]\t Time 0.662 (0.699)\t Loss 0.091911 (0.095838)\n",
      "\n",
      "Epoch: [9][20/288]\t Time 0.657 (0.680)\t Loss 0.152161 (0.104022)\n",
      "\n",
      "Epoch: [9][30/288]\t Time 0.657 (0.674)\t Loss 0.123190 (0.105009)\n",
      "\n",
      "Epoch: [9][40/288]\t Time 0.662 (0.671)\t Loss 0.137581 (0.106193)\n",
      "\n",
      "Epoch: [9][50/288]\t Time 0.651 (0.669)\t Loss 0.109206 (0.108139)\n",
      "\n",
      "Epoch: [9][60/288]\t Time 0.660 (0.668)\t Loss 0.081510 (0.107248)\n",
      "\n",
      "Epoch: [9][70/288]\t Time 0.659 (0.667)\t Loss 0.100441 (0.106721)\n",
      "\n",
      "Epoch: [9][80/288]\t Time 0.661 (0.666)\t Loss 0.059274 (0.104843)\n",
      "\n",
      "Epoch: [9][90/288]\t Time 0.656 (0.665)\t Loss 0.148933 (0.107033)\n",
      "\n",
      "Epoch: [9][100/288]\t Time 0.654 (0.665)\t Loss 0.110883 (0.106852)\n",
      "\n",
      "Epoch: [9][110/288]\t Time 0.662 (0.665)\t Loss 0.139643 (0.107427)\n",
      "\n",
      "Epoch: [9][120/288]\t Time 0.657 (0.664)\t Loss 0.070899 (0.106643)\n",
      "\n",
      "Epoch: [9][130/288]\t Time 0.664 (0.664)\t Loss 0.094529 (0.106840)\n",
      "\n",
      "Epoch: [9][140/288]\t Time 0.668 (0.664)\t Loss 0.097557 (0.107172)\n",
      "\n",
      "Epoch: [9][150/288]\t Time 0.663 (0.664)\t Loss 0.083312 (0.107141)\n",
      "\n",
      "Epoch: [9][160/288]\t Time 0.665 (0.664)\t Loss 0.122281 (0.106589)\n",
      "\n",
      "Epoch: [9][170/288]\t Time 0.651 (0.663)\t Loss 0.111986 (0.106109)\n",
      "\n",
      "Epoch: [9][180/288]\t Time 0.659 (0.663)\t Loss 0.096985 (0.106224)\n",
      "\n",
      "Epoch: [9][190/288]\t Time 0.666 (0.663)\t Loss 0.094556 (0.105780)\n",
      "\n",
      "Epoch: [9][200/288]\t Time 0.659 (0.663)\t Loss 0.130058 (0.105866)\n",
      "\n",
      "Epoch: [9][210/288]\t Time 0.658 (0.663)\t Loss 0.062044 (0.105432)\n",
      "\n",
      "Epoch: [9][220/288]\t Time 0.655 (0.663)\t Loss 0.109743 (0.105755)\n",
      "\n",
      "Epoch: [9][230/288]\t Time 0.661 (0.662)\t Loss 0.096841 (0.106055)\n",
      "\n",
      "Epoch: [9][240/288]\t Time 0.669 (0.662)\t Loss 0.062204 (0.105714)\n",
      "\n",
      "Epoch: [9][250/288]\t Time 0.662 (0.662)\t Loss 0.092412 (0.105334)\n",
      "\n",
      "Epoch: [9][260/288]\t Time 0.669 (0.662)\t Loss 0.108145 (0.105307)\n",
      "\n",
      "Epoch: [9][270/288]\t Time 0.662 (0.662)\t Loss 0.118171 (0.105523)\n",
      "\n",
      "Epoch: [9][280/288]\t Time 0.659 (0.662)\t Loss 0.063555 (0.105921)\n",
      "\n",
      "Epoch: [9]\t Train Loss: 0.105722  \t Val Loss: 0.105738\n",
      " ./epoch_log.txt\n",
      "False 0.10415008891787794\n",
      "Epoch: [10][0/288]\t Time 1.071 (1.071)\t Loss 0.069850 (0.069850)\n",
      "\n",
      "Epoch: [10][10/288]\t Time 0.661 (0.698)\t Loss 0.126409 (0.104370)\n",
      "\n",
      "Epoch: [10][20/288]\t Time 0.668 (0.680)\t Loss 0.091661 (0.101745)\n",
      "\n",
      "Epoch: [10][30/288]\t Time 0.664 (0.674)\t Loss 0.117081 (0.106907)\n",
      "\n",
      "Epoch: [10][40/288]\t Time 0.663 (0.671)\t Loss 0.096709 (0.106438)\n",
      "\n",
      "Epoch: [10][50/288]\t Time 0.662 (0.669)\t Loss 0.092842 (0.106517)\n",
      "\n",
      "Epoch: [10][60/288]\t Time 0.661 (0.668)\t Loss 0.082338 (0.106284)\n",
      "\n",
      "Epoch: [10][70/288]\t Time 0.666 (0.667)\t Loss 0.142476 (0.106699)\n",
      "\n",
      "Epoch: [10][80/288]\t Time 0.667 (0.667)\t Loss 0.096007 (0.106881)\n",
      "\n",
      "Epoch: [10][90/288]\t Time 0.662 (0.667)\t Loss 0.128553 (0.106834)\n",
      "\n",
      "Epoch: [10][100/288]\t Time 0.666 (0.666)\t Loss 0.104299 (0.106175)\n",
      "\n",
      "Epoch: [10][110/288]\t Time 0.660 (0.666)\t Loss 0.114637 (0.106219)\n",
      "\n",
      "Epoch: [10][120/288]\t Time 0.656 (0.666)\t Loss 0.093940 (0.105546)\n",
      "\n",
      "Epoch: [10][130/288]\t Time 0.667 (0.666)\t Loss 0.097687 (0.105179)\n",
      "\n",
      "Epoch: [10][140/288]\t Time 0.672 (0.666)\t Loss 0.071185 (0.104943)\n",
      "\n",
      "Epoch: [10][150/288]\t Time 0.662 (0.666)\t Loss 0.067230 (0.104727)\n",
      "\n",
      "Epoch: [10][160/288]\t Time 0.668 (0.666)\t Loss 0.099778 (0.105272)\n",
      "\n",
      "Epoch: [10][170/288]\t Time 0.671 (0.666)\t Loss 0.100754 (0.105114)\n",
      "\n",
      "Epoch: [10][180/288]\t Time 0.662 (0.666)\t Loss 0.082841 (0.105127)\n",
      "\n",
      "Epoch: [10][190/288]\t Time 0.669 (0.666)\t Loss 0.115810 (0.105908)\n",
      "\n",
      "Epoch: [10][200/288]\t Time 0.662 (0.666)\t Loss 0.091035 (0.106433)\n",
      "\n",
      "Epoch: [10][210/288]\t Time 0.657 (0.665)\t Loss 0.123903 (0.106595)\n",
      "\n",
      "Epoch: [10][220/288]\t Time 0.657 (0.665)\t Loss 0.079615 (0.106000)\n",
      "\n",
      "Epoch: [10][230/288]\t Time 0.662 (0.665)\t Loss 0.079136 (0.105859)\n",
      "\n",
      "Epoch: [10][240/288]\t Time 0.658 (0.665)\t Loss 0.098445 (0.106195)\n",
      "\n",
      "Epoch: [10][250/288]\t Time 0.662 (0.665)\t Loss 0.100931 (0.106038)\n",
      "\n",
      "Epoch: [10][260/288]\t Time 0.654 (0.664)\t Loss 0.096460 (0.105835)\n",
      "\n",
      "Epoch: [10][270/288]\t Time 0.659 (0.664)\t Loss 0.160720 (0.105906)\n",
      "\n",
      "Epoch: [10][280/288]\t Time 0.658 (0.664)\t Loss 0.082364 (0.105650)\n",
      "\n",
      "Epoch: [10]\t Train Loss: 0.105899  \t Val Loss: 0.105423\n",
      " ./epoch_log.txt\n",
      "False 0.10415008891787794\n",
      "Epoch: [11][0/288]\t Time 1.092 (1.092)\t Loss 0.078392 (0.078392)\n",
      "\n",
      "Epoch: [11][10/288]\t Time 0.663 (0.704)\t Loss 0.099871 (0.096495)\n",
      "\n",
      "Epoch: [11][20/288]\t Time 0.659 (0.684)\t Loss 0.071766 (0.097204)\n",
      "\n",
      "Epoch: [11][30/288]\t Time 0.656 (0.677)\t Loss 0.107661 (0.102536)\n",
      "\n",
      "Epoch: [11][40/288]\t Time 0.666 (0.673)\t Loss 0.081314 (0.101879)\n",
      "\n",
      "Epoch: [11][50/288]\t Time 0.657 (0.671)\t Loss 0.095511 (0.102522)\n",
      "\n",
      "Epoch: [11][60/288]\t Time 0.659 (0.669)\t Loss 0.117516 (0.102263)\n",
      "\n",
      "Epoch: [11][70/288]\t Time 0.662 (0.668)\t Loss 0.094666 (0.103501)\n",
      "\n",
      "Epoch: [11][80/288]\t Time 0.662 (0.667)\t Loss 0.125465 (0.104051)\n",
      "\n",
      "Epoch: [11][90/288]\t Time 0.660 (0.666)\t Loss 0.086165 (0.104192)\n",
      "\n",
      "Epoch: [11][100/288]\t Time 0.660 (0.666)\t Loss 0.099121 (0.104332)\n",
      "\n",
      "Epoch: [11][110/288]\t Time 0.660 (0.665)\t Loss 0.121966 (0.104434)\n",
      "\n",
      "Epoch: [11][120/288]\t Time 0.658 (0.665)\t Loss 0.111275 (0.104067)\n",
      "\n",
      "Epoch: [11][130/288]\t Time 0.661 (0.665)\t Loss 0.067762 (0.104431)\n",
      "\n",
      "Epoch: [11][140/288]\t Time 0.657 (0.664)\t Loss 0.058632 (0.104676)\n",
      "\n",
      "Epoch: [11][150/288]\t Time 0.664 (0.664)\t Loss 0.113909 (0.104809)\n",
      "\n",
      "Epoch: [11][160/288]\t Time 0.656 (0.664)\t Loss 0.082700 (0.104961)\n",
      "\n",
      "Epoch: [11][170/288]\t Time 0.659 (0.664)\t Loss 0.099644 (0.105223)\n",
      "\n",
      "Epoch: [11][180/288]\t Time 0.658 (0.664)\t Loss 0.079782 (0.105456)\n",
      "\n",
      "Epoch: [11][190/288]\t Time 0.653 (0.663)\t Loss 0.085920 (0.105858)\n",
      "\n",
      "Epoch: [11][200/288]\t Time 0.659 (0.663)\t Loss 0.103459 (0.105687)\n",
      "\n",
      "Epoch: [11][210/288]\t Time 0.654 (0.663)\t Loss 0.101027 (0.106118)\n",
      "\n",
      "Epoch: [11][220/288]\t Time 0.662 (0.663)\t Loss 0.120696 (0.106198)\n",
      "\n",
      "Epoch: [11][230/288]\t Time 0.657 (0.663)\t Loss 0.108368 (0.106674)\n",
      "\n",
      "Epoch: [11][240/288]\t Time 0.648 (0.663)\t Loss 0.111716 (0.106730)\n",
      "\n",
      "Epoch: [11][250/288]\t Time 0.660 (0.663)\t Loss 0.104295 (0.106580)\n",
      "\n",
      "Epoch: [11][260/288]\t Time 0.655 (0.663)\t Loss 0.094520 (0.106645)\n",
      "\n",
      "Epoch: [11][270/288]\t Time 0.659 (0.663)\t Loss 0.104064 (0.106363)\n",
      "\n",
      "Epoch: [11][280/288]\t Time 0.655 (0.663)\t Loss 0.080231 (0.106182)\n",
      "\n",
      "Epoch: [11]\t Train Loss: 0.106227  \t Val Loss: 0.113537\n",
      " ./epoch_log.txt\n",
      "False 0.10415008891787794\n",
      "Epoch: [12][0/288]\t Time 1.084 (1.084)\t Loss 0.145445 (0.145445)\n",
      "\n",
      "Epoch: [12][10/288]\t Time 0.661 (0.701)\t Loss 0.077336 (0.105400)\n",
      "\n",
      "Epoch: [12][20/288]\t Time 0.664 (0.684)\t Loss 0.062253 (0.105496)\n",
      "\n",
      "Epoch: [12][30/288]\t Time 0.664 (0.678)\t Loss 0.089981 (0.104896)\n",
      "\n",
      "Epoch: [12][40/288]\t Time 0.670 (0.675)\t Loss 0.068135 (0.101095)\n",
      "\n",
      "Epoch: [12][50/288]\t Time 0.664 (0.673)\t Loss 0.091466 (0.099694)\n",
      "\n",
      "Epoch: [12][60/288]\t Time 0.666 (0.672)\t Loss 0.085839 (0.102339)\n",
      "\n",
      "Epoch: [12][70/288]\t Time 0.670 (0.671)\t Loss 0.121683 (0.104006)\n",
      "\n",
      "Epoch: [12][80/288]\t Time 0.662 (0.670)\t Loss 0.118249 (0.105074)\n",
      "\n",
      "Epoch: [12][90/288]\t Time 0.659 (0.669)\t Loss 0.112397 (0.105185)\n",
      "\n",
      "Epoch: [12][100/288]\t Time 0.665 (0.668)\t Loss 0.091579 (0.104982)\n",
      "\n",
      "Epoch: [12][110/288]\t Time 0.658 (0.667)\t Loss 0.115949 (0.105637)\n",
      "\n",
      "Epoch: [12][120/288]\t Time 0.664 (0.667)\t Loss 0.117967 (0.105607)\n",
      "\n",
      "Epoch: [12][130/288]\t Time 0.661 (0.666)\t Loss 0.066996 (0.105730)\n",
      "\n",
      "Epoch: [12][140/288]\t Time 0.662 (0.666)\t Loss 0.101975 (0.105756)\n",
      "\n",
      "Epoch: [12][150/288]\t Time 0.660 (0.666)\t Loss 0.086766 (0.105612)\n",
      "\n",
      "Epoch: [12][160/288]\t Time 0.662 (0.665)\t Loss 0.084795 (0.105187)\n",
      "\n",
      "Epoch: [12][170/288]\t Time 0.665 (0.665)\t Loss 0.111900 (0.104532)\n",
      "\n",
      "Epoch: [12][180/288]\t Time 0.663 (0.665)\t Loss 0.110441 (0.105676)\n",
      "\n",
      "Epoch: [12][190/288]\t Time 0.660 (0.664)\t Loss 0.080537 (0.105696)\n",
      "\n",
      "Epoch: [12][200/288]\t Time 0.666 (0.664)\t Loss 0.104065 (0.105356)\n",
      "\n",
      "Epoch: [12][210/288]\t Time 0.662 (0.664)\t Loss 0.087445 (0.105455)\n",
      "\n",
      "Epoch: [12][220/288]\t Time 0.666 (0.664)\t Loss 0.121472 (0.105894)\n",
      "\n",
      "Epoch: [12][230/288]\t Time 0.669 (0.664)\t Loss 0.098111 (0.105889)\n",
      "\n",
      "Epoch: [12][240/288]\t Time 0.657 (0.664)\t Loss 0.120550 (0.105815)\n",
      "\n",
      "Epoch: [12][250/288]\t Time 0.662 (0.664)\t Loss 0.118657 (0.105520)\n",
      "\n",
      "Epoch: [12][260/288]\t Time 0.658 (0.664)\t Loss 0.089478 (0.105968)\n",
      "\n",
      "Epoch: [12][270/288]\t Time 0.661 (0.664)\t Loss 0.107421 (0.106111)\n",
      "\n",
      "Epoch: [12][280/288]\t Time 0.657 (0.664)\t Loss 0.132722 (0.106364)\n",
      "\n",
      "Epoch: [12]\t Train Loss: 0.106208  \t Val Loss: 0.105924\n",
      " ./epoch_log.txt\n",
      "False 0.10415008891787794\n",
      "Epoch: [13][0/288]\t Time 1.061 (1.061)\t Loss 0.091838 (0.091838)\n",
      "\n",
      "Epoch: [13][10/288]\t Time 0.665 (0.702)\t Loss 0.069630 (0.101930)\n",
      "\n",
      "Epoch: [13][20/288]\t Time 0.665 (0.684)\t Loss 0.115810 (0.101522)\n",
      "\n",
      "Epoch: [13][30/288]\t Time 0.668 (0.678)\t Loss 0.120244 (0.101928)\n",
      "\n",
      "Epoch: [13][40/288]\t Time 0.663 (0.674)\t Loss 0.110093 (0.103118)\n",
      "\n",
      "Epoch: [13][50/288]\t Time 0.666 (0.672)\t Loss 0.100121 (0.103819)\n",
      "\n",
      "Epoch: [13][60/288]\t Time 0.661 (0.670)\t Loss 0.075851 (0.102813)\n",
      "\n",
      "Epoch: [13][70/288]\t Time 0.669 (0.669)\t Loss 0.085170 (0.104363)\n",
      "\n",
      "Epoch: [13][80/288]\t Time 0.662 (0.668)\t Loss 0.111594 (0.104130)\n",
      "\n",
      "Epoch: [13][90/288]\t Time 0.662 (0.667)\t Loss 0.118941 (0.104509)\n",
      "\n",
      "Epoch: [13][100/288]\t Time 0.664 (0.666)\t Loss 0.085724 (0.104501)\n",
      "\n",
      "Epoch: [13][110/288]\t Time 0.659 (0.666)\t Loss 0.078966 (0.104041)\n",
      "\n",
      "Epoch: [13][120/288]\t Time 0.662 (0.665)\t Loss 0.100932 (0.105007)\n",
      "\n",
      "Epoch: [13][130/288]\t Time 0.661 (0.665)\t Loss 0.081672 (0.104121)\n",
      "\n",
      "Epoch: [13][140/288]\t Time 0.664 (0.665)\t Loss 0.101090 (0.104153)\n",
      "\n",
      "Epoch: [13][150/288]\t Time 0.664 (0.665)\t Loss 0.102220 (0.104395)\n",
      "\n",
      "Epoch: [13][160/288]\t Time 0.660 (0.665)\t Loss 0.086587 (0.104646)\n",
      "\n",
      "Epoch: [13][170/288]\t Time 0.670 (0.665)\t Loss 0.094602 (0.104362)\n",
      "\n",
      "Epoch: [13][180/288]\t Time 0.658 (0.665)\t Loss 0.091511 (0.104218)\n",
      "\n",
      "Epoch: [13][190/288]\t Time 0.663 (0.665)\t Loss 0.106487 (0.104552)\n",
      "\n",
      "Epoch: [13][200/288]\t Time 0.661 (0.665)\t Loss 0.093564 (0.104629)\n",
      "\n",
      "Epoch: [13][210/288]\t Time 0.661 (0.665)\t Loss 0.107220 (0.104499)\n",
      "\n",
      "Epoch: [13][220/288]\t Time 0.665 (0.664)\t Loss 0.097071 (0.105158)\n",
      "\n",
      "Epoch: [13][230/288]\t Time 0.658 (0.664)\t Loss 0.081579 (0.104945)\n",
      "\n",
      "Epoch: [13][240/288]\t Time 0.664 (0.664)\t Loss 0.101136 (0.104895)\n",
      "\n",
      "Epoch: [13][250/288]\t Time 0.664 (0.664)\t Loss 0.132142 (0.104680)\n",
      "\n",
      "Epoch: [13][260/288]\t Time 0.659 (0.664)\t Loss 0.095046 (0.104944)\n",
      "\n",
      "Epoch: [13][270/288]\t Time 0.667 (0.664)\t Loss 0.096018 (0.104723)\n",
      "\n",
      "Epoch: [13][280/288]\t Time 0.662 (0.664)\t Loss 0.083091 (0.104439)\n",
      "\n",
      "Epoch: [13]\t Train Loss: 0.104464  \t Val Loss: 0.105908\n",
      " ./epoch_log.txt\n",
      "False 0.10415008891787794\n",
      "Epoch: [14][0/288]\t Time 1.068 (1.068)\t Loss 0.079897 (0.079897)\n",
      "\n",
      "Epoch: [14][10/288]\t Time 0.663 (0.700)\t Loss 0.096969 (0.093000)\n",
      "\n",
      "Epoch: [14][20/288]\t Time 0.666 (0.683)\t Loss 0.089134 (0.096843)\n",
      "\n",
      "Epoch: [14][30/288]\t Time 0.661 (0.677)\t Loss 0.083800 (0.098603)\n",
      "\n",
      "Epoch: [14][40/288]\t Time 0.665 (0.674)\t Loss 0.108786 (0.100011)\n",
      "\n",
      "Epoch: [14][50/288]\t Time 0.664 (0.673)\t Loss 0.118880 (0.100026)\n",
      "\n",
      "Epoch: [14][60/288]\t Time 0.669 (0.672)\t Loss 0.111710 (0.100501)\n",
      "\n",
      "Epoch: [14][70/288]\t Time 0.664 (0.671)\t Loss 0.127525 (0.101443)\n",
      "\n",
      "Epoch: [14][80/288]\t Time 0.663 (0.670)\t Loss 0.138737 (0.101580)\n",
      "\n",
      "Epoch: [14][90/288]\t Time 0.664 (0.669)\t Loss 0.090292 (0.102689)\n",
      "\n",
      "Epoch: [14][100/288]\t Time 0.662 (0.669)\t Loss 0.143537 (0.103620)\n",
      "\n",
      "Epoch: [14][110/288]\t Time 0.666 (0.668)\t Loss 0.113120 (0.103827)\n",
      "\n",
      "Epoch: [14][120/288]\t Time 0.666 (0.668)\t Loss 0.114282 (0.104363)\n",
      "\n",
      "Epoch: [14][130/288]\t Time 0.656 (0.668)\t Loss 0.075651 (0.104919)\n",
      "\n",
      "Epoch: [14][140/288]\t Time 0.666 (0.668)\t Loss 0.114493 (0.104843)\n",
      "\n",
      "Epoch: [14][150/288]\t Time 0.668 (0.668)\t Loss 0.113033 (0.105100)\n",
      "\n",
      "Epoch: [14][160/288]\t Time 0.673 (0.667)\t Loss 0.099167 (0.105003)\n",
      "\n",
      "Epoch: [14][170/288]\t Time 0.668 (0.667)\t Loss 0.124301 (0.104983)\n",
      "\n",
      "Epoch: [14][180/288]\t Time 0.670 (0.667)\t Loss 0.099383 (0.105104)\n",
      "\n",
      "Epoch: [14][190/288]\t Time 0.667 (0.667)\t Loss 0.176799 (0.105938)\n",
      "\n",
      "Epoch: [14][200/288]\t Time 0.664 (0.667)\t Loss 0.120104 (0.105525)\n",
      "\n",
      "Epoch: [14][210/288]\t Time 0.667 (0.667)\t Loss 0.081607 (0.105572)\n",
      "\n",
      "Epoch: [14][220/288]\t Time 0.668 (0.667)\t Loss 0.082685 (0.105100)\n",
      "\n",
      "Epoch: [14][230/288]\t Time 0.654 (0.667)\t Loss 0.126287 (0.104925)\n",
      "\n",
      "Epoch: [14][240/288]\t Time 0.663 (0.667)\t Loss 0.163765 (0.105173)\n",
      "\n",
      "Epoch: [14][250/288]\t Time 0.667 (0.667)\t Loss 0.173085 (0.105722)\n",
      "\n",
      "Epoch: [14][260/288]\t Time 0.671 (0.667)\t Loss 0.124833 (0.105835)\n",
      "\n",
      "Epoch: [14][270/288]\t Time 0.667 (0.667)\t Loss 0.109764 (0.106156)\n",
      "\n",
      "Epoch: [14][280/288]\t Time 0.667 (0.667)\t Loss 0.091776 (0.105768)\n",
      "\n",
      "Epoch: [14]\t Train Loss: 0.105612  \t Val Loss: 0.105907\n",
      " ./epoch_log.txt\n",
      "False 0.10415008891787794\n",
      "Epoch: [15][0/288]\t Time 1.063 (1.063)\t Loss 0.095488 (0.095488)\n",
      "\n",
      "Epoch: [15][10/288]\t Time 0.657 (0.698)\t Loss 0.105205 (0.104538)\n",
      "\n",
      "Epoch: [15][20/288]\t Time 0.664 (0.682)\t Loss 0.079583 (0.101287)\n",
      "\n",
      "Epoch: [15][30/288]\t Time 0.663 (0.677)\t Loss 0.093005 (0.102906)\n",
      "\n",
      "Epoch: [15][40/288]\t Time 0.657 (0.674)\t Loss 0.073023 (0.101374)\n",
      "\n",
      "Epoch: [15][50/288]\t Time 0.664 (0.672)\t Loss 0.094281 (0.100677)\n",
      "\n",
      "Epoch: [15][60/288]\t Time 0.666 (0.671)\t Loss 0.075910 (0.101317)\n",
      "\n",
      "Epoch: [15][70/288]\t Time 0.667 (0.670)\t Loss 0.085096 (0.102217)\n",
      "\n",
      "Epoch: [15][80/288]\t Time 0.663 (0.670)\t Loss 0.096218 (0.102387)\n",
      "\n",
      "Epoch: [15][90/288]\t Time 0.666 (0.669)\t Loss 0.126580 (0.104052)\n",
      "\n",
      "Epoch: [15][100/288]\t Time 0.671 (0.669)\t Loss 0.107403 (0.105421)\n",
      "\n",
      "Epoch: [15][110/288]\t Time 0.658 (0.668)\t Loss 0.105410 (0.105301)\n",
      "\n",
      "Epoch: [15][120/288]\t Time 0.667 (0.668)\t Loss 0.108086 (0.104931)\n",
      "\n",
      "Epoch: [15][130/288]\t Time 0.664 (0.667)\t Loss 0.151108 (0.105989)\n",
      "\n",
      "Epoch: [15][140/288]\t Time 0.664 (0.667)\t Loss 0.136980 (0.106301)\n",
      "\n",
      "Epoch: [15][150/288]\t Time 0.662 (0.667)\t Loss 0.096471 (0.106080)\n",
      "\n",
      "Epoch: [15][160/288]\t Time 0.663 (0.666)\t Loss 0.111428 (0.106641)\n",
      "\n",
      "Epoch: [15][170/288]\t Time 0.655 (0.666)\t Loss 0.080452 (0.105955)\n",
      "\n",
      "Epoch: [15][180/288]\t Time 0.663 (0.666)\t Loss 0.092951 (0.105968)\n",
      "\n",
      "Epoch: [15][190/288]\t Time 0.657 (0.665)\t Loss 0.095774 (0.106128)\n",
      "\n",
      "Epoch: [15][200/288]\t Time 0.663 (0.665)\t Loss 0.068848 (0.105619)\n",
      "\n",
      "Epoch: [15][210/288]\t Time 0.658 (0.665)\t Loss 0.085899 (0.105286)\n",
      "\n",
      "Epoch: [15][220/288]\t Time 0.666 (0.665)\t Loss 0.157412 (0.105269)\n",
      "\n",
      "Epoch: [15][230/288]\t Time 0.658 (0.665)\t Loss 0.099074 (0.105148)\n",
      "\n",
      "Epoch: [15][240/288]\t Time 0.666 (0.665)\t Loss 0.139997 (0.105275)\n",
      "\n",
      "Epoch: [15][250/288]\t Time 0.670 (0.665)\t Loss 0.115124 (0.105390)\n",
      "\n",
      "Epoch: [15][260/288]\t Time 0.666 (0.665)\t Loss 0.096376 (0.105479)\n",
      "\n",
      "Epoch: [15][270/288]\t Time 0.667 (0.665)\t Loss 0.123271 (0.105775)\n",
      "\n",
      "Epoch: [15][280/288]\t Time 0.669 (0.665)\t Loss 0.113028 (0.105795)\n",
      "\n",
      "Epoch: [15]\t Train Loss: 0.105484  \t Val Loss: 0.104305\n",
      " ./epoch_log.txt\n",
      "False 0.10415008891787794\n",
      "Epoch: [16][0/288]\t Time 1.057 (1.057)\t Loss 0.079308 (0.079308)\n",
      "\n",
      "Epoch: [16][10/288]\t Time 0.659 (0.695)\t Loss 0.106046 (0.104824)\n",
      "\n",
      "Epoch: [16][20/288]\t Time 0.666 (0.680)\t Loss 0.086574 (0.097229)\n",
      "\n",
      "Epoch: [16][30/288]\t Time 0.661 (0.674)\t Loss 0.123347 (0.101086)\n",
      "\n",
      "Epoch: [16][40/288]\t Time 0.659 (0.671)\t Loss 0.117928 (0.101821)\n",
      "\n",
      "Epoch: [16][50/288]\t Time 0.661 (0.669)\t Loss 0.120021 (0.101643)\n",
      "\n",
      "Epoch: [16][60/288]\t Time 0.659 (0.668)\t Loss 0.142519 (0.103585)\n",
      "\n",
      "Epoch: [16][70/288]\t Time 0.661 (0.667)\t Loss 0.098635 (0.102902)\n",
      "\n",
      "Epoch: [16][80/288]\t Time 0.657 (0.666)\t Loss 0.122581 (0.102769)\n",
      "\n",
      "Epoch: [16][90/288]\t Time 0.661 (0.666)\t Loss 0.116938 (0.103250)\n",
      "\n",
      "Epoch: [16][100/288]\t Time 0.660 (0.665)\t Loss 0.096212 (0.102686)\n",
      "\n",
      "Epoch: [16][110/288]\t Time 0.662 (0.665)\t Loss 0.119849 (0.102184)\n",
      "\n",
      "Epoch: [16][120/288]\t Time 0.654 (0.665)\t Loss 0.098326 (0.101836)\n",
      "\n",
      "Epoch: [16][130/288]\t Time 0.659 (0.665)\t Loss 0.099485 (0.102632)\n",
      "\n",
      "Epoch: [16][140/288]\t Time 0.652 (0.664)\t Loss 0.085313 (0.103934)\n",
      "\n",
      "Epoch: [16][150/288]\t Time 0.660 (0.664)\t Loss 0.124843 (0.103761)\n",
      "\n",
      "Epoch: [16][160/288]\t Time 0.653 (0.664)\t Loss 0.067210 (0.104008)\n",
      "\n",
      "Epoch: [16][170/288]\t Time 0.663 (0.664)\t Loss 0.085080 (0.103559)\n",
      "\n",
      "Epoch: [16][180/288]\t Time 0.657 (0.664)\t Loss 0.107740 (0.102974)\n",
      "\n",
      "Epoch: [16][190/288]\t Time 0.656 (0.663)\t Loss 0.110091 (0.102589)\n",
      "\n",
      "Epoch: [16][200/288]\t Time 0.658 (0.663)\t Loss 0.138108 (0.102536)\n",
      "\n",
      "Epoch: [16][210/288]\t Time 0.655 (0.663)\t Loss 0.109788 (0.102194)\n",
      "\n",
      "Epoch: [16][220/288]\t Time 0.666 (0.663)\t Loss 0.072955 (0.102155)\n",
      "\n",
      "Epoch: [16][230/288]\t Time 0.664 (0.663)\t Loss 0.090878 (0.102350)\n",
      "\n",
      "Epoch: [16][240/288]\t Time 0.653 (0.663)\t Loss 0.097096 (0.102370)\n",
      "\n",
      "Epoch: [16][250/288]\t Time 0.662 (0.663)\t Loss 0.123136 (0.102776)\n",
      "\n",
      "Epoch: [16][260/288]\t Time 0.652 (0.663)\t Loss 0.100079 (0.103100)\n",
      "\n",
      "Epoch: [16][270/288]\t Time 0.661 (0.663)\t Loss 0.116442 (0.103028)\n",
      "\n",
      "Epoch: [16][280/288]\t Time 0.671 (0.663)\t Loss 0.096396 (0.103053)\n",
      "\n",
      "Epoch: [16]\t Train Loss: 0.103249  \t Val Loss: 0.107440\n",
      " ./epoch_log.txt\n",
      "False 0.10415008891787794\n",
      "Epoch: [17][0/288]\t Time 1.050 (1.050)\t Loss 0.085400 (0.085400)\n",
      "\n",
      "Epoch: [17][10/288]\t Time 0.654 (0.696)\t Loss 0.103941 (0.098591)\n",
      "\n",
      "Epoch: [17][20/288]\t Time 0.661 (0.679)\t Loss 0.127182 (0.101240)\n",
      "\n",
      "Epoch: [17][30/288]\t Time 0.657 (0.674)\t Loss 0.058167 (0.102643)\n",
      "\n",
      "Epoch: [17][40/288]\t Time 0.662 (0.671)\t Loss 0.092593 (0.102257)\n",
      "\n",
      "Epoch: [17][50/288]\t Time 0.668 (0.669)\t Loss 0.124175 (0.103176)\n",
      "\n",
      "Epoch: [17][60/288]\t Time 0.664 (0.668)\t Loss 0.123263 (0.102762)\n",
      "\n",
      "Epoch: [17][70/288]\t Time 0.674 (0.667)\t Loss 0.121280 (0.102969)\n",
      "\n",
      "Epoch: [17][80/288]\t Time 0.661 (0.667)\t Loss 0.135586 (0.103026)\n",
      "\n",
      "Epoch: [17][90/288]\t Time 0.671 (0.667)\t Loss 0.103888 (0.103459)\n",
      "\n",
      "Epoch: [17][100/288]\t Time 0.670 (0.667)\t Loss 0.078455 (0.102714)\n",
      "\n",
      "Epoch: [17][110/288]\t Time 0.658 (0.667)\t Loss 0.107906 (0.103323)\n",
      "\n",
      "Epoch: [17][120/288]\t Time 0.664 (0.666)\t Loss 0.143072 (0.104012)\n",
      "\n",
      "Epoch: [17][130/288]\t Time 0.667 (0.666)\t Loss 0.064859 (0.104501)\n",
      "\n",
      "Epoch: [17][140/288]\t Time 0.662 (0.666)\t Loss 0.072850 (0.104245)\n",
      "\n",
      "Epoch: [17][150/288]\t Time 0.671 (0.666)\t Loss 0.107601 (0.104236)\n",
      "\n",
      "Epoch: [17][160/288]\t Time 0.652 (0.666)\t Loss 0.100182 (0.103828)\n",
      "\n",
      "Epoch: [17][170/288]\t Time 0.673 (0.666)\t Loss 0.114087 (0.103523)\n",
      "\n",
      "Epoch: [17][180/288]\t Time 0.668 (0.666)\t Loss 0.073612 (0.103920)\n",
      "\n",
      "Epoch: [17][190/288]\t Time 0.667 (0.666)\t Loss 0.116388 (0.104028)\n",
      "\n",
      "Epoch: [17][200/288]\t Time 0.666 (0.666)\t Loss 0.130663 (0.104592)\n",
      "\n",
      "Epoch: [17][210/288]\t Time 0.668 (0.666)\t Loss 0.102181 (0.104513)\n",
      "\n",
      "Epoch: [17][220/288]\t Time 0.667 (0.666)\t Loss 0.131765 (0.104931)\n",
      "\n",
      "Epoch: [17][230/288]\t Time 0.666 (0.666)\t Loss 0.071989 (0.104948)\n",
      "\n",
      "Epoch: [17][240/288]\t Time 0.658 (0.666)\t Loss 0.126694 (0.105213)\n",
      "\n",
      "Epoch: [17][250/288]\t Time 0.665 (0.666)\t Loss 0.089727 (0.105400)\n",
      "\n",
      "Epoch: [17][260/288]\t Time 0.656 (0.666)\t Loss 0.084968 (0.105129)\n",
      "\n",
      "Epoch: [17][270/288]\t Time 0.665 (0.666)\t Loss 0.076543 (0.105202)\n",
      "\n",
      "Epoch: [17][280/288]\t Time 0.669 (0.666)\t Loss 0.125651 (0.105304)\n",
      "\n",
      "Epoch: [17]\t Train Loss: 0.105021  \t Val Loss: 0.104293\n",
      " ./epoch_log.txt\n",
      "False 0.10415008891787794\n",
      "Epoch: [18][0/288]\t Time 1.082 (1.082)\t Loss 0.111053 (0.111053)\n",
      "\n",
      "Epoch: [18][10/288]\t Time 0.657 (0.698)\t Loss 0.095025 (0.094249)\n",
      "\n",
      "Epoch: [18][20/288]\t Time 0.662 (0.680)\t Loss 0.121443 (0.099874)\n",
      "\n",
      "Epoch: [18][30/288]\t Time 0.659 (0.675)\t Loss 0.104894 (0.102857)\n",
      "\n",
      "Epoch: [18][40/288]\t Time 0.667 (0.671)\t Loss 0.094483 (0.103968)\n",
      "\n",
      "Epoch: [18][50/288]\t Time 0.666 (0.669)\t Loss 0.120287 (0.105099)\n",
      "\n",
      "Epoch: [18][60/288]\t Time 0.661 (0.668)\t Loss 0.085852 (0.105173)\n",
      "\n",
      "Epoch: [18][70/288]\t Time 0.656 (0.667)\t Loss 0.115094 (0.103737)\n",
      "\n",
      "Epoch: [18][80/288]\t Time 0.657 (0.666)\t Loss 0.071694 (0.102835)\n",
      "\n",
      "Epoch: [18][90/288]\t Time 0.658 (0.665)\t Loss 0.134125 (0.103269)\n",
      "\n",
      "Epoch: [18][100/288]\t Time 0.653 (0.665)\t Loss 0.085467 (0.103312)\n",
      "\n",
      "Epoch: [18][110/288]\t Time 0.662 (0.665)\t Loss 0.107500 (0.102783)\n",
      "\n",
      "Epoch: [18][120/288]\t Time 0.657 (0.664)\t Loss 0.084133 (0.102443)\n",
      "\n",
      "Epoch: [18][130/288]\t Time 0.664 (0.664)\t Loss 0.094836 (0.101255)\n",
      "\n",
      "Epoch: [18][140/288]\t Time 0.652 (0.664)\t Loss 0.114107 (0.101475)\n",
      "\n",
      "Epoch: [18][150/288]\t Time 0.656 (0.664)\t Loss 0.096432 (0.101316)\n",
      "\n",
      "Epoch: [18][160/288]\t Time 0.656 (0.664)\t Loss 0.101677 (0.101157)\n",
      "\n",
      "Epoch: [18][170/288]\t Time 0.652 (0.663)\t Loss 0.124018 (0.101316)\n",
      "\n",
      "Epoch: [18][180/288]\t Time 0.659 (0.663)\t Loss 0.142306 (0.101357)\n",
      "\n",
      "Epoch: [18][190/288]\t Time 0.658 (0.663)\t Loss 0.079878 (0.101407)\n",
      "\n",
      "Epoch: [18][200/288]\t Time 0.662 (0.663)\t Loss 0.119128 (0.101592)\n",
      "\n",
      "Epoch: [18][210/288]\t Time 0.657 (0.663)\t Loss 0.127739 (0.101966)\n",
      "\n",
      "Epoch: [18][220/288]\t Time 0.657 (0.663)\t Loss 0.100285 (0.102369)\n",
      "\n",
      "Epoch: [18][230/288]\t Time 0.661 (0.663)\t Loss 0.119993 (0.102402)\n",
      "\n",
      "Epoch: [18][240/288]\t Time 0.655 (0.663)\t Loss 0.093581 (0.102725)\n",
      "\n",
      "Epoch: [18][250/288]\t Time 0.664 (0.663)\t Loss 0.116691 (0.103114)\n",
      "\n",
      "Epoch: [18][260/288]\t Time 0.666 (0.663)\t Loss 0.133727 (0.103514)\n",
      "\n",
      "Epoch: [18][270/288]\t Time 0.662 (0.662)\t Loss 0.089026 (0.103567)\n",
      "\n",
      "Epoch: [18][280/288]\t Time 0.664 (0.662)\t Loss 0.104753 (0.104021)\n",
      "\n",
      "Epoch: [18]\t Train Loss: 0.104095  \t Val Loss: 0.104087\n",
      " ./epoch_log.txt\n",
      "True 0.10408730121950309\n",
      "Epoch: [19][0/288]\t Time 1.074 (1.074)\t Loss 0.075839 (0.075839)\n",
      "\n",
      "Epoch: [19][10/288]\t Time 0.659 (0.698)\t Loss 0.118370 (0.111169)\n",
      "\n",
      "Epoch: [19][20/288]\t Time 0.668 (0.681)\t Loss 0.083247 (0.104861)\n",
      "\n",
      "Epoch: [19][30/288]\t Time 0.662 (0.674)\t Loss 0.114202 (0.102640)\n",
      "\n",
      "Epoch: [19][40/288]\t Time 0.667 (0.671)\t Loss 0.108717 (0.102623)\n",
      "\n",
      "Epoch: [19][50/288]\t Time 0.660 (0.669)\t Loss 0.102668 (0.100986)\n",
      "\n",
      "Epoch: [19][60/288]\t Time 0.667 (0.668)\t Loss 0.099437 (0.103547)\n",
      "\n",
      "Epoch: [19][70/288]\t Time 0.662 (0.667)\t Loss 0.104935 (0.103143)\n",
      "\n",
      "Epoch: [19][80/288]\t Time 0.662 (0.666)\t Loss 0.126856 (0.103042)\n",
      "\n",
      "Epoch: [19][90/288]\t Time 0.667 (0.666)\t Loss 0.067762 (0.102596)\n",
      "\n",
      "Epoch: [19][100/288]\t Time 0.661 (0.665)\t Loss 0.126473 (0.103585)\n",
      "\n",
      "Epoch: [19][110/288]\t Time 0.663 (0.665)\t Loss 0.095857 (0.104241)\n",
      "\n",
      "Epoch: [19][120/288]\t Time 0.663 (0.665)\t Loss 0.104384 (0.103957)\n",
      "\n",
      "Epoch: [19][130/288]\t Time 0.670 (0.665)\t Loss 0.128582 (0.103424)\n",
      "\n",
      "Epoch: [19][140/288]\t Time 0.664 (0.665)\t Loss 0.077659 (0.102742)\n",
      "\n",
      "Epoch: [19][150/288]\t Time 0.659 (0.665)\t Loss 0.126868 (0.103747)\n",
      "\n",
      "Epoch: [19][160/288]\t Time 0.663 (0.665)\t Loss 0.135745 (0.104262)\n",
      "\n",
      "Epoch: [19][170/288]\t Time 0.657 (0.665)\t Loss 0.077190 (0.103805)\n",
      "\n",
      "Epoch: [19][180/288]\t Time 0.666 (0.665)\t Loss 0.144787 (0.104195)\n",
      "\n",
      "Epoch: [19][190/288]\t Time 0.666 (0.665)\t Loss 0.091926 (0.105073)\n",
      "\n",
      "Epoch: [19][200/288]\t Time 0.670 (0.665)\t Loss 0.119339 (0.105071)\n",
      "\n",
      "Epoch: [19][210/288]\t Time 0.666 (0.665)\t Loss 0.140197 (0.105437)\n",
      "\n",
      "Epoch: [19][220/288]\t Time 0.665 (0.665)\t Loss 0.111270 (0.104752)\n",
      "\n",
      "Epoch: [19][230/288]\t Time 0.665 (0.665)\t Loss 0.129424 (0.104759)\n",
      "\n",
      "Epoch: [19][240/288]\t Time 0.661 (0.665)\t Loss 0.082311 (0.104974)\n",
      "\n",
      "Epoch: [19][250/288]\t Time 0.662 (0.665)\t Loss 0.102057 (0.104664)\n",
      "\n",
      "Epoch: [19][260/288]\t Time 0.664 (0.665)\t Loss 0.140565 (0.104683)\n",
      "\n",
      "Epoch: [19][270/288]\t Time 0.660 (0.665)\t Loss 0.098041 (0.105105)\n",
      "\n",
      "Epoch: [19][280/288]\t Time 0.667 (0.665)\t Loss 0.118646 (0.105109)\n",
      "\n",
      "Epoch: [19]\t Train Loss: 0.105221  \t Val Loss: 0.103623\n",
      " ./epoch_log.txt\n",
      "True 0.10362279332346386\n",
      "Epoch: [20][0/288]\t Time 1.052 (1.052)\t Loss 0.072968 (0.072968)\n",
      "\n",
      "Epoch: [20][10/288]\t Time 0.657 (0.697)\t Loss 0.110212 (0.095745)\n",
      "\n",
      "Epoch: [20][20/288]\t Time 0.665 (0.679)\t Loss 0.096711 (0.099652)\n",
      "\n",
      "Epoch: [20][30/288]\t Time 0.663 (0.673)\t Loss 0.096768 (0.100363)\n",
      "\n",
      "Epoch: [20][40/288]\t Time 0.657 (0.670)\t Loss 0.101415 (0.100743)\n",
      "\n",
      "Epoch: [20][50/288]\t Time 0.664 (0.668)\t Loss 0.116561 (0.100724)\n",
      "\n",
      "Epoch: [20][60/288]\t Time 0.658 (0.666)\t Loss 0.115675 (0.101591)\n",
      "\n",
      "Epoch: [20][70/288]\t Time 0.661 (0.666)\t Loss 0.139231 (0.102166)\n",
      "\n",
      "Epoch: [20][80/288]\t Time 0.660 (0.665)\t Loss 0.148818 (0.102218)\n",
      "\n",
      "Epoch: [20][90/288]\t Time 0.660 (0.664)\t Loss 0.097218 (0.102064)\n",
      "\n",
      "Epoch: [20][100/288]\t Time 0.660 (0.664)\t Loss 0.071103 (0.101955)\n",
      "\n",
      "Epoch: [20][110/288]\t Time 0.659 (0.664)\t Loss 0.096690 (0.101859)\n",
      "\n",
      "Epoch: [20][120/288]\t Time 0.665 (0.663)\t Loss 0.096760 (0.101568)\n",
      "\n",
      "Epoch: [20][130/288]\t Time 0.659 (0.663)\t Loss 0.133537 (0.102568)\n",
      "\n",
      "Epoch: [20][140/288]\t Time 0.657 (0.663)\t Loss 0.099742 (0.102522)\n",
      "\n",
      "Epoch: [20][150/288]\t Time 0.663 (0.663)\t Loss 0.074661 (0.102547)\n",
      "\n",
      "Epoch: [20][160/288]\t Time 0.659 (0.663)\t Loss 0.117556 (0.103203)\n",
      "\n",
      "Epoch: [20][170/288]\t Time 0.661 (0.662)\t Loss 0.107547 (0.103742)\n",
      "\n",
      "Epoch: [20][180/288]\t Time 0.661 (0.662)\t Loss 0.096193 (0.104572)\n",
      "\n",
      "Epoch: [20][190/288]\t Time 0.659 (0.662)\t Loss 0.092435 (0.104863)\n",
      "\n",
      "Epoch: [20][200/288]\t Time 0.661 (0.662)\t Loss 0.133788 (0.105368)\n",
      "\n",
      "Epoch: [20][210/288]\t Time 0.661 (0.662)\t Loss 0.111226 (0.105194)\n",
      "\n",
      "Epoch: [20][220/288]\t Time 0.660 (0.662)\t Loss 0.138904 (0.104980)\n",
      "\n",
      "Epoch: [20][230/288]\t Time 0.657 (0.662)\t Loss 0.095191 (0.104505)\n",
      "\n",
      "Epoch: [20][240/288]\t Time 0.658 (0.662)\t Loss 0.100229 (0.104379)\n",
      "\n",
      "Epoch: [20][250/288]\t Time 0.661 (0.662)\t Loss 0.119148 (0.104232)\n",
      "\n",
      "Epoch: [20][260/288]\t Time 0.655 (0.662)\t Loss 0.085582 (0.104008)\n",
      "\n",
      "Epoch: [20][270/288]\t Time 0.658 (0.662)\t Loss 0.114678 (0.104082)\n",
      "\n",
      "Epoch: [20][280/288]\t Time 0.658 (0.662)\t Loss 0.114477 (0.103853)\n",
      "\n",
      "Epoch: [20]\t Train Loss: 0.103931  \t Val Loss: 0.106016\n",
      " ./epoch_log.txt\n",
      "False 0.10362279332346386\n",
      "Epoch: [21][0/288]\t Time 1.068 (1.068)\t Loss 0.118606 (0.118606)\n",
      "\n",
      "Epoch: [21][10/288]\t Time 0.667 (0.701)\t Loss 0.095098 (0.104295)\n",
      "\n",
      "Epoch: [21][20/288]\t Time 0.660 (0.680)\t Loss 0.116095 (0.106691)\n",
      "\n",
      "Epoch: [21][30/288]\t Time 0.668 (0.674)\t Loss 0.092538 (0.106963)\n",
      "\n",
      "Epoch: [21][40/288]\t Time 0.663 (0.671)\t Loss 0.093720 (0.104480)\n",
      "\n",
      "Epoch: [21][50/288]\t Time 0.660 (0.669)\t Loss 0.088252 (0.104113)\n",
      "\n",
      "Epoch: [21][60/288]\t Time 0.665 (0.668)\t Loss 0.111142 (0.103439)\n",
      "\n",
      "Epoch: [21][70/288]\t Time 0.659 (0.667)\t Loss 0.082337 (0.102252)\n",
      "\n",
      "Epoch: [21][80/288]\t Time 0.664 (0.666)\t Loss 0.090694 (0.101302)\n",
      "\n",
      "Epoch: [21][90/288]\t Time 0.658 (0.666)\t Loss 0.108794 (0.101738)\n",
      "\n",
      "Epoch: [21][100/288]\t Time 0.665 (0.665)\t Loss 0.124047 (0.101991)\n",
      "\n",
      "Epoch: [21][110/288]\t Time 0.661 (0.665)\t Loss 0.098800 (0.103151)\n",
      "\n",
      "Epoch: [21][120/288]\t Time 0.662 (0.665)\t Loss 0.146765 (0.103615)\n",
      "\n",
      "Epoch: [21][130/288]\t Time 0.656 (0.664)\t Loss 0.086864 (0.103771)\n",
      "\n",
      "Epoch: [21][140/288]\t Time 0.659 (0.664)\t Loss 0.118996 (0.103931)\n",
      "\n",
      "Epoch: [21][150/288]\t Time 0.657 (0.664)\t Loss 0.106406 (0.103780)\n",
      "\n",
      "Epoch: [21][160/288]\t Time 0.657 (0.664)\t Loss 0.084340 (0.103624)\n",
      "\n",
      "Epoch: [21][170/288]\t Time 0.661 (0.664)\t Loss 0.108230 (0.103274)\n",
      "\n",
      "Epoch: [21][180/288]\t Time 0.653 (0.663)\t Loss 0.116801 (0.103602)\n",
      "\n",
      "Epoch: [21][190/288]\t Time 0.660 (0.663)\t Loss 0.088724 (0.103400)\n",
      "\n",
      "Epoch: [21][200/288]\t Time 0.658 (0.663)\t Loss 0.109701 (0.103879)\n",
      "\n",
      "Epoch: [21][210/288]\t Time 0.658 (0.663)\t Loss 0.105955 (0.103356)\n",
      "\n",
      "Epoch: [21][220/288]\t Time 0.658 (0.663)\t Loss 0.110039 (0.103294)\n",
      "\n",
      "Epoch: [21][230/288]\t Time 0.656 (0.663)\t Loss 0.113771 (0.103554)\n",
      "\n",
      "Epoch: [21][240/288]\t Time 0.660 (0.663)\t Loss 0.103621 (0.104264)\n",
      "\n",
      "Epoch: [21][250/288]\t Time 0.657 (0.663)\t Loss 0.089787 (0.104341)\n",
      "\n",
      "Epoch: [21][260/288]\t Time 0.664 (0.663)\t Loss 0.138440 (0.104058)\n",
      "\n",
      "Epoch: [21][270/288]\t Time 0.654 (0.663)\t Loss 0.091891 (0.104104)\n",
      "\n",
      "Epoch: [21][280/288]\t Time 0.661 (0.662)\t Loss 0.113963 (0.104098)\n",
      "\n",
      "Epoch: [21]\t Train Loss: 0.103795  \t Val Loss: 0.109390\n",
      " ./epoch_log.txt\n",
      "False 0.10362279332346386\n",
      "Epoch: [22][0/288]\t Time 1.094 (1.094)\t Loss 0.096660 (0.096660)\n",
      "\n",
      "Epoch: [22][10/288]\t Time 0.666 (0.705)\t Loss 0.101592 (0.110228)\n",
      "\n",
      "Epoch: [22][20/288]\t Time 0.662 (0.683)\t Loss 0.087078 (0.104403)\n",
      "\n",
      "Epoch: [22][30/288]\t Time 0.661 (0.676)\t Loss 0.123338 (0.102805)\n",
      "\n",
      "Epoch: [22][40/288]\t Time 0.660 (0.672)\t Loss 0.110976 (0.103957)\n",
      "\n",
      "Epoch: [22][50/288]\t Time 0.661 (0.670)\t Loss 0.086271 (0.103985)\n",
      "\n",
      "Epoch: [22][60/288]\t Time 0.664 (0.668)\t Loss 0.048848 (0.102510)\n",
      "\n",
      "Epoch: [22][70/288]\t Time 0.658 (0.667)\t Loss 0.083602 (0.101838)\n",
      "\n",
      "Epoch: [22][80/288]\t Time 0.662 (0.667)\t Loss 0.114561 (0.102864)\n",
      "\n",
      "Epoch: [22][90/288]\t Time 0.660 (0.666)\t Loss 0.122549 (0.103423)\n",
      "\n",
      "Epoch: [22][100/288]\t Time 0.662 (0.666)\t Loss 0.106327 (0.103547)\n",
      "\n",
      "Epoch: [22][110/288]\t Time 0.658 (0.665)\t Loss 0.096509 (0.103584)\n",
      "\n",
      "Epoch: [22][120/288]\t Time 0.661 (0.665)\t Loss 0.108349 (0.104059)\n",
      "\n",
      "Epoch: [22][130/288]\t Time 0.661 (0.664)\t Loss 0.126502 (0.103634)\n",
      "\n",
      "Epoch: [22][140/288]\t Time 0.659 (0.664)\t Loss 0.106541 (0.103302)\n",
      "\n",
      "Epoch: [22][150/288]\t Time 0.665 (0.664)\t Loss 0.096452 (0.103685)\n",
      "\n",
      "Epoch: [22][160/288]\t Time 0.660 (0.664)\t Loss 0.103339 (0.102909)\n",
      "\n",
      "Epoch: [22][170/288]\t Time 0.667 (0.664)\t Loss 0.088085 (0.103018)\n",
      "\n",
      "Epoch: [22][180/288]\t Time 0.654 (0.663)\t Loss 0.083496 (0.103176)\n",
      "\n",
      "Epoch: [22][190/288]\t Time 0.658 (0.663)\t Loss 0.095907 (0.103675)\n",
      "\n",
      "Epoch: [22][200/288]\t Time 0.657 (0.663)\t Loss 0.113934 (0.104298)\n",
      "\n",
      "Epoch: [22][210/288]\t Time 0.650 (0.663)\t Loss 0.123960 (0.104016)\n",
      "\n",
      "Epoch: [22][220/288]\t Time 0.658 (0.663)\t Loss 0.116331 (0.104035)\n",
      "\n",
      "Epoch: [22][230/288]\t Time 0.655 (0.663)\t Loss 0.106811 (0.103784)\n",
      "\n",
      "Epoch: [22][240/288]\t Time 0.659 (0.663)\t Loss 0.060380 (0.104033)\n",
      "\n",
      "Epoch: [22][250/288]\t Time 0.657 (0.663)\t Loss 0.119749 (0.104258)\n",
      "\n",
      "Epoch: [22][260/288]\t Time 0.652 (0.663)\t Loss 0.087786 (0.104130)\n",
      "\n",
      "Epoch: [22][270/288]\t Time 0.659 (0.662)\t Loss 0.119864 (0.104125)\n",
      "\n",
      "Epoch: [22][280/288]\t Time 0.655 (0.662)\t Loss 0.104819 (0.104235)\n",
      "\n",
      "Epoch: [22]\t Train Loss: 0.104027  \t Val Loss: 0.106508\n",
      " ./epoch_log.txt\n",
      "False 0.10362279332346386\n",
      "Epoch: [23][0/288]\t Time 1.070 (1.070)\t Loss 0.084392 (0.084392)\n",
      "\n",
      "Epoch: [23][10/288]\t Time 0.661 (0.699)\t Loss 0.101573 (0.094980)\n",
      "\n",
      "Epoch: [23][20/288]\t Time 0.658 (0.681)\t Loss 0.067682 (0.095824)\n",
      "\n",
      "Epoch: [23][30/288]\t Time 0.657 (0.674)\t Loss 0.128632 (0.098038)\n",
      "\n",
      "Epoch: [23][40/288]\t Time 0.660 (0.671)\t Loss 0.102998 (0.098828)\n",
      "\n",
      "Epoch: [23][50/288]\t Time 0.660 (0.669)\t Loss 0.097707 (0.100619)\n",
      "\n",
      "Epoch: [23][60/288]\t Time 0.661 (0.667)\t Loss 0.118875 (0.102907)\n",
      "\n",
      "Epoch: [23][70/288]\t Time 0.660 (0.667)\t Loss 0.132010 (0.104509)\n",
      "\n",
      "Epoch: [23][80/288]\t Time 0.657 (0.666)\t Loss 0.093492 (0.103979)\n",
      "\n",
      "Epoch: [23][90/288]\t Time 0.664 (0.666)\t Loss 0.102063 (0.103972)\n",
      "\n",
      "Epoch: [23][100/288]\t Time 0.655 (0.665)\t Loss 0.143719 (0.105134)\n",
      "\n",
      "Epoch: [23][110/288]\t Time 0.666 (0.665)\t Loss 0.076784 (0.105005)\n",
      "\n",
      "Epoch: [23][120/288]\t Time 0.656 (0.665)\t Loss 0.093064 (0.104514)\n",
      "\n",
      "Epoch: [23][130/288]\t Time 0.662 (0.664)\t Loss 0.096225 (0.104297)\n",
      "\n",
      "Epoch: [23][140/288]\t Time 0.659 (0.664)\t Loss 0.092167 (0.103837)\n",
      "\n",
      "Epoch: [23][150/288]\t Time 0.651 (0.664)\t Loss 0.099007 (0.103331)\n",
      "\n",
      "Epoch: [23][160/288]\t Time 0.659 (0.664)\t Loss 0.083298 (0.103515)\n",
      "\n",
      "Epoch: [23][170/288]\t Time 0.652 (0.664)\t Loss 0.108443 (0.103829)\n",
      "\n",
      "Epoch: [23][180/288]\t Time 0.661 (0.663)\t Loss 0.148446 (0.103907)\n",
      "\n",
      "Epoch: [23][190/288]\t Time 0.666 (0.663)\t Loss 0.093612 (0.103824)\n",
      "\n",
      "Epoch: [23][200/288]\t Time 0.665 (0.663)\t Loss 0.139808 (0.104034)\n",
      "\n",
      "Epoch: [23][210/288]\t Time 0.668 (0.663)\t Loss 0.091775 (0.103840)\n",
      "\n",
      "Epoch: [23][220/288]\t Time 0.661 (0.663)\t Loss 0.131206 (0.104034)\n",
      "\n",
      "Epoch: [23][230/288]\t Time 0.666 (0.663)\t Loss 0.091376 (0.103647)\n",
      "\n",
      "Epoch: [23][240/288]\t Time 0.664 (0.663)\t Loss 0.099171 (0.104226)\n",
      "\n",
      "Epoch: [23][250/288]\t Time 0.665 (0.663)\t Loss 0.083395 (0.104081)\n",
      "\n",
      "Epoch: [23][260/288]\t Time 0.663 (0.663)\t Loss 0.124762 (0.104386)\n",
      "\n",
      "Epoch: [23][270/288]\t Time 0.660 (0.663)\t Loss 0.082814 (0.104371)\n",
      "\n",
      "Epoch: [23][280/288]\t Time 0.666 (0.663)\t Loss 0.126405 (0.104217)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(startModel='./resnet_for_rnn/ResNet34_LSTM_experiment/checkpoint.pth.tar', startEpoch=5, numEpochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotting helper for loss\n",
    "def  plotLossStats(trainingLoss, validationLoss):\n",
    "    plt.plot(trainingLoss, label='trainingLoss')\n",
    "    plt.plot(vaidationLoss, label='validation loss')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Training and Validation Loss history')\n",
    "    plt.show()\n",
    "\n",
    "np.loadtxt('log_ResNet34_LSTM_experiment.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "a = np.loadtxt('log_ResNet34_LSTM_experiment.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
